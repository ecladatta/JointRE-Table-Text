{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1029b029-4c8b-4740-8a71-b8a8fb3ac17c",
   "metadata": {},
   "source": [
    "# 1. bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721ebe6e-bd29-4264-809a-f9de7153f2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mettaleb/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-06 09:11:17.974364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759734678.523248 3975010 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759734678.760325 3975010 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-06 09:11:20.531862: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/mettaleb/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "from groq import Groq\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a4f765-3d0b-488c-a78a-db979645a559",
   "metadata": {},
   "source": [
    "# 2.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf4b46-2d85-4dcd-898a-3a150f50b7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a923e8-3528-4c5d-b2cb-af577a38b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/projects/melodi/mettaleb/Annotation/corpus_challenge/test/F2_nous.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "d = {}\n",
    "\n",
    "important_keys = ['Company type', 'Industry', 'Founded', 'Founder', 'Headquarters']\n",
    "\n",
    "for idx, doc in enumerate(data.get(\"documents\", [])):\n",
    "    texts = []\n",
    "    extraction_meta = doc.get(\"raw\", {}).get(\"_source\", {}).get(\"extractionMetadata\", [])\n",
    "    for meta in extraction_meta:\n",
    "        for t in meta.get(\"texts\", []):\n",
    "            texts.append(t.get(\"value\", \"\"))\n",
    "    texts = \" \".join(texts).strip()\n",
    "\n",
    "    tables = []\n",
    "    for meta in extraction_meta:\n",
    "        for tbl in meta.get(\"tables\", []):\n",
    "            table_data = tbl.get(\"tableData\", [])\n",
    "            cond1 = all(len(row) == 2 for row in table_data)\n",
    "            cond2 = len(table_data) > 0 and table_data[0] == ['0', '1']\n",
    "            cond3 = any(row[0] in important_keys for row in table_data[1:])\n",
    "\n",
    "            if cond1 and cond2 and cond3:\n",
    "                headers = [row[0] for row in table_data[1:]]\n",
    "                values = [row[1] for row in table_data[1:]]\n",
    "                new_table = {\"tableData\": [headers, values]}\n",
    "                tables.append(new_table)\n",
    "            else:\n",
    "                tables.append({\"tableData\": table_data})\n",
    "\n",
    "    #TRIPLETS\n",
    "    triplets_list = []\n",
    "    for ann in doc.get(\"annotations\", []):\n",
    "        subj = ann.get(\"subject\", {}).get(\"annotationValue\", \"\")\n",
    "        obj = ann.get(\"object\", {}).get(\"annotationValue\", \"\")\n",
    "        pred_val = ann.get(\"predicate\", {}).get(\"entityValue\", \"\")\n",
    "        if pred_val.lower() == \"pertinence\":\n",
    "            continue\n",
    "        triplet_str = f\"{subj} ; {obj} ; {pred_val}\"\n",
    "        triplets_list.append(triplet_str)\n",
    "\n",
    "    triplets = \" | \".join(triplets_list)\n",
    "\n",
    "    #Sauvegarde\n",
    "    d[idx] = [texts, tables, triplets]\n",
    "\n",
    "#for k, v in list(d.items())[:5]:\n",
    "#    print(f\"Doc {k}:\")\n",
    " #   print(\"  Text:\", v[0][:100], \"...\")\n",
    "  #  print(\"  Nb tables:\", len(v[1]))\n",
    "   # for t in v[1]:\n",
    "    #    print(\"  Table:\", t)\n",
    "    #print(\"  Triplets:\", v[2])\n",
    "   # print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6c036-a544-4b09-abb8-46f71ca845dc",
   "metadata": {},
   "source": [
    "#### Convertit une table au format structurée CSV-like (string).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ca0171-d1d2-40b2-ba4d-8955ccb98e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_csvlike(table_dict):\n",
    "    table = table_dict.get(\"tableData\", [])\n",
    "    if not table:\n",
    "        return \"\"\n",
    "    headers = table[0]\n",
    "    headers = [h.strip() if h.strip() else f\"Col{i+1}\" for i, h in enumerate(headers)]\n",
    "    rows = table[1:]\n",
    "    csv_lines = [\" | \".join(headers)]\n",
    "    for row in rows:\n",
    "        # Compléter si ligne plus courte\n",
    "        row_extended = row + [\"\"] * (len(headers) - len(row))\n",
    "        csv_lines.append(\" | \".join(row_extended))\n",
    "    \n",
    "    return \"\\n\".join(csv_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ffa724f-1713-4584-b119-1fde6b4cff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for k, v in list(d.items())[:5]:\n",
    "#    print()\n",
    "#    print(table_to_csvlike(v[1][0]))\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a583558a-3031-4f19-a77d-e9bc1fa3fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = ['acquired_by','brand_of', 'client_of', 'collaboration', 'competitor_of', 'merged_with', 'product_or_service_of', 'regulated_by', 'shareholder_of', 'subsidiary_of', 'traded_on']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b194c9-a71b-42da-a76d-2b1606089a55",
   "metadata": {},
   "source": [
    "# 3.Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17de2b-62e3-4f18-90ef-55410fb67626",
   "metadata": {},
   "source": [
    "## Avec Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc4cff6-6804-4daf-8035-c22be773dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caffdae3-9c0d-4562-9029-05b348b964eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key= api_key)\n",
    "#DEFAULT_MODEL = \"llama-3.3-70b-versatile\"\n",
    "#DEFAULT_MODEL = \"deepseek-r1-distill-llama-70b\"\n",
    "DEFAULT_MODEL = \"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
    "def assistant(content: str):\n",
    "    return { \"role\": \"assistant\", \"content\": content }\n",
    "\n",
    "def user(content: str):\n",
    "    return { \"role\": \"user\", \"content\": content }\n",
    "\n",
    "def chat_completion(\n",
    "    messages: List[Dict],\n",
    "    model = DEFAULT_MODEL,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.2,\n",
    ") -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "        \n",
    "\n",
    "def completion(\n",
    "    prompt: str,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.2,\n",
    ") -> str:\n",
    "    return chat_completion(\n",
    "        [user(prompt)],\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "def complete_and_print(prompt: str, model: str = DEFAULT_MODEL):\n",
    "    #print(f'==============\\n{prompt}\\n==============')\n",
    "    response = completion(prompt, model)\n",
    "    #print(response, end='\\n\\n')\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be4b22-b582-4418-b008-3c7f91912cf0",
   "metadata": {},
   "source": [
    "## Avec HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e524a5f-61bd-4bb5-a707-03393406a687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "#DEFAULT_MODEL = \"deepseek-ai/DeepSeek-V3.2-Exp\"\n",
    "#DEFAULT_MODEL = \"deepseek-ai/DeepSeek-R1\"\n",
    "DEFAULT_MODEL = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
    "#DEFAULT_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "#DEFAULT_MODEL = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "\n",
    "CACHE_DIR = \"/projects/melodi/mettaleb/huggingface_cache\"\n",
    "\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ab6de-0323-4b64-b5fa-d58220b9c833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6241f77-04f5-48ef-8f1b-66b6a9959dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "def assistant(content: str) -> Dict:\n",
    "    return {\"role\": \"assistant\", \"content\": content}\n",
    "\n",
    "def user(content: str) -> Dict:\n",
    "    return {\"role\": \"user\", \"content\": content}\n",
    "\n",
    "\n",
    "def chat_completion(\n",
    "    messages: List[Dict],\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.2,\n",
    ") -> str:\n",
    "    \n",
    "    prompt = \"\"\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        content = msg[\"content\"]\n",
    "        prompt += f\"{role.upper()}:\\n{content}\\n\\n\"\n",
    "\n",
    "    output = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    return output[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "def completion(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.1\n",
    ") -> str:\n",
    "    return chat_completion([user(prompt)], max_new_tokens, temperature, top_p)\n",
    "\n",
    "def complete_and_print(prompt: str):\n",
    "    response = completion(prompt)\n",
    "    print(response)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c961d99-cb3a-4cc4-a712-feb509eb2cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1911ee5-badc-4baf-85d8-417987a447e5",
   "metadata": {},
   "source": [
    "## 3.1 Zero-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb98add4-ae59-46e9-8376-86467694b4d5",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df8e8e00-fe68-43cb-ba3f-cf8fd3469710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupere_prompt(num_prompt, i):\n",
    "    if num_prompt == 1:\n",
    "        Prompt = f\"\"\"  \n",
    "        You are an NLP expert specializing in cross-source relation extraction between a free-text passage and a tabular dataset. Produce only valid relation triplets (entity1, relation, entity2) that connect at least one entity from the text with at least one entity from the table.\n",
    "        \n",
    "        Inputs:\n",
    "        - Text: {v[0]}\\n\n",
    "        - Table (CSV-like string):\\n {table_to_csvlike(v[1][0])}\\n\\n\n",
    "        - Allowed relation types: [relations]\n",
    "\n",
    "        \n",
    "        Global rules:\\n\n",
    "        - Output only triplets in the strict format defined below. Do not include explanations, notes, or extra text.\n",
    "        - Use only relation labels provided in \"Allowed relation types\". Ignore all other relation labels.\n",
    "        - Each triplet must include at least one text-sourced entity and at least one table-sourced entity.\n",
    "        - If no valid triplet exists, output exactly: \"NO_RELATION\"\n",
    "        \n",
    "        Entity requirements:\\n\n",
    "        - entity1 and entity2 must be named entities: proper-noun phrases referring to real-world entities. Do not use generic/common nouns (e.g., \"the company\", \"the city\").\n",
    "        - Text-sourced entities must be extracted from the text. Table-sourced entities must be taken from table cell values (not headers).\n",
    "      \n",
    "        Output format (strict):\n",
    "        \n",
    "        - If at least one valid triplet exists, output them on a single line using exactly: entity1, entity2: relation | entity3, entity4: relation | ...\n",
    "        - If none are valid, output exactly: NO_RELATION\n",
    "        \n",
    "        Return only the triplets in the strict output format.\n",
    "        \"\"\"\n",
    "    if num_prompt == 2:\n",
    "        Prompt = f\"\"\"\n",
    "        As a specialist in relation extraction, your task is to identify and output valid relational triplets from provided text and tabular data. \n",
    "        Each triplet should consist of two named entities (one from the text and one from the table) and a relation connecting them.\n",
    "        \n",
    "        - Follow these detailed guidelines:\n",
    "        \n",
    "            - Triplets Formation: \n",
    "                . Each triplet should link one entity from the text ('entity1') with one entity from the table ('entity2').\n",
    "                . Entity Requirements: Ensure both 'entity1' and 'entity2' are named entities.\n",
    "                . Relation Types: Use only the relation types listed in the provided 'Possible relation types'.\n",
    "                . Exclude any relations not listed.\n",
    "                . Contextual Validity: Validate relations based on the combined context of the text and the table. Ignore relations that are valid only in isolation.\n",
    "                . No Relation Scenario: If no valid relations are found, return 'NO_RELATION'.\n",
    "                . Output Format: Present your findings as a series of triplets in the format: 'entity1, entity2: relation1 | entity3, entity4: relation2 | ...', without additional text or explanations.\n",
    "                \n",
    "            - Data Inputs:\n",
    "                - Text: {v[0]}\\n  \n",
    "                - Table:\\n {table_to_csvlike(v[1][0])}\\n  \n",
    "                - Possible relation types: [{relations}]\\n\\n \n",
    "                \n",
    "            - Objective: \n",
    "                 -Analyze both the text and the table to extract all valid relation triplets. Ensure the output strictly adheres to the specified format.\n",
    "        \"\"\"\n",
    "    if num_prompt == 3:\n",
    "        Prompt = f\"\"\"  \n",
    "        You are an expert in Natural Language Processing (NLP) specializing in relation extraction.  \n",
    "        Your task is to extract relations expressed strictly as triplets: (entity1, relation, entity2).  \n",
    "\n",
    "        Constraints:\n",
    "        - Both entity1 and entity2 must be valid named entities.  \n",
    "        - At least one entity must come from the text and the other from the table.  \n",
    "        - Only keep relations that are explicitly listed in the provided \"Possible relation types\".  \n",
    "        - Ignore any relation not in this list.  \n",
    "        - The extracted triplets must reflect connections valid, only when combining both sources (text + table), not when taken in isolation.  \n",
    "        - If no valid triplet exists, return \"NO_RELATION\".  \n",
    "        - Output must contain only the triplets in the required format. Do not include explanations, reasoning, or extra text.  \n",
    "\n",
    "        Output Format (strict):  \n",
    "        entity1, entity2: relation1 | entity3, entity4: relation2 | ...  \n",
    "\n",
    "        Available Data:  \n",
    "        - Text segment: {v[0]}  \n",
    "        - Table content:\\n {table_to_csvlike(v[1][0])}  \n",
    "        - Possible relation types: [{relations}]  \n",
    "\n",
    "        Your task:  \n",
    "        1. Identify relations where one entity is in the text and the other in the table.  \n",
    "        2. Keep only relations that match the provided list.  \n",
    "        3. Return the result strictly in the format:  entity1, entity2: relation1 | entity3, entity4: relation2  \n",
    "        4. If no valid relation exists, return \"NO_RELATION\".  \n",
    "\"\"\"\n",
    "\n",
    "    if num_prompt == 4: # à utiliser avec le modèle LLaMA\n",
    "        Prompt = f\"\"\"\n",
    "                <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "                As a Natural Language Processing (NLP) expert specializing in relation extraction.\n",
    "        Your task is to identify and extract valid relations expressed as triplets (entity1, relation, entity2) \n",
    "        from both a given text segment and a table content.\n",
    "        \n",
    "        Constraints:\n",
    "        - Both entity1 and entity2 must be valid named entities.\n",
    "        - Only extract relations where one entity is from the text and the other is from the table.\n",
    "        - Only use relations that are explicitly listed in the provided \"Possible relation types\".\n",
    "        - If no valid relation exists, return \"NO_RELATION\".\n",
    "        - The output must **only** contain the extracted triplets in the requested format, with no explanations, reasoning, or extra text.\n",
    "        \n",
    "        Output Format:\n",
    "        entity1, entity2: relation1 | entity3, entity4: relation2 | ...\n",
    "        \n",
    "        <|eot_id|>\n",
    "        \n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        You are provided with:\n",
    "        \n",
    "        - Text segment: {v[0]}\n",
    "        - Table content:\\n {table_to_csvlike(v[1][0])}\n",
    "        - Possible relation types: [{relations}]\n",
    "        \n",
    "        Your task:\n",
    "        1. Identify relations where at least one entity is in the text and the other in the table.\n",
    "        2. Construct relation triplets combining entities from both sources.\n",
    "        3. Only keep relations that match the provided list of relation types.\n",
    "        4. Return the result strictly in the format: entity1, entity2: relation1 | entity3, entity4: relation2\n",
    "        \n",
    "        If no valid relation exists, return \"NO_RELATION\".\n",
    "        \n",
    "        <|eot_id|>\n",
    "        \n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\"\n",
    "    return Prompt\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c314c8-cad4-4ab2-b526-fe83313e6a56",
   "metadata": {},
   "source": [
    "## 3.2 Few-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dad2b0a-2108-47fd-8259-3666f98487c7",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ca2b3ff-20c7-491b-b568-35ad16ce62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trilets_gold = []\n",
    "texts = []\n",
    "tables = []\n",
    "for k, v in list(d.items()):\n",
    "    trilets_gold.append(v[2])\n",
    "    texts.append(v[0])\n",
    "    tables.append(table_to_csvlike(v[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e22c5c-6651-439e-8074-b9125d8ec479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae7cc1-455b-465e-8437-a2ee9b53204e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "856005bb-6a64-4470-af75-277662c005b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupere_prompt(num_prompt, i):\n",
    "    if num_prompt == 1:\n",
    "        Prompt = f\"\"\" You are a relation-extraction specialist. Extract cross-source relation triplets (entity1, relation, entity2)\n",
    "        by connecting one entity from the Text with one entity from the Table.\n",
    "\n",
    "        Inputs:\n",
    "        - Text\n",
    "        - Table (CSV-like)\n",
    "        - Possible relation types: [{relations}]\n",
    "        \n",
    "        Output format (strict):\n",
    "        - Return only triplets, no extra text, no explanations, and no newline: entity1, entity2: relation | entity3, entity4: relation | ...\n",
    "        - If no valid triplet exists, return exactly: NO_RELATION\n",
    "        \n",
    "        Core constraints:\n",
    "        - Cross-source requirement: In every triplet, one entity must come from the Text and the other from the Table.\n",
    "        - entity1 and entity2 must be named entities. Do not invent entities.\n",
    "        - Use only relation labels listed in Possible relation types, exactly as written.\n",
    "        - Do not infer relations that are not supported by the Text and/or Table.\n",
    "        \n",
    "        Relation direction and semantics:\n",
    "        - Use the conventional direction implied by the relation label. Examples if present in Possible relation types:\n",
    "            . Acquired_by: e2 purchases controlling stake in e1. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Brand of: e2 offers products or services of e1 (Brand). The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Client of: e1 uses (and presumably pays for) products or services offered by e2. The relation is directed. The inverted relation is best described by “Supplier of”.\n",
    "            . Collaboration: e1 and e2 collaborate in (parts of their) business activities. The relation is undirected.\n",
    "            . Competitor of: e1 competes for resources with e2. The relation is undirected.\n",
    "            . Merged with: e1 and e2 merged (parts of) their business activities. The relation is undirected.\n",
    "            . Product or service of: e1 is offered for commercial distribution by e2. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Regulated by: e2 regulates (parts of) the business activity of e1. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Shareholder of: e1 owns shares in e2. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Subsidiary of: e2 legally owns e1. The relation is directed. The inverted relation is best described by “Parent of”.\n",
    "            . Traded on: Shares of e1 are listed on e2 (Stock exchange). The relation is directed. The inverted relation is best described by “lists”.\n",
    "\n",
    "        Processing steps:\n",
    "        \n",
    "        1. Parse the CSV-like table into header and rows.\n",
    "        2. Identify candidate named entities from text and table:\n",
    "        3. Generate candidate cross-source pairs (one from Text, one from Table).\n",
    "        4. Determine whether any relation from Possible relation types is supported by:\n",
    "            - Schema evidence from the Table (e.g., a header like \"Products\" can support Product_or_service_of between the row's entity and that cell), provided the relation is in the allowed list and direction is correct.\n",
    "        5. Keep only unambiguous, supported triplets that satisfy all constraints.\n",
    "        \n",
    "\n",
    "        Few-shot examples:\n",
    "        \n",
    "            - Example 1:\n",
    "                . Text: {texts[1]}\n",
    "                . Table:\\n{tables[1]}\n",
    "                . Output:\\n{trilets_gold[1]}\\n\\n  \n",
    "        \n",
    "            - Example 2:\n",
    "                . Text: {texts[4]}\n",
    "                . Table:\\n{tables[4]}\n",
    "                . Output:\\n{trilets_gold[4]} \\n\\n               \n",
    "            - Example 3:\n",
    "                . Text: {texts[5]}\n",
    "                . Table:\\n{tables[5]}\n",
    "                . Output: NO_RELATION\\n\\n  \n",
    "                \n",
    "        Now process the new data\n",
    "        \n",
    "        - Text: {v[0]}\n",
    "        - Table: {table_to_csvlike(v[1][0])}\n",
    "        - Output : \n",
    "        \n",
    "        Task Analyze the Text and the Table together and extract all valid cross-source relation triplets. Return only the triplets in the exact required format, or NO_RELATION if none. \"\"\"\n",
    "    if num_prompt == 2:\n",
    "        Prompt = f\"\"\"  \n",
    "            You are an NLP expert specializing in relation extraction. Extract cross-source relation triplets (entity1, relation, entity2)\n",
    "        by combining evidence from a text passage and a table.\n",
    "        \n",
    "        Task:\n",
    "        \n",
    "        - Identify relations where at least one entity is sourced from the text and at least one from the table.\n",
    "        - Construct valid triplets that use only relations from the provided list.\n",
    "        - Output only the triplets in the exact format specified below. If none are valid, output \"NO_RELATION\"\n",
    "        \n",
    "        Definitions and constraints:\n",
    "    \n",
    "        - Named entities: proper-noun phrases referring to real-world entities (e.g., persons, organizations, companies, locations, products). Do not use generic terms or common nouns.\n",
    "        - Entity sourcing:\n",
    "            . Text entities must be extracted from the text span.\n",
    "            . Table entities must be cell values from the table.\n",
    "            . Each triplet must include at least one entity sourced from the text and at least one sourced from the table.\n",
    "        - Relation validity:\n",
    "        - Use only relations listed in Allowed relation types\n",
    "        \n",
    "        Output format (strict)\n",
    "        \n",
    "        - If at least one valid triplet exists, output them on a single line using this exact pattern: entity1, entity2: relation | entity3, entity4: relation | ...\n",
    "        - No extra text, and no newline.\n",
    "        - If no valid triplet exists, output exactly: \"NO_RELATION\"\n",
    "        \n",
    "\n",
    "        Few-shot examples\n",
    "        \n",
    "        Example 1\n",
    "                . Text: \n",
    "                . Table:\n",
    "                . Allowed relation types: [{relations}]\n",
    "                . Output:\n",
    "        Example 2\n",
    "                . Text: \n",
    "                . Table:\n",
    "                . Allowed relation types: [{relations}]\n",
    "                . Output:\n",
    "        Example 3\n",
    "                . Text: \n",
    "                . Table:\n",
    "                . Allowed relation types: [{relations}]\n",
    "                . Output:\n",
    "        Example 4 (no relation)\n",
    "                . Text: \n",
    "                . Table:\n",
    "                . Allowed relation types: [{relations}]\n",
    "                . Output:\n",
    "                \n",
    "        New data\n",
    "        \n",
    "        - Text: {{text}}\n",
    "        - Table (CSV-like): {{table_csv}}\n",
    "        - Allowed relation types: [{relations}]\n",
    "        - Output:\n",
    "        Return only the triplets.\n",
    "            \"\"\"\n",
    "\n",
    "    if num_prompt == 3: # à utiliser avec le modèle LLaMA\n",
    "        Prompt = f\"\"\"\n",
    "          <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "            You are an NLP expert specializing in cross-source relation extraction. Your goal is to output only valid relation triplets (entity1, relation, entity2) that are jointly supported by a free-text passage and a tabular dataset.\n",
    "            \n",
    "            Inputs\n",
    "            - Text\n",
    "            - Table (CSV-like string)\n",
    "            - Allowed relation types: [relations]\n",
    "            \n",
    "            Core requirements\n",
    "            - Named entities only: entity1 and entity2 must be proper-noun entities (persons, organizations, companies, locations, products). Do not use generic/common nouns (e.g., \"company\", \"city\") or pure numbers/dates unless they are part of a named entity.\n",
    "            - Cross-source constraint: each triplet must include at least one entity sourced from the text and at least one entity sourced from the table. If both entities appear in both sources, designate one as text-sourced and the other as table-sourced to satisfy the constraint.\n",
    "            - Relation validity: use only labels from \"Possible relation types\". Respect semantic direction implied by the label (e.g., founded_by: subject=organization/company, object=person).\n",
    "            - Evidence agreement: a triplet is valid only if (a) the text explicitly states or strongly implies the relation between the same two entities, and (b) the table contains those entities in the same row (across any columns). Table headers are not entities.\n",
    "            - Matching and canonicalization:\n",
    "              - Parse the first row as headers; subsequent rows are records.\n",
    "              - Consider entity pairs formed within the same row across columns; do not form pairs using headers.\n",
    "              - Match entities case-insensitively and after trimming whitespace.\n",
    "              - When an entity appears in both sources, prefer the table cell’s spelling for output; otherwise, use the text surface form.\n",
    "            - De-duplication and ordering: output each unique (entity1, relation, entity2) once. Sort triplets by relation, then entity1, then entity2 (case-insensitive) for deterministic output.\n",
    "            \n",
    "            Output format (strict)\n",
    "            - If at least one valid triplet exists, output them on a single line:\n",
    "              entity1, entity2: relation | entity3, entity4: relation | ...\n",
    "            - Use \", \" between entities, \": \" before the relation, and \" | \" between triplets.\n",
    "            - No trailing separator, no extra text, and no newline.\n",
    "            - Escaping: if an entity contains a comma, colon, or pipe, wrap it in double quotes and escape embedded quotes by doubling them (e.g., \"ACME, Inc.\").\n",
    "            - If no valid triplet exists, output exactly: NO_RELATION\n",
    "            \n",
    "            Procedure\n",
    "            1) Extract named entities from the text.\n",
    "            2) Parse the CSV-like table, collect cell values from each data row (ignore headers), and form candidate entity pairs within each row across columns.\n",
    "            3) For each candidate pair, check whether the text expresses one of the allowed relations between the same two entities; assign the correct label and direction.\n",
    "            4) Canonicalize entity strings, remove duplicates, sort (relation, entity1, entity2), and output in the strict format.\n",
    "            \n",
    "            Few-shot examples\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            \\nExample 1:\n",
    "            Text: \n",
    "            Table: \n",
    "            Relations: \n",
    "            Output: \n",
    "            \n",
    "            Example 2:\n",
    "            Text: \n",
    "            Table: \n",
    "            Relations: \n",
    "            Output: \n",
    "            \n",
    "            \\nExample 3:\\n\n",
    "            Text: \n",
    "            Table: \n",
    "            Relations: \n",
    "            Output: \n",
    "            \n",
    "            Example 4 (no relation):\n",
    "            Text: \n",
    "            Table: \n",
    "            Relations: \n",
    "            Output: NO_RELATION\n",
    "            <|eot_id|>\n",
    "            \n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            Text: {v[0]}\n",
    "            Table: {table_to_csvlike(v[1][0])}\n",
    "            Possible relation types: [{relations}]\n",
    "            \n",
    "            Return only the triplets in the strict format.\n",
    "            <|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\"\n",
    "    return Prompt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f74793a8-e683-497a-bce6-6444de232e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b2637-9b7c-4383-a628-52ce4d3768e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47dc590-26c3-496f-892d-e3e2c3dc8b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81393552-eb59-4151-99f6-6c978ce71641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c06d95e4-cd90-4303-a4ae-1530fa604b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_instructions=[]\n",
    "for i, v in list(d.items()):\n",
    "    prompt = recupere_prompt(1, i)\n",
    "    demo_instructions.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "082455ed-8753-4d26-91cf-ed919ad3d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(demo_instructions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2ec224c-5ca9-4c22-86fb-ece02369186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultatsF = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaed9f0-351c-4710-8ea7-ede889f9bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats = []\n",
    "for inst in range(0,100):\n",
    "    reponse = complete_and_print(demo_instructions[inst])\n",
    "    resultats.append(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4636365-3b08-4a70-86e1-3759ddded662",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultatsF = resultatsF + resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27e3b10e-c9fb-4597-9901-2e9bcf7c50d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(resultatsF[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9518dae-dbad-4054-83ab-486836a84e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resultatsF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b99a1eeb-225d-4af2-8e36-826f435109d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_result = pd.read_csv(\"resultsfs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fb3017a-d020-4898-925f-4d9d98756b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_result= few_shot_result[\"0\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "528d8034-1ab0-469d-a4ba-40361245a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_result = zero_shot_result + resultatsF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbdb5a9-7254-4ffd-92ac-264667e65c04",
   "metadata": {},
   "source": [
    "#### Post-traitement (cas DeepSeek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b96beb6-2b3b-4041-978d-0d2066f7ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#few_shot_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c72271-ff19-4375-8a07-8e541fac3c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8479ea5e-9539-4dbf-82e4-2217afc6889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_output(text):\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79d7c221-ccc9-4b82-9aee-83f19af1497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_resultF = []\n",
    "for output in few_shot_result:\n",
    "    cleaned = clean_output(output)\n",
    "    few_shot_resultF.append(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac964f7b-2b19-427b-9c9b-11dff6191bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GV Films; Film production Film distribution; product_or_service_of | GV Films; Motion pictures (Tamil); product_or_service_of | Sujatha Films; Film production Film distribution; product_or_service_of | Sujatha Films; Motion pictures (Tamil); product_or_service_of | Mani Ratnam; G. Venkateswaran; collaboration | Mahesh Manjrekar; G. Venkateswaran; collaboration | Sanjay Dutt; G. Venkateswaran; collaboration | Kasthuri Shankar; G. Venkateswaran; collaboration | Urchagam; Film production Film distribution; product_or_service_of | Urchagam; Motion pictures (Tamil); product_or_service_of | Kaivantha Kalai; Film production Film distribution; product_or_service_of | Kaivantha Kalai; Motion pictures (Tamil); product_or_service_of | Thirudi; Film production Film distribution; product_or_service_of | Thirudi; Motion pictures (Tamil); product_or_service_of',\n",
       " 'Spanfeller Media Group ; Tribune Publishing ; subsidiary_of']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_resultF[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5bab6-f792-4d6c-a560-f92ae190e720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "134fa5e7-4e16-4662-b249-00c2ca270a00",
   "metadata": {},
   "source": [
    "## Preprocessing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4587e2ab-ecf1-42ab-b828-80b0424e48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_resultF = [x.replace(\", \", \"; \").replace(\": \", \"; \") for x in few_shot_resultF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89e03d11-93e9-4079-9bcb-84ff9fe49618",
   "metadata": {},
   "outputs": [],
   "source": [
    "trilets_gold = []\n",
    "for k, v in list(d.items()):\n",
    "    trilets_gold.append(v[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541710b-205c-44a1-b0b2-7e8007f43124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c52388e8-e2aa-4995-b639-a8eee993f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    if not text.strip(): \n",
    "        return triplets\n",
    "    \n",
    "    parts = re.split(r\"\\||\\n\", text)\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        if \";\" in part:\n",
    "            elems = [p.strip() for p in part.split(\";\")]\n",
    "            if len(elems) == 3 and elems[1] in relations or elems[2] in relations:\n",
    "                if elems[1] in relations:\n",
    "                    triplets.append(f\"\"\"{elems[0].lower().replace(\" \",\"\")}; {elems[1].lower().replace(\" \",\"\")}; {elems[2].lower().replace(\" \",\"\")}\"\"\")\n",
    "                elif elems[2] in relations:\n",
    "                    triplets.append(f\"\"\"{elems[0].lower().replace(\" \",\"\")}; {elems[2].lower().replace(\" \",\"\")}; {elems[1].lower().replace(\" \",\"\")}\"\"\")\n",
    "            continue\n",
    "        \n",
    "        m = re.match(r\"(.+?),\\s*(.+?):\\s*(\\w+)\", part)\n",
    "        if m:\n",
    "            e1, e2, rel = m.groups()\n",
    "            if rel in relations:\n",
    "                triplets.append(f\"\"\"{e1.strip().lower().replace(\" \",\"\")}; {rel.strip().lower().replace(\" \",\"\")}; {e2.strip().lower().replace(\" \",\"\")}\"\"\")\n",
    "    return triplets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edff96c3-2013-4641-a63d-4edd6bdfca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_results = [extract_triplets(el) for el in few_shot_resultF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1b5e691-add7-4d9d-9955-4d12c656e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trilets_gold = [extract_triplets(el) for el in trilets_gold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8414de51-26c0-4cb1-8545-f3c365490773",
   "metadata": {},
   "outputs": [],
   "source": [
    "trilets_gold = trilets_gold[:148]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7de30f4-4dda-4f68-950d-50cbb31ae34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], ['tribunepublishing; subsidiary_of; spanfellermediagroup(smg)']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trilets_gold[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627d16d-0ab1-4b25-bbb3-1c46088e9bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3437b0-90b0-4d92-9608-321b2e2167ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "154fcde3-8e46-46b3-9946-f2a7453b5433",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a00eda00-0e31-4251-a935-e53c20ef541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation_triplets_modified import evaluation_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca5c8591-e24e-460f-9e9f-e8f63ae97c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluation_triplets(trilets_gold, few_shot_results, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8af99f5f-007a-4cac-b97f-866d944aebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Exact matching': {'precision': 0.2105543710021322, 'recall': 0.710431654676259, 'f1': 0.3248355263157895}, 'Partial matching (head+tail)': {'precision': 0.2601279317697228, 'recall': 0.8776978417266187, 'f1': 0.40131578947368424}, 'Partial matching (relation + 1 entity)': {'precision': 0.4957356076759062, 'recall': 1.6726618705035972, 'f1': 0.7648026315789473}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "580d6480-410f-4027-bf97-22fd82da5f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['gvfilms; product_or_service_of; entertainment',\n",
       "  'gvfilms; product_or_service_of; motionpictures(tamil)',\n",
       "  'motionpictures(tamil); product_or_service_of; gvfilms'],\n",
       " ['spanfellermediagroup; subsidiary_of; tribunepublishing',\n",
       "  'tribunepublishing; acquired_by; spanfellermediagroup'],\n",
       " ['advansbanquecongo; subsidiary_of; advanssa',\n",
       "  'advansbanque; brand_of; advansbanquecongo',\n",
       "  'kfwentwicklungsbank; shareholder_of; advansbanquecongo',\n",
       "  'africandevelopmentbank; shareholder_of; advansbanquecongo',\n",
       "  'internationalfinancecorporation; shareholder_of; advansbanquecongo',\n",
       "  'horusdevelopmentfinance; shareholder_of; advansbanquecongo'],\n",
       " ['archipelagofilms; product_or_service_of; filmandtelevision',\n",
       "  'andrewyoung; collaboration; archipelagofilms',\n",
       "  'susantodd; collaboration; archipelagofilms'],\n",
       " ['seventhchannel; collaboration; starvijay',\n",
       "  'seventhchannel; product_or_service_of; mundhinampaartheney',\n",
       "  'seventhchannel; product_or_service_of; vithagan',\n",
       "  'magizhthirumeni; product_or_service_of; mundhinampaartheney',\n",
       "  'r.parthiban; product_or_service_of; vithagan',\n",
       "  'seventhchannel; product_or_service_of; mahabharat']]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0df9e092-737a-4748-a39f-0bd7f2200900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['tribunepublishing; subsidiary_of; spanfellermediagroup(smg)'],\n",
       " [],\n",
       " [],\n",
       " ['maanbumigumaanavan; product_or_service_of; seventhchannel',\n",
       "  'seenu; product_or_service_of; seventhchannel',\n",
       "  'vettaiyaaduvilaiyaadu; product_or_service_of; seventhchannel',\n",
       "  'vithagan; product_or_service_of; seventhchannel',\n",
       "  'indiralohathilnaazhagappan; product_or_service_of; seventhchannel',\n",
       "  'mundhinampaartheney; product_or_service_of; seventhchannel',\n",
       "  'pudhiyathendral; product_or_service_of; seventhchannel',\n",
       "  'coolie; product_or_service_of; seventhchannel']]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trilets_gold[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d9d1ffd7-c241-4278-9397-a77beb843f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact matching: {'precision': 0.11205976520811099, 'recall': 0.17781541066892464, 'f1': 0.13747954173486088}\n",
      "Partial matching (head+tail): {'precision': 0.3489861259338314, 'recall': 0.5537679932260796, 'f1': 0.42815057283142394}\n",
      "Partial matching (relation + 1 entity): {'precision': 0.29562433297758806, 'recall': 0.4690939881456393, 'f1': 0.3626841243862521}\n"
     ]
    }
   ],
   "source": [
    "'precision': 0.4957356076759062, 'recall': 0.92726618705035972,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b6e62-ec93-4bb3-9596-f1008ec3e099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2b636-d905-478d-8803-329f5aab69cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chargement du modèle meta-llama/Llama-4-Scout-17B-16E-Instruct depuis Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 14:35:55.261260: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-05 14:35:55.276977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759667755.297976  159196 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759667755.304017  159196 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-05 14:35:55.319164: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Fetching 50 files: 100%|██████████| 50/50 [00:00<00:00, 16262.03it/s]\n",
      "Loading checkpoint shards:  74%|███████▍  | 37/50 [04:17<01:29,  6.85s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Dict, Any\n",
    "from huggingface_hub import login \n",
    "login(\"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\")\n",
    "# 📦 Répertoire de cache pour les modèles\n",
    "CACHE_DIR = \"/projects/melodi/mettaleb/huggingface_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_model(model_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Charge le modèle et son tokenizer à partir de Hugging Face.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Chargement du modèle {model_name} depuis Hugging Face...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "\n",
    "    print(f\"[INFO] Modèle {model_name} chargé avec succès ✅\")\n",
    "    return {\"tokenizer\": tokenizer, \"model\": model}\n",
    "\n",
    "\n",
    "def generate_response(model: Dict[str, Any], prompt: str,\n",
    "                      max_length: int = 200,\n",
    "                      num_beams: int = 4,\n",
    "                      no_repeat_ngram_size: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Génère une réponse à partir du modèle donné et du prompt fourni.\n",
    "    \"\"\"\n",
    "    tokenizer = model[\"tokenizer\"]\n",
    "    model = model[\"model\"]\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Exécute la pipeline complète : chargement du modèle, génération et affichage du texte.\n",
    "    \"\"\"\n",
    "    model_name = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"  # 💡 Tu peux remplacer par ton modèle\n",
    "    prompt = \"Explique les avantages du deep learning dans le domaine médical.\"\n",
    "\n",
    "    # 1️⃣ Charger le modèle\n",
    "    model = load_model(model_name)\n",
    "\n",
    "    # 2️⃣ Générer une réponse\n",
    "    print(\"[INFO] Génération de texte...\")\n",
    "    response = generate_response(model, prompt, max_length=150)\n",
    "\n",
    "    # 3️⃣ Afficher le résultat\n",
    "    print(\"\\n=== RÉPONSE DU MODÈLE ===\\n\")\n",
    "    print(response)\n",
    "    print(\"\\n==========================\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fbe785-bc2d-4838-8c08-61391df60dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
