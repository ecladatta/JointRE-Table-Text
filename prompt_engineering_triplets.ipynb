{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1029b029-4c8b-4740-8a71-b8a8fb3ac17c",
   "metadata": {},
   "source": [
    "# 1. bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac6c5a4-6878-4bd7-94c6-7dd3159fe4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login \n",
    "login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721ebe6e-bd29-4264-809a-f9de7153f2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 10:46:16.378132: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759999576.401091 1434550 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759999576.408243 1434550 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-09 10:46:16.431909: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "from groq import Groq\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a4f765-3d0b-488c-a78a-db979645a559",
   "metadata": {},
   "source": [
    "# 2.Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5345e2a2-f229-4841-a0fd-90459e5b89bf",
   "metadata": {},
   "source": [
    "Ce script charge le corpus JSON brut, extrait les textes, les tables et les triplets d’annotations, puis les organise dans une structure unifiée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a923e8-3528-4c5d-b2cb-af577a38b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/projects/melodi/mettaleb/Annotation/corpus_challenge/test/F2_nous.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "d = {}\n",
    "\n",
    "important_keys = ['Company type', 'Industry', 'Founded', 'Founder', 'Headquarters']\n",
    "\n",
    "for idx, doc in enumerate(data.get(\"documents\", [])):\n",
    "    texts = []\n",
    "    extraction_meta = doc.get(\"raw\", {}).get(\"_source\", {}).get(\"extractionMetadata\", [])\n",
    "    for meta in extraction_meta:\n",
    "        for t in meta.get(\"texts\", []):\n",
    "            texts.append(t.get(\"value\", \"\"))\n",
    "    texts = \" \".join(texts).strip()\n",
    "\n",
    "    tables = []\n",
    "    for meta in extraction_meta:\n",
    "        for tbl in meta.get(\"tables\", []):\n",
    "            table_data = tbl.get(\"tableData\", [])\n",
    "            cond1 = all(len(row) == 2 for row in table_data)\n",
    "            cond2 = len(table_data) > 0 and table_data[0] == ['0', '1']\n",
    "            cond3 = any(row[0] in important_keys for row in table_data[1:])\n",
    "\n",
    "            if cond1 and cond2 and cond3:\n",
    "                headers = [row[0] for row in table_data[1:]]\n",
    "                values = [row[1] for row in table_data[1:]]\n",
    "                new_table = {\"tableData\": [headers, values]}\n",
    "                tables.append(new_table)\n",
    "            else:\n",
    "                tables.append({\"tableData\": table_data})\n",
    "\n",
    "    triplets_list = []\n",
    "    for ann in doc.get(\"annotations\", []):\n",
    "        subj = ann.get(\"subject\", {}).get(\"annotationValue\", \"\")\n",
    "        obj = ann.get(\"object\", {}).get(\"annotationValue\", \"\")\n",
    "        pred_val = ann.get(\"predicate\", {}).get(\"entityValue\", \"\")\n",
    "        if pred_val.lower() == \"pertinence\":\n",
    "            continue\n",
    "        triplet_str = f\"{subj} ; {obj} ; {pred_val}\"\n",
    "        triplets_list.append(triplet_str)\n",
    "\n",
    "    triplets = \" | \".join(triplets_list)\n",
    "\n",
    "    d[idx] = [texts, tables, triplets]\n",
    "\n",
    "#for k, v in list(d.items())[:5]:\n",
    "#    print(f\"Doc {k}:\")\n",
    " #   print(\"  Text:\", v[0][:100], \"...\")\n",
    "  #  print(\"  Nb tables:\", len(v[1]))\n",
    "   # for t in v[1]:\n",
    "    #    print(\"  Table:\", t)\n",
    "    #print(\"  Triplets:\", v[2])\n",
    "   # print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6c036-a544-4b09-abb8-46f71ca845dc",
   "metadata": {},
   "source": [
    "#### Convertit une table au format structurée CSV-like (string).\n",
    "Transforme une table extraite du corpus en un format lisible de type CSV, en normalisant les en-têtes et les lignes afin de faciliter son intégration dans les prompts des LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ca0171-d1d2-40b2-ba4d-8955ccb98e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_csvlike(table_dict):\n",
    "    table = table_dict.get(\"tableData\", [])\n",
    "    if not table:\n",
    "        return \"\"\n",
    "    headers = table[0]\n",
    "    headers = [h.strip() if h.strip() else f\"Col{i+1}\" for i, h in enumerate(headers)]\n",
    "    rows = table[1:]\n",
    "    csv_lines = [\" | \".join(headers)]\n",
    "    for row in rows:\n",
    "        row_extended = row + [\"\"] * (len(headers) - len(row))\n",
    "        csv_lines.append(\" | \".join(row_extended))\n",
    "    \n",
    "    return \"\\n\".join(csv_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21481611-d417-4045-8b7c-153425b3c5ee",
   "metadata": {},
   "source": [
    "## Liste des relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a583558a-3031-4f19-a77d-e9bc1fa3fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = ['acquired_by','brand_of', 'client_of', 'collaboration', 'competitor_of', 'merged_with', 'product_or_service_of', 'regulated_by', 'shareholder_of', 'subsidiary_of', 'traded_on']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b194c9-a71b-42da-a76d-2b1606089a55",
   "metadata": {},
   "source": [
    "# 3.Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47443fdd-e168-4b0a-b066-6cdebde1111c",
   "metadata": {},
   "source": [
    "Ce module définit les fonctions nécessaires pour interagir avec différents modèles de langage (en utilisant Groq ou Hugging Face), en configurant les appels API, les paramètres de génération (température, top-p, etc.) et les formats de messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17de2b-62e3-4f18-90ef-55410fb67626",
   "metadata": {},
   "source": [
    "## Avec Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1cc4cff6-6804-4daf-8035-c22be773dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "caffdae3-9c0d-4562-9029-05b348b964eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key= api_key)\n",
    "#DEFAULT_MODEL = \"llama-3.3-70b-versatile\"\n",
    "DEFAULT_MODEL = \"deepseek-r1-distill-llama-70b\"\n",
    "#DEFAULT_MODEL = \"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
    "def assistant(content: str):\n",
    "    return { \"role\": \"assistant\", \"content\": content }\n",
    "\n",
    "def user(content: str):\n",
    "    return { \"role\": \"user\", \"content\": content }\n",
    "\n",
    "def chat_completion(\n",
    "    messages: List[Dict],\n",
    "    model = DEFAULT_MODEL,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.2,\n",
    ") -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "        \n",
    "\n",
    "def completion(\n",
    "    prompt: str,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.2,\n",
    ") -> str:\n",
    "    return chat_completion(\n",
    "        [user(prompt)],\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "def complete_and_print(prompt: str, model: str = DEFAULT_MODEL):\n",
    "    #print(f'==============\\n{prompt}\\n==============')\n",
    "    response = completion(prompt, model)\n",
    "    #print(response, end='\\n\\n')\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be4b22-b582-4418-b008-3c7f91912cf0",
   "metadata": {},
   "source": [
    "## Avec HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e524a5f-61bd-4bb5-a707-03393406a687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     11\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     12\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(DEFAULT_MODEL, cache_dir\u001b[38;5;241m=\u001b[39mCACHE_DIR)\n\u001b[0;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDEFAULT_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCACHE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     29\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmatmul\u001b[38;5;241m.\u001b[39mallow_tf32 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:288\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:5157\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5155\u001b[0m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[1;32m   5156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 5157\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5159\u001b[0m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[1;32m   5160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:1473\u001b[0m, in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1470\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1473\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1476\u001b[0m     tied_params \u001b[38;5;241m=\u001b[39m find_tied_parameters(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:117\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "#DEFAULT_MODEL = \"deepseek-ai/DeepSeek-V3.2-Exp\"\n",
    "#DEFAULT_MODEL = \"deepseek-ai/DeepSeek-R1\"\n",
    "DEFAULT_MODEL = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
    "#DEFAULT_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "#DEFAULT_MODEL = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "\n",
    "CACHE_DIR = \"/projects/melodi/mettaleb/huggingface_cache\"\n",
    "\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, cache_dir=CACHE_DIR)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ").eval()\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ab6de-0323-4b64-b5fa-d58220b9c833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6241f77-04f5-48ef-8f1b-66b6a9959dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "def assistant(content: str) -> Dict:\n",
    "    return {\"role\": \"assistant\", \"content\": content}\n",
    "\n",
    "def user(content: str) -> Dict:\n",
    "    return {\"role\": \"user\", \"content\": content}\n",
    "\n",
    "\n",
    "def chat_completion(\n",
    "    messages: List[Dict],\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.2,\n",
    ") -> str:\n",
    "    \n",
    "    prompt = \"\"\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        content = msg[\"content\"]\n",
    "        prompt += f\"{role.upper()}:\\n{content}\\n\\n\"\n",
    "\n",
    "    output = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    return output[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "def completion(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.1\n",
    ") -> str:\n",
    "    return chat_completion([user(prompt)], max_new_tokens, temperature, top_p)\n",
    "\n",
    "def complete_and_print(prompt: str):\n",
    "    response = completion(prompt)\n",
    "    print(response)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c961d99-cb3a-4cc4-a712-feb509eb2cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1911ee5-badc-4baf-85d8-417987a447e5",
   "metadata": {},
   "source": [
    "## 3.1 Zero-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb98add4-ae59-46e9-8376-86467694b4d5",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df8e8e00-fe68-43cb-ba3f-cf8fd3469710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupere_prompt(num_prompt, i):\n",
    "    if num_prompt == 1:\n",
    "        Prompt = f\"\"\"  \n",
    "        You are an expert in Natural Language Processing (NLP) specializing in relation extraction.  \n",
    "        Your task is to extract relations expressed strictly as triplets: (entity1, relation, entity2).  \n",
    "\n",
    "        Constraints:\n",
    "        - Both entity1 and entity2 must be valid named entities.  \n",
    "        - At least one entity must come from the text and the other from the table.  \n",
    "        - Only keep relations that are explicitly listed in the provided \"Possible relation types\".  \n",
    "        - Ignore any relation not in this list.  \n",
    "        - The extracted triplets must reflect connections valid, only when combining both sources (text + table), not when taken in isolation.  \n",
    "        - If no valid triplet exists, return \"NO_RELATION\".  \n",
    "        - Output must contain only the triplets in the required format. Do not include explanations, reasoning, or extra text.  \n",
    "\n",
    "        Output Format (strict):  \n",
    "        entity1, entity2: relation1 | entity3, entity4: relation2 | ...  \n",
    "\n",
    "        \n",
    "        Relation direction and semantics:\n",
    "        \n",
    "        - Use the conventional direction implied by the relation label. Examples if present in Possible relation types:\n",
    "            . Acquired_by: e2 purchases controlling stake in e1. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Brand of: e2 offers products or services of e1 (Brand). The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Client of: e1 uses (and presumably pays for) products or services offered by e2. The relation is directed. The inverted relation is best described by “Supplier of”.\n",
    "            . Collaboration: e1 and e2 collaborate in (parts of their) business activities. The relation is undirected.\n",
    "            . Competitor of: e1 competes for resources with e2. The relation is undirected.\n",
    "            . Merged with: e1 and e2 merged (parts of) their business activities. The relation is undirected.\n",
    "            . Product or service of: e1 is offered for commercial distribution by e2. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Regulated by: e2 regulates (parts of) the business activity of e1. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Shareholder of: e1 owns shares in e2. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Subsidiary of: e2 legally owns e1. The relation is directed. The inverted relation is best described by “Parent of”.\n",
    "            . Traded on: Shares of e1 are listed on e2 (Stock exchange). The relation is directed. The inverted relation is best described by “lists”.\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        Available Data:  \n",
    "        - Text segment: {v[0]}  \n",
    "        - Table content:\\n {table_to_csvlike(v[1][0])}  \n",
    "        - Possible relation types: [{relations}]  \n",
    "\n",
    "        Your task:  \n",
    "        1. Identify relations where one entity is in the text and the other in the table.  \n",
    "        2. Keep only relations that match the provided list. \n",
    "\"\"\"\n",
    "\n",
    "    if num_prompt == 2: # à utiliser avec le modèle LLaMA\n",
    "        Prompt = f\"\"\"\n",
    "                <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "                As a Natural Language Processing (NLP) expert specializing in relation extraction.\n",
    "        Your task is to identify and extract valid relations expressed as triplets (entity1, relation, entity2) \n",
    "        from both a given text segment and a table content.\n",
    "        \n",
    "        Constraints:\n",
    "        - Both entity1 and entity2 must be valid named entities.\n",
    "        - Only extract relations where one entity is from the text and the other is from the table.\n",
    "        - Only use relations that are explicitly listed in the provided \"Possible relation types\".\n",
    "        - If no valid relation exists, return \"NO_RELATION\".\n",
    "        - The output must **only** contain the extracted triplets in the requested format, with no explanations, reasoning, or extra text.\n",
    "        \n",
    "        Output Format:\n",
    "        entity1, entity2: relation1 | entity3, entity4: relation2 | ...\n",
    "        \n",
    "        <|eot_id|>\n",
    "        \n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        You are provided with:\n",
    "        \n",
    "        - Text segment: {v[0]}\n",
    "        - Table content:\\n {table_to_csvlike(v[1][0])}\n",
    "        - Possible relation types: [{relations}]\n",
    "        \n",
    "        Your task:\n",
    "        1. Identify relations where at least one entity is in the text and the other in the table.\n",
    "        2. Construct relation triplets combining entities from both sources.\n",
    "        3. Only keep relations that match the provided list of relation types.\n",
    "        4. Return the result strictly in the format: entity1, entity2: relation1 | entity3, entity4: relation2\n",
    "        \n",
    "        If no valid relation exists, return \"NO_RELATION\".\n",
    "        \n",
    "        <|eot_id|>\n",
    "        \n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\"\n",
    "    return Prompt\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c314c8-cad4-4ab2-b526-fe83313e6a56",
   "metadata": {},
   "source": [
    "## 3.2 Few-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dad2b0a-2108-47fd-8259-3666f98487c7",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ca2b3ff-20c7-491b-b568-35ad16ce62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trilets_gold = []\n",
    "texts = []\n",
    "tables = []\n",
    "for k, v in list(d.items()):\n",
    "    trilets_gold.append(v[2])\n",
    "    texts.append(v[0])\n",
    "    tables.append(table_to_csvlike(v[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e22c5c-6651-439e-8074-b9125d8ec479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae7cc1-455b-465e-8437-a2ee9b53204e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "856005bb-6a64-4470-af75-277662c005b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupere_prompt(num_prompt, i):\n",
    "    if num_prompt == 1:\n",
    "        Prompt = f\"\"\"  \n",
    "        You are an expert in Natural Language Processing (NLP) specializing in relation extraction.  \n",
    "        Your task is to extract relations expressed strictly as triplets: (entity1, relation, entity2).  \n",
    "\n",
    "        * Constraints:\n",
    "        - Both entity1 and entity2 must be valid named entities.  \n",
    "        - At least one entity must come from the text and the other from the table.  \n",
    "        - Only keep relations that are explicitly listed in the provided \"Possible relation types\".  \n",
    "        - Ignore any relation not in this list.  \n",
    "        - The extracted triplets must reflect connections valid, only when combining both sources (text + table), not when taken in isolation.  \n",
    "        - If no valid triplet exists, return \"NO_RELATION\".  \n",
    "        - Output must contain only the triplets in the required format. Do not include explanations, reasoning, or extra text.  \n",
    "\n",
    "        * Output Format (strict):  \n",
    "            entity1, entity2: relation1 | entity3, entity4: relation2 | ...  \n",
    "\n",
    "        \n",
    "        * Relation direction and semantics:\n",
    "        \n",
    "            - Use the conventional direction implied by the relation label. Examples if present in Possible relation types:\n",
    "                . Acquired_by: e2 purchases controlling stake in e1. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "                . Brand of: e2 offers products or services of e1 (Brand). The relation is directed. The inverted relation is best described by the same relation type.\n",
    "                . Client of: e1 uses (and presumably pays for) products or services offered by e2. The relation is directed. The inverted relation is best described by “Supplier of”.\n",
    "                . Collaboration: e1 and e2 collaborate in (parts of their) business activities. The relation is undirected.\n",
    "                . Competitor of: e1 competes for resources with e2. The relation is undirected.\n",
    "                . Merged with: e1 and e2 merged (parts of) their business activities. The relation is undirected.\n",
    "                . Product or service of: e1 is offered for commercial distribution by e2. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "                . Regulated by: e2 regulates (parts of) the business activity of e1. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "                . Shareholder of: e1 owns shares in e2. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "                . Subsidiary of: e2 legally owns e1. The relation is directed. The inverted relation is best described by “Parent of”.\n",
    "                . Traded on: Shares of e1 are listed on e2 (Stock exchange). The relation is directed. The inverted relation is best described by “lists”.\n",
    "            \n",
    "        \n",
    "\n",
    "        * Few-shot examples:\n",
    "        \n",
    "            - Example 1:\n",
    "                . Text: {texts[1]}\n",
    "                . Table:\\n{tables[1]}\n",
    "                . Output:\\n{trilets_gold[1]}\\n\\n  \n",
    "        \n",
    "            - Example 2:\n",
    "                . Text: {texts[4]}\n",
    "                . Table:\\n{tables[4]}\n",
    "                . Output:\\n{trilets_gold[4]} \\n\\n               \n",
    "            - Example 3:\n",
    "                . Text: {texts[5]}\n",
    "                . Table:\\n{tables[5]}\n",
    "                . Output: NO_RELATION\\n\\n  \n",
    "                \n",
    "        Now process the new data\n",
    "        \n",
    "        - Text: {v[0]}\n",
    "        - Table: {table_to_csvlike(v[1][0])}\n",
    "        - Output : \"\"\"\n",
    "        \n",
    "    if num_prompt == 2: # à utiliser avec le modèle LLaMA\n",
    "        Prompt = f\"\"\"\n",
    "          <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "            You are an NLP expert specializing in cross-source relation extraction. Your goal is to output only valid relation triplets (entity1, relation, entity2) that are jointly supported by a free-text passage and a tabular dataset.\n",
    "            \n",
    "            Inputs\n",
    "            - Text\n",
    "            - Table (CSV-like string)\n",
    "            - Allowed relation types: [relations]\n",
    "            \n",
    "            Core requirements\n",
    "            - Named entities only: entity1 and entity2 must be proper-noun entities (persons, organizations, companies, locations, products). Do not use generic/common nouns (e.g., \"company\", \"city\") or pure numbers/dates unless they are part of a named entity.\n",
    "            - Cross-source constraint: each triplet must include at least one entity sourced from the text and at least one entity sourced from the table. If both entities appear in both sources, designate one as text-sourced and the other as table-sourced to satisfy the constraint.\n",
    "            - Relation validity: use only labels from \"Possible relation types\". Respect semantic direction implied by the label (e.g., founded_by: subject=organization/company, object=person).\n",
    "            - Evidence agreement: a triplet is valid only if (a) the text explicitly states or strongly implies the relation between the same two entities, and (b) the table contains those entities in the same row (across any columns). Table headers are not entities.\n",
    "            - Matching and canonicalization:\n",
    "              - Parse the first row as headers; subsequent rows are records.\n",
    "              - Consider entity pairs formed within the same row across columns; do not form pairs using headers.\n",
    "              - Match entities case-insensitively and after trimming whitespace.\n",
    "              - When an entity appears in both sources, prefer the table cell’s spelling for output; otherwise, use the text surface form.\n",
    "            - De-duplication and ordering: output each unique (entity1, relation, entity2) once. Sort triplets by relation, then entity1, then entity2 (case-insensitive) for deterministic output.\n",
    "            \n",
    "            Output format (strict)\n",
    "            - If at least one valid triplet exists, output them on a single line:\n",
    "              entity1, entity2: relation | entity3, entity4: relation | ...\n",
    "            - Use \", \" between entities, \": \" before the relation, and \" | \" between triplets.\n",
    "            - No trailing separator, no extra text, and no newline.\n",
    "            - Escaping: if an entity contains a comma, colon, or pipe, wrap it in double quotes and escape embedded quotes by doubling them (e.g., \"ACME, Inc.\").\n",
    "            - If no valid triplet exists, output exactly: NO_RELATION\n",
    "            \n",
    "            Procedure\n",
    "            1) Extract named entities from the text.\n",
    "            2) Parse the CSV-like table, collect cell values from each data row (ignore headers), and form candidate entity pairs within each row across columns.\n",
    "            3) For each candidate pair, check whether the text expresses one of the allowed relations between the same two entities; assign the correct label and direction.\n",
    "            4) Canonicalize entity strings, remove duplicates, sort (relation, entity1, entity2), and output in the strict format.\n",
    "            \n",
    "            Few-shot examples\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            \\nExample 1:\n",
    "            Text: \n",
    "            Table: \n",
    "            Relations: \n",
    "            Output: \n",
    "            \n",
    "            Example 2:\n",
    "            Text: \n",
    "            Table: \n",
    "            Relations: \n",
    "            Output: \n",
    "            \n",
    "            \\nExample 3:\\n\n",
    "            Text: \n",
    "            Table: \n",
    "            Relations: \n",
    "            Output: \n",
    "            \n",
    "            Example 4 (no relation):\n",
    "            Text: \n",
    "            Table: \n",
    "            Relations: \n",
    "            Output: NO_RELATION\n",
    "            <|eot_id|>\n",
    "            \n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            Text: {v[0]}\n",
    "            Table: {table_to_csvlike(v[1][0])}\n",
    "            Possible relation types: [{relations}]\n",
    "            \n",
    "            Return only the triplets in the strict format.\n",
    "            <|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\"\n",
    "    return Prompt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f74793a8-e683-497a-bce6-6444de232e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b2637-9b7c-4383-a628-52ce4d3768e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b66515f1-74f1-47b0-86f4-ac4941c8ff69",
   "metadata": {},
   "source": [
    "## 3.3 Lancer les prompts sur les LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81393552-eb59-4151-99f6-6c978ce71641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c06d95e4-cd90-4303-a4ae-1530fa604b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_instructions=[]\n",
    "for i, v in list(d.items()):\n",
    "    prompt = recupere_prompt(1, i)\n",
    "    demo_instructions.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "082455ed-8753-4d26-91cf-ed919ad3d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(demo_instructions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2ec224c-5ca9-4c22-86fb-ece02369186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultatsF = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bdaed9f0-351c-4710-8ea7-ede889f9bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats = []\n",
    "for inst in range(252,255):\n",
    "    reponse = complete_and_print(demo_instructions[inst])\n",
    "    resultats.append(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f4636365-3b08-4a70-86e1-3759ddded662",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultatsF = resultatsF + resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "27e3b10e-c9fb-4597-9901-2e9bcf7c50d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resultatsF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8124298-4caa-49c9-a4f3-d9b2973b3dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ee71ca7-01de-49cb-93de-5b6c910dfd0d",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "32b7a3eb-874d-4044-b5f0-41472d238017",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(resultatsF)\n",
    "df.to_csv(\"results_few-shot_DeepSeek.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9518dae-dbad-4054-83ab-486836a84e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b99a1eeb-225d-4af2-8e36-826f435109d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_result = pd.read_csv(\"results_fewshot_Llama4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2fb3017a-d020-4898-925f-4d9d98756b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_result= few_shot_result[\"0\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "528d8034-1ab0-469d-a4ba-40361245a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_result = zero_shot_result + resultatsF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbdb5a9-7254-4ffd-92ac-264667e65c04",
   "metadata": {},
   "source": [
    "#### Post-traitement (cas DeepSeek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce1282-6504-4a7e-8b58-5ab4f42635ee",
   "metadata": {},
   "source": [
    "Ce script nettoie les réponses générées par le modèle DeepSeek en supprimant les balises de raisonnement internes (<think>...</think>) afin de ne conserver que le l'output final pertinent pour l’évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8479ea5e-9539-4dbf-82e4-2217afc6889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_output(text):\n",
    "\n",
    "    cleaned_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "79d7c221-ccc9-4b82-9aee-83f19af1497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_result = []\n",
    "for output in resultatsF:\n",
    "    cleaned = extract_final_output(output)\n",
    "    few_shot_result.append(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134fa5e7-4e16-4662-b249-00c2ca270a00",
   "metadata": {},
   "source": [
    "## Preprocessing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "20afb532-4f1a-414b-9fd5-848f981f16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ce script isole automatiquement les triplets valides (entité1 ; entité2 ; relation) à partir des réponses textuelles du modèle,\n",
    "#en filtrant les explications ou textes parasites souvent générés par les LLMs.\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_triplets(llm_output: str) -> str:\n",
    "    text = llm_output.strip()\n",
    "    pattern = r'([A-Za-z0-9\\s\\(\\)\\.\\-&]+)\\s*[,;:]\\s*([A-Za-z0-9\\s\\(\\)\\.\\-&]+)\\s*[,;:]\\s*([A-Za-z_]+)'\n",
    "\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    if not matches:\n",
    "        return \"NO_RELATION\"\n",
    "    \n",
    "    triplets = []\n",
    "    for e1, e2, rel in matches:\n",
    "        e1 = e1.strip()\n",
    "        e2 = e2.strip()\n",
    "        rel = rel.strip()\n",
    "        triplets.append(f\"{e1}; {e2}; {rel}\")\n",
    "    \n",
    "    triplets = list(dict.fromkeys(triplets))\n",
    "    return \" | \".join(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ad6571bc-a4cd-435a-b966-6f0cf3b03413",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_resultF = []\n",
    "for x in few_shot_result:\n",
    "    few_shot_resultF.append(extract_triplets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce887405-58cc-4664-b65c-75ad2224cdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587e2ab-ecf1-42ab-b828-80b0424e48bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "89e03d11-93e9-4079-9bcb-84ff9fe49618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c52388e8-e2aa-4995-b639-a8eee993f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Cette fonction uniformise la structure des triplets extraits, en harmonisant la casse et la mise en forme (entité1 ; entité2 ; relation),\n",
    "tout en filtrant les relations valides selon une liste prédéfinie.\"\"\"\n",
    "def extract_triplets_format(text):\n",
    "    triplets = []\n",
    "    \n",
    "    if not text.strip():\n",
    "        return triplets\n",
    "    \n",
    "    parts = re.split(r\"\\||\\n\", text)\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        if \";\" in part:\n",
    "            elems = [p.strip() for p in part.split(\";\")]\n",
    "            try:\n",
    "                if len(elems) == 3 and elems[1].lower() in relations or elems[2].lower() in relations:\n",
    "                    if elems[1].lower() in relations:\n",
    "                        triplets.append(f\"\"\"{elems[0].lower().replace(\" \",\"\")}; {elems[1].lower().replace(\" \",\"\")}; {elems[2].lower().replace(\" \",\"\")}\"\"\")\n",
    "                    elif elems[2].lower() in relations:\n",
    "                        triplets.append(f\"\"\"{elems[0].lower().replace(\" \",\"\")}; {elems[2].lower().replace(\" \",\"\")}; {elems[1].lower().replace(\" \",\"\")}\"\"\")\n",
    "            except:\n",
    "                triplets.append(\"x; NO_RELATION; x\")\n",
    "            continue\n",
    "        \n",
    "        m = re.match(r\"(.+?),\\s*(.+?):\\s*(\\w+)\", part)\n",
    "        if m:\n",
    "            e1, e2, rel = m.groups()\n",
    "            if rel in relations:\n",
    "                triplets.append(f\"\"\"{e1.strip().lower().replace(\" \",\"\")}; {rel.strip().lower().replace(\" \",\"\")}; {e2.strip().lower().replace(\" \",\"\")}\"\"\")\n",
    "    return triplets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "edff96c3-2013-4641-a63d-4edd6bdfca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_results = [extract_triplets_format(el) for el in few_shot_resultF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b1b5e691-add7-4d9d-9955-4d12c656e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trilets_gold = []\n",
    "for k, v in list(d.items()):\n",
    "    trilets_gold.append(v[2])\n",
    "    \n",
    "trilets_gold = [extract_triplets_format(el) for el in trilets_gold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8414de51-26c0-4cb1-8545-f3c365490773",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trilets_gold[140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "72d9312f-a5b3-403e-b126-235eea3f6cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(few_shot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154fcde3-8e46-46b3-9946-f2a7453b5433",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a00eda00-0e31-4251-a935-e53c20ef541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation_triplets import evaluation_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ca5c8591-e24e-460f-9e9f-e8f63ae97c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluation_triplets( few_shot_results,trilets_gold, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8af99f5f-007a-4cac-b97f-866d944aebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Exact matching': {'precision': 0.656934306569343, 'recall': 0.24112525117213665, 'f1': 0.35276825085742286}, 'Partial matching (head+tail)': {'precision': 0.7235401459854015, 'recall': 0.2655726724715338, 'f1': 0.3885350318471338}, 'Partial matching (relation + 1 entity)': {'precision': 0.7436131386861314, 'recall': 0.2729403884795713, 'f1': 0.3993140617344439}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d6480-410f-4027-bf97-22fd82da5f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9e092-737a-4748-a39f-0bd7f2200900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d1ffd7-c241-4278-9397-a77beb843f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b6e62-ec93-4bb3-9596-f1008ec3e099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fbe785-bc2d-4838-8c08-61391df60dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
