{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1029b029-4c8b-4740-8a71-b8a8fb3ac17c",
   "metadata": {},
   "source": [
    "# 1. bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721ebe6e-bd29-4264-809a-f9de7153f2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mettaleb/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-06 09:11:17.974364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759734678.523248 3975010 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759734678.760325 3975010 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-06 09:11:20.531862: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/mettaleb/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "from groq import Groq\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a4f765-3d0b-488c-a78a-db979645a559",
   "metadata": {},
   "source": [
    "# 2.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf4b46-2d85-4dcd-898a-3a150f50b7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a923e8-3528-4c5d-b2cb-af577a38b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/projects/melodi/mettaleb/Annotation/corpus_challenge/test/F2_nous.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "d = {}\n",
    "\n",
    "important_keys = ['Company type', 'Industry', 'Founded', 'Founder', 'Headquarters']\n",
    "\n",
    "for idx, doc in enumerate(data.get(\"documents\", [])):\n",
    "    texts = []\n",
    "    extraction_meta = doc.get(\"raw\", {}).get(\"_source\", {}).get(\"extractionMetadata\", [])\n",
    "    for meta in extraction_meta:\n",
    "        for t in meta.get(\"texts\", []):\n",
    "            texts.append(t.get(\"value\", \"\"))\n",
    "    texts = \" \".join(texts).strip()\n",
    "\n",
    "    tables = []\n",
    "    for meta in extraction_meta:\n",
    "        for tbl in meta.get(\"tables\", []):\n",
    "            table_data = tbl.get(\"tableData\", [])\n",
    "            cond1 = all(len(row) == 2 for row in table_data)\n",
    "            cond2 = len(table_data) > 0 and table_data[0] == ['0', '1']\n",
    "            cond3 = any(row[0] in important_keys for row in table_data[1:])\n",
    "\n",
    "            if cond1 and cond2 and cond3:\n",
    "                headers = [row[0] for row in table_data[1:]]\n",
    "                values = [row[1] for row in table_data[1:]]\n",
    "                new_table = {\"tableData\": [headers, values]}\n",
    "                tables.append(new_table)\n",
    "            else:\n",
    "                tables.append({\"tableData\": table_data})\n",
    "\n",
    "    triplets_list = []\n",
    "    for ann in doc.get(\"annotations\", []):\n",
    "        subj = ann.get(\"subject\", {}).get(\"annotationValue\", \"\")\n",
    "        obj = ann.get(\"object\", {}).get(\"annotationValue\", \"\")\n",
    "        pred_val = ann.get(\"predicate\", {}).get(\"entityValue\", \"\")\n",
    "        if pred_val.lower() == \"pertinence\":\n",
    "            continue\n",
    "        triplet_str = f\"{subj} ; {obj} ; {pred_val}\"\n",
    "        triplets_list.append(triplet_str)\n",
    "\n",
    "    triplets = \" | \".join(triplets_list)\n",
    "\n",
    "    d[idx] = [texts, tables, triplets]\n",
    "\n",
    "#for k, v in list(d.items())[:5]:\n",
    "#    print(f\"Doc {k}:\")\n",
    " #   print(\"  Text:\", v[0][:100], \"...\")\n",
    "  #  print(\"  Nb tables:\", len(v[1]))\n",
    "   # for t in v[1]:\n",
    "    #    print(\"  Table:\", t)\n",
    "    #print(\"  Triplets:\", v[2])\n",
    "   # print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6c036-a544-4b09-abb8-46f71ca845dc",
   "metadata": {},
   "source": [
    "#### Convertit une table au format structurée CSV-like (string).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ca0171-d1d2-40b2-ba4d-8955ccb98e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_csvlike(table_dict):\n",
    "    table = table_dict.get(\"tableData\", [])\n",
    "    if not table:\n",
    "        return \"\"\n",
    "    headers = table[0]\n",
    "    headers = [h.strip() if h.strip() else f\"Col{i+1}\" for i, h in enumerate(headers)]\n",
    "    rows = table[1:]\n",
    "    csv_lines = [\" | \".join(headers)]\n",
    "    for row in rows:\n",
    "        row_extended = row + [\"\"] * (len(headers) - len(row))\n",
    "        csv_lines.append(\" | \".join(row_extended))\n",
    "    \n",
    "    return \"\\n\".join(csv_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ffa724f-1713-4584-b119-1fde6b4cff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for k, v in list(d.items())[:5]:\n",
    "#    print()\n",
    "#    print(table_to_csvlike(v[1][0]))\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a583558a-3031-4f19-a77d-e9bc1fa3fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = ['acquired_by','brand_of', 'client_of', 'collaboration', 'competitor_of', 'merged_with', 'product_or_service_of', 'regulated_by', 'shareholder_of', 'subsidiary_of', 'traded_on']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b194c9-a71b-42da-a76d-2b1606089a55",
   "metadata": {},
   "source": [
    "# 3.Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17de2b-62e3-4f18-90ef-55410fb67626",
   "metadata": {},
   "source": [
    "## Avec Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc4cff6-6804-4daf-8035-c22be773dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caffdae3-9c0d-4562-9029-05b348b964eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key= api_key)\n",
    "#DEFAULT_MODEL = \"llama-3.3-70b-versatile\"\n",
    "#DEFAULT_MODEL = \"deepseek-r1-distill-llama-70b\"\n",
    "DEFAULT_MODEL = \"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
    "def assistant(content: str):\n",
    "    return { \"role\": \"assistant\", \"content\": content }\n",
    "\n",
    "def user(content: str):\n",
    "    return { \"role\": \"user\", \"content\": content }\n",
    "\n",
    "def chat_completion(\n",
    "    messages: List[Dict],\n",
    "    model = DEFAULT_MODEL,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.2,\n",
    ") -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "        \n",
    "\n",
    "def completion(\n",
    "    prompt: str,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.2,\n",
    ") -> str:\n",
    "    return chat_completion(\n",
    "        [user(prompt)],\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "def complete_and_print(prompt: str, model: str = DEFAULT_MODEL):\n",
    "    #print(f'==============\\n{prompt}\\n==============')\n",
    "    response = completion(prompt, model)\n",
    "    #print(response, end='\\n\\n')\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be4b22-b582-4418-b008-3c7f91912cf0",
   "metadata": {},
   "source": [
    "## Avec HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e524a5f-61bd-4bb5-a707-03393406a687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "#DEFAULT_MODEL = \"deepseek-ai/DeepSeek-V3.2-Exp\"\n",
    "#DEFAULT_MODEL = \"deepseek-ai/DeepSeek-R1\"\n",
    "DEFAULT_MODEL = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
    "#DEFAULT_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "#DEFAULT_MODEL = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "\n",
    "CACHE_DIR = \"/projects/melodi/mettaleb/huggingface_cache\"\n",
    "\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ab6de-0323-4b64-b5fa-d58220b9c833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6241f77-04f5-48ef-8f1b-66b6a9959dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "def assistant(content: str) -> Dict:\n",
    "    return {\"role\": \"assistant\", \"content\": content}\n",
    "\n",
    "def user(content: str) -> Dict:\n",
    "    return {\"role\": \"user\", \"content\": content}\n",
    "\n",
    "\n",
    "def chat_completion(\n",
    "    messages: List[Dict],\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.2,\n",
    ") -> str:\n",
    "    \n",
    "    prompt = \"\"\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        content = msg[\"content\"]\n",
    "        prompt += f\"{role.upper()}:\\n{content}\\n\\n\"\n",
    "\n",
    "    output = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    return output[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "def completion(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.1\n",
    ") -> str:\n",
    "    return chat_completion([user(prompt)], max_new_tokens, temperature, top_p)\n",
    "\n",
    "def complete_and_print(prompt: str):\n",
    "    response = completion(prompt)\n",
    "    print(response)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c961d99-cb3a-4cc4-a712-feb509eb2cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1911ee5-badc-4baf-85d8-417987a447e5",
   "metadata": {},
   "source": [
    "## 3.1 Zero-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb98add4-ae59-46e9-8376-86467694b4d5",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df8e8e00-fe68-43cb-ba3f-cf8fd3469710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupere_prompt(num_prompt, i):\n",
    "    if num_prompt == 1:\n",
    "        Prompt = f\"\"\"  \n",
    "        You are an NLP expert specializing in cross-source relation extraction between a free-text passage and a tabular dataset. Produce only valid relation triplets (entity1, relation, entity2) that connect at least one entity from the text with at least one entity from the table.\n",
    "        \n",
    "        Inputs:\n",
    "        - Text: {v[0]}\\n\n",
    "        - Table (CSV-like string):\\n {table_to_csvlike(v[1][0])}\\n\\n\n",
    "        - Allowed relation types: [relations]\n",
    "\n",
    "        \n",
    "        Global rules:\\n\n",
    "        - Output only triplets in the strict format defined below. Do not include explanations, notes, or extra text.\n",
    "        - Use only relation labels provided in \"Allowed relation types\". Ignore all other relation labels.\n",
    "        - Each triplet must include at least one text-sourced entity and at least one table-sourced entity.\n",
    "        - If no valid triplet exists, output exactly: \"NO_RELATION\"\n",
    "        \n",
    "        Entity requirements:\\n\n",
    "        - entity1 and entity2 must be named entities: proper-noun phrases referring to real-world entities. Do not use generic/common nouns (e.g., \"the company\", \"the city\").\n",
    "        - Text-sourced entities must be extracted from the text. Table-sourced entities must be taken from table cell values (not headers).\n",
    "      \n",
    "        Output format (strict):\n",
    "        \n",
    "        - If at least one valid triplet exists, output them on a single line using exactly: entity1, entity2: relation | entity3, entity4: relation | ...\n",
    "        - If none are valid, output exactly: NO_RELATION\n",
    "        \n",
    "        Return only the triplets in the strict output format.\n",
    "        \"\"\"\n",
    "    if num_prompt == 2:\n",
    "        Prompt = f\"\"\"\n",
    "        As a specialist in relation extraction, your task is to identify and output valid relational triplets from provided text and tabular data. \n",
    "        Each triplet should consist of two named entities (one from the text and one from the table) and a relation connecting them.\n",
    "        \n",
    "        - Follow these detailed guidelines:\n",
    "        \n",
    "            - Triplets Formation: \n",
    "                . Each triplet should link one entity from the text ('entity1') with one entity from the table ('entity2').\n",
    "                . Entity Requirements: Ensure both 'entity1' and 'entity2' are named entities.\n",
    "                . Relation Types: Use only the relation types listed in the provided 'Possible relation types'.\n",
    "                . Exclude any relations not listed.\n",
    "                . Contextual Validity: Validate relations based on the combined context of the text and the table. Ignore relations that are valid only in isolation.\n",
    "                . No Relation Scenario: If no valid relations are found, return 'NO_RELATION'.\n",
    "                . Output Format: Present your findings as a series of triplets in the format: 'entity1, entity2: relation1 | entity3, entity4: relation2 | ...', without additional text or explanations.\n",
    "                \n",
    "            - Data Inputs:\n",
    "                - Text: {v[0]}\\n  \n",
    "                - Table:\\n {table_to_csvlike(v[1][0])}\\n  \n",
    "                - Possible relation types: [{relations}]\\n\\n \n",
    "                \n",
    "            - Objective: \n",
    "                 -Analyze both the text and the table to extract all valid relation triplets. Ensure the output strictly adheres to the specified format.\n",
    "        \"\"\"\n",
    "    if num_prompt == 3:\n",
    "        Prompt = f\"\"\"  \n",
    "        You are an expert in Natural Language Processing (NLP) specializing in relation extraction.  \n",
    "        Your task is to extract relations expressed strictly as triplets: (entity1, relation, entity2).  \n",
    "\n",
    "        Constraints:\n",
    "        - Both entity1 and entity2 must be valid named entities.  \n",
    "        - At least one entity must come from the text and the other from the table.  \n",
    "        - Only keep relations that are explicitly listed in the provided \"Possible relation types\".  \n",
    "        - Ignore any relation not in this list.  \n",
    "        - The extracted triplets must reflect connections valid, only when combining both sources (text + table), not when taken in isolation.  \n",
    "        - If no valid triplet exists, return \"NO_RELATION\".  \n",
    "        - Output must contain only the triplets in the required format. Do not include explanations, reasoning, or extra text.  \n",
    "\n",
    "        Output Format (strict):  \n",
    "        entity1, entity2: relation1 | entity3, entity4: relation2 | ...  \n",
    "\n",
    "        Available Data:  \n",
    "        - Text segment: {v[0]}  \n",
    "        - Table content:\\n {table_to_csvlike(v[1][0])}  \n",
    "        - Possible relation types: [{relations}]  \n",
    "\n",
    "        Your task:  \n",
    "        1. Identify relations where one entity is in the text and the other in the table.  \n",
    "        2. Keep only relations that match the provided list.  \n",
    "        3. Return the result strictly in the format:  entity1, entity2: relation1 | entity3, entity4: relation2  \n",
    "        4. If no valid relation exists, return \"NO_RELATION\".  \n",
    "\"\"\"\n",
    "\n",
    "    if num_prompt == 4: # à utiliser avec le modèle LLaMA\n",
    "        Prompt = f\"\"\"\n",
    "                <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "                As a Natural Language Processing (NLP) expert specializing in relation extraction.\n",
    "        Your task is to identify and extract valid relations expressed as triplets (entity1, relation, entity2) \n",
    "        from both a given text segment and a table content.\n",
    "        \n",
    "        Constraints:\n",
    "        - Both entity1 and entity2 must be valid named entities.\n",
    "        - Only extract relations where one entity is from the text and the other is from the table.\n",
    "        - Only use relations that are explicitly listed in the provided \"Possible relation types\".\n",
    "        - If no valid relation exists, return \"NO_RELATION\".\n",
    "        - The output must **only** contain the extracted triplets in the requested format, with no explanations, reasoning, or extra text.\n",
    "        \n",
    "        Output Format:\n",
    "        entity1, entity2: relation1 | entity3, entity4: relation2 | ...\n",
    "        \n",
    "        <|eot_id|>\n",
    "        \n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        You are provided with:\n",
    "        \n",
    "        - Text segment: {v[0]}\n",
    "        - Table content:\\n {table_to_csvlike(v[1][0])}\n",
    "        - Possible relation types: [{relations}]\n",
    "        \n",
    "        Your task:\n",
    "        1. Identify relations where at least one entity is in the text and the other in the table.\n",
    "        2. Construct relation triplets combining entities from both sources.\n",
    "        3. Only keep relations that match the provided list of relation types.\n",
    "        4. Return the result strictly in the format: entity1, entity2: relation1 | entity3, entity4: relation2\n",
    "        \n",
    "        If no valid relation exists, return \"NO_RELATION\".\n",
    "        \n",
    "        <|eot_id|>\n",
    "        \n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\"\n",
    "    return Prompt\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c314c8-cad4-4ab2-b526-fe83313e6a56",
   "metadata": {},
   "source": [
    "## 3.2 Few-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dad2b0a-2108-47fd-8259-3666f98487c7",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ca2b3ff-20c7-491b-b568-35ad16ce62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trilets_gold = []\n",
    "texts = []\n",
    "tables = []\n",
    "for k, v in list(d.items()):\n",
    "    trilets_gold.append(v[2])\n",
    "    texts.append(v[0])\n",
    "    tables.append(table_to_csvlike(v[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e22c5c-6651-439e-8074-b9125d8ec479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae7cc1-455b-465e-8437-a2ee9b53204e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "856005bb-6a64-4470-af75-277662c005b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupere_prompt(num_prompt, i):\n",
    "    if num_prompt == 1:\n",
    "        Prompt = f\"\"\" You are a relation-extraction specialist. Extract cross-source relation triplets (entity1, relation, entity2)\n",
    "        by connecting one entity from the Text with one entity from the Table.\n",
    "\n",
    "        Inputs:\n",
    "        - Text\n",
    "        - Table (CSV-like)\n",
    "        - Possible relation types: [{relations}]\n",
    "        \n",
    "        Output format (strict):\n",
    "         entity1, entity2: relation1 | entity3, entity4: relation2 | ...         \n",
    "         If no valid triplet exists, return exactly: NO_RELATION\n",
    "        \n",
    "        Core constraints:\n",
    "        - Cross-source requirement: In every triplet, one entity must come from the Text and the other from the Table.\n",
    "        - entity1 and entity2 must be named entities. Do not invent entities.\n",
    "        - Use only relation labels listed in Possible relation types, exactly as written.\n",
    "        - Output must contain only the triplets in the required format. Do not include explanations, reasoning, or extra text.   \n",
    "       \n",
    "        Relation direction and semantics:\n",
    "        - Use the conventional direction implied by the relation label. Examples if present in Possible relation types:\n",
    "            . Acquired_by: e2 purchases controlling stake in e1. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Brand of: e2 offers products or services of e1 (Brand). The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Client of: e1 uses (and presumably pays for) products or services offered by e2. The relation is directed. The inverted relation is best described by “Supplier of”.\n",
    "            . Collaboration: e1 and e2 collaborate in (parts of their) business activities. The relation is undirected.\n",
    "            . Competitor of: e1 competes for resources with e2. The relation is undirected.\n",
    "            . Merged with: e1 and e2 merged (parts of) their business activities. The relation is undirected.\n",
    "            . Product or service of: e1 is offered for commercial distribution by e2. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Regulated by: e2 regulates (parts of) the business activity of e1. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Shareholder of: e1 owns shares in e2. The relation is directed. The inverted relation is best described by the same relation type.\n",
    "            . Subsidiary of: e2 legally owns e1. The relation is directed. The inverted relation is best described by “Parent of”.\n",
    "            . Traded on: Shares of e1 are listed on e2 (Stock exchange). The relation is directed. The inverted relation is best described by “lists”.\n",
    "\n",
    "\n",
    "\n",
    "        Few-shot examples:\n",
    "        \n",
    "            - Example 1:\n",
    "                . Text: {texts[1]}\n",
    "                . Table:\\n{tables[1]}\n",
    "                . Output:\\n{trilets_gold[1]}\\n\\n  \n",
    "        \n",
    "            - Example 2:\n",
    "                . Text: {texts[4]}\n",
    "                . Table:\\n{tables[4]}\n",
    "                . Output:\\n{trilets_gold[4]} \\n\\n               \n",
    "            - Example 3:\n",
    "                . Text: {texts[5]}\n",
    "                . Table:\\n{tables[5]}\n",
    "                . Output: NO_RELATION\\n\\n  \n",
    "                \n",
    "        Now process the new data\n",
    "        \n",
    "        - Text: {v[0]}\n",
    "        - Table: {table_to_csvlike(v[1][0])}\n",
    "        - Output : \n",
    "        \n",
    "        Task Analyze the Text and the Table together and extract all valid cross-source relation triplets. Return only the triplets in the exact required format, or NO_RELATION if none. \"\"\"\n",
    "    if num_prompt == 2:\n",
    "        Prompt = f\"\"\"  \n",
    "            You are an NLP expert specializing in relation extraction. Extract cross-source relation triplets (entity1, relation, entity2)\n",
    "        by combining evidence from a text passage and a table.\n",
    "        \n",
    "        Task:\n",
    "        \n",
    "        - Identify relations where at least one entity is sourced from the text and at least one from the table.\n",
    "        - Construct valid triplets that use only relations from the provided list.\n",
    "        - Output only the triplets in the exact format specified below. If none are valid, output \"NO_RELATION\"\n",
    "        \n",
    "        Definitions and constraints:\n",
    "    \n",
    "        - Named entities: proper-noun phrases referring to real-world entities (e.g., persons, organizations, companies, locations, products). Do not use generic terms or common nouns.\n",
    "        - Entity sourcing:\n",
    "            . Text entities must be extracted from the text span.\n",
    "            . Table entities must be cell values from the table.\n",
    "            . Each triplet must include at least one entity sourced from the text and at least one sourced from the table.\n",
    "        - Relation validity:\n",
    "        - Use only relations listed in Allowed relation types\n",
    "        \n",
    "        Output format (strict)\n",
    "        \n",
    "        - If at least one valid triplet exists, output them on a single line using this exact pattern: entity1, entity2: relation | entity3, entity4: relation | ...\n",
    "        - No extra text, and no newline.\n",
    "        - If no valid triplet exists, output exactly: \"NO_RELATION\"\n",
    "        \n",
    "\n",
    "        Few-shot examples\n",
    "        \n",
    "        Example 1\n",
    "                . Text: \n",
    "                . Table:\n",
    "                . Allowed relation types: [{relations}]\n",
    "                . Output:\n",
    "        Example 2\n",
    "                . Text: \n",
    "                . Table:\n",
    "                . Allowed relation types: [{relations}]\n",
    "                . Output:\n",
    "        Example 3\n",
    "                . Text: \n",
    "                . Table:\n",
    "                . Allowed relation types: [{relations}]\n",
    "                . Output:\n",
    "        Example 4 (no relation)\n",
    "                . Text: \n",
    "                . Table:\n",
    "                . Allowed relation types: [{relations}]\n",
    "                . Output:\n",
    "                \n",
    "        New data\n",
    "        \n",
    "        - Text: {{text}}\n",
    "        - Table (CSV-like): {{table_csv}}\n",
    "        - Allowed relation types: [{relations}]\n",
    "        - Output:\n",
    "        Return only the triplets.\n",
    "            \"\"\"\n",
    "\n",
    "    if num_prompt == 3: # à utiliser avec le modèle LLaMA\n",
    "        Prompt = f\"\"\"\n",
    "          <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "            You are an NLP expert specializing in cross-source relation extraction. Your goal is to output only valid relation triplets (entity1, relation, entity2) that are jointly supported by a free-text passage and a tabular dataset.\n",
    "            \n",
    "            Inputs\n",
    "            - Text\n",
    "            - Table (CSV-like string)\n",
    "            - Allowed relation types: [relations]\n",
    "            \n",
    "            Core requirements\n",
    "            - Named entities only: entity1 and entity2 must be proper-noun entities (persons, organizations, companies, locations, products). Do not use generic/common nouns (e.g., \"company\", \"city\") or pure numbers/dates unless they are part of a named entity.\n",
    "            - Cross-source constraint: each triplet must include at least one entity sourced from the text and at least one entity sourced from the table. If both entities appear in both sources, designate one as text-sourced and the other as table-sourced to satisfy the constraint.\n",
    "            - Relation validity: use only labels from \"Possible relation types\". Respect semantic direction implied by the label (e.g., founded_by: subject=organization/company, object=person).\n",
    "            - Evidence agreement: a triplet is valid only if (a) the text explicitly states or strongly implies the relation between the same two entities, and (b) the table contains those entities in the same row (across any columns). Table headers are not entities.\n",
    "            - Matching and canonicalization:\n",
    "              - Parse the first row as headers; subsequent rows are records.\n",
    "              - Consider entity pairs formed within the same row across columns; do not form pairs using headers.\n",
    "              - Match entities case-insensitively and after trimming whitespace.\n",
    "              - When an entity appears in both sources, prefer the table cell’s spelling for output; otherwise, use the text surface form.\n",
    "            - De-duplication and ordering: output each unique (entity1, relation, entity2) once. Sort triplets by relation, then entity1, then entity2 (case-insensitive) for deterministic output.\n",
    "            \n",
    "            Output format (strict)\n",
    "            - If at least one valid triplet exists, output them on a single line:\n",
    "              entity1, entity2: relation | entity3, entity4: relation | ...\n",
    "            - Use \", \" between entities, \": \" before the relation, and \" | \" between triplets.\n",
    "            - No trailing separator, no extra text, and no newline.\n",
    "            - Escaping: if an entity contains a comma, colon, or pipe, wrap it in double quotes and escape embedded quotes by doubling them (e.g., \"ACME, Inc.\").\n",
    "            - If no valid triplet exists, output exactly: NO_RELATION\n",
    "            \n",
    "            Procedure\n",
    "            1) Extract named entities from the text.\n",
    "            2) Parse the CSV-like table, collect cell values from each data row (ignore headers), and form candidate entity pairs within each row across columns.\n",
    "            3) For each candidate pair, check whether the text expresses one of the allowed relations between the same two entities; assign the correct label and direction.\n",
    "            4) Canonicalize entity strings, remove duplicates, sort (relation, entity1, entity2), and output in the strict format.\n",
    "            \n",
    "            Few-shot examples\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            \\nExample 1:\n",
    "            Text: \n",
    "            Table: \n",
    "            Relations: \n",
    "            Output: \n",
    "            \n",
    "            Example 2:\n",
    "            Text: \n",
    "            Table: \n",
    "            Relations: \n",
    "            Output: \n",
    "            \n",
    "            \\nExample 3:\\n\n",
    "            Text: \n",
    "            Table: \n",
    "            Relations: \n",
    "            Output: \n",
    "            \n",
    "            Example 4 (no relation):\n",
    "            Text: \n",
    "            Table: \n",
    "            Relations: \n",
    "            Output: NO_RELATION\n",
    "            <|eot_id|>\n",
    "            \n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            Text: {v[0]}\n",
    "            Table: {table_to_csvlike(v[1][0])}\n",
    "            Possible relation types: [{relations}]\n",
    "            \n",
    "            Return only the triplets in the strict format.\n",
    "            <|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\"\n",
    "    return Prompt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f74793a8-e683-497a-bce6-6444de232e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b2637-9b7c-4383-a628-52ce4d3768e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47dc590-26c3-496f-892d-e3e2c3dc8b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81393552-eb59-4151-99f6-6c978ce71641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c06d95e4-cd90-4303-a4ae-1530fa604b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_instructions=[]\n",
    "for i, v in list(d.items()):\n",
    "    prompt = recupere_prompt(1, i)\n",
    "    demo_instructions.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "082455ed-8753-4d26-91cf-ed919ad3d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(demo_instructions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2ec224c-5ca9-4c22-86fb-ece02369186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultatsF = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bdaed9f0-351c-4710-8ea7-ede889f9bffd",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 503 - {'error': {'message': 'meta-llama/llama-4-maverick-17b-128e-instruct is currently over capacity. Please try again and back off exponentially. Visit https://groqstatus.com to see if there is an active incident.', 'type': 'internal_server_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m resultats \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inst \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     reponse \u001b[38;5;241m=\u001b[39m \u001b[43mcomplete_and_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdemo_instructions\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     resultats\u001b[38;5;241m.\u001b[39mappend(reponse)\n",
      "Cell \u001b[0;32mIn[17], line 41\u001b[0m, in \u001b[0;36mcomplete_and_print\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomplete_and_print\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_MODEL):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m#print(f'==============\\n{prompt}\\n==============')\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m#print(response, end='\\n\\n')\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[0;32mIn[17], line 32\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(prompt, model, temperature, top_p)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompletion\u001b[39m(\n\u001b[1;32m     27\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     28\u001b[0m     model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_MODEL,\n\u001b[1;32m     29\u001b[0m     temperature: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     30\u001b[0m     top_p: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     31\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43muser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m, in \u001b[0;36mchat_completion\u001b[0;34m(messages, model, temperature, top_p)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat_completion\u001b[39m(\n\u001b[1;32m     12\u001b[0m     messages: List[Dict],\n\u001b[1;32m     13\u001b[0m     model \u001b[38;5;241m=\u001b[39m DEFAULT_MODEL,\n\u001b[1;32m     14\u001b[0m     temperature: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     15\u001b[0m     top_p: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     16\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 17\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/groq/resources/chat/completions.py:341\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    214\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/groq/_base_client.py:1222\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1210\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1217\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1218\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1219\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1220\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1221\u001b[0m     )\n\u001b[0;32m-> 1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/groq/_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1028\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1030\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1031\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mInternalServerError\u001b[0m: Error code: 503 - {'error': {'message': 'meta-llama/llama-4-maverick-17b-128e-instruct is currently over capacity. Please try again and back off exponentially. Visit https://groqstatus.com to see if there is an active incident.', 'type': 'internal_server_error'}}"
     ]
    }
   ],
   "source": [
    "resultats = []\n",
    "for inst in range(0,200):\n",
    "    reponse = complete_and_print(demo_instructions[inst])\n",
    "    resultats.append(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4636365-3b08-4a70-86e1-3759ddded662",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultatsF = resultatsF + resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3b10e-c9fb-4597-9901-2e9bcf7c50d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8124298-4caa-49c9-a4f3-d9b2973b3dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67e4e7f5-f3ac-4e7d-9a3f-03759ba413e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b7a3eb-874d-4044-b5f0-41472d238017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9518dae-dbad-4054-83ab-486836a84e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b99a1eeb-225d-4af2-8e36-826f435109d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_result = pd.read_csv(\"resultsfs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fb3017a-d020-4898-925f-4d9d98756b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_result= few_shot_result[\"0\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "528d8034-1ab0-469d-a4ba-40361245a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_result = zero_shot_result + resultatsF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbdb5a9-7254-4ffd-92ac-264667e65c04",
   "metadata": {},
   "source": [
    "#### Post-traitement (cas DeepSeek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b96beb6-2b3b-4041-978d-0d2066f7ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#few_shot_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c72271-ff19-4375-8a07-8e541fac3c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8479ea5e-9539-4dbf-82e4-2217afc6889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_output(text):\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79d7c221-ccc9-4b82-9aee-83f19af1497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_resultF = []\n",
    "for output in few_shot_result:\n",
    "    cleaned = clean_output(output)\n",
    "    few_shot_resultF.append(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac964f7b-2b19-427b-9c9b-11dff6191bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GV Films; Film production Film distribution; product_or_service_of | GV Films; Motion pictures (Tamil); product_or_service_of | Sujatha Films; Film production Film distribution; product_or_service_of | Sujatha Films; Motion pictures (Tamil); product_or_service_of | Mani Ratnam; G. Venkateswaran; collaboration | Mahesh Manjrekar; G. Venkateswaran; collaboration | Sanjay Dutt; G. Venkateswaran; collaboration | Kasthuri Shankar; G. Venkateswaran; collaboration | Urchagam; Film production Film distribution; product_or_service_of | Urchagam; Motion pictures (Tamil); product_or_service_of | Kaivantha Kalai; Film production Film distribution; product_or_service_of | Kaivantha Kalai; Motion pictures (Tamil); product_or_service_of | Thirudi; Film production Film distribution; product_or_service_of | Thirudi; Motion pictures (Tamil); product_or_service_of',\n",
       " 'Spanfeller Media Group ; Tribune Publishing ; subsidiary_of']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_resultF[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5bab6-f792-4d6c-a560-f92ae190e720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "134fa5e7-4e16-4662-b249-00c2ca270a00",
   "metadata": {},
   "source": [
    "## Preprocessing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20afb532-4f1a-414b-9fd5-848f981f16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cas un LLM sort : extra text | Explications, etc\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_triplets(llm_output: str) -> str:\n",
    "    text = llm_output.strip()\n",
    "    pattern = r'([A-Za-z0-9\\s\\(\\)\\.\\-&,]+,\\s*[A-Za-z0-9\\s\\(\\)\\.\\-&,]+:\\s*[A-Za-z_]+(?:\\s*\\|\\s*[A-Za-z0-9\\s\\(\\)\\.\\-&,]+,\\s*[A-Za-z0-9\\s\\(\\)\\.\\-&,]+:\\s*[A-Za-z_]+)*)'\n",
    "\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    if matches:\n",
    "        # On prend le dernier matchee\n",
    "        cleaned = matches[-1].strip()\n",
    "        return cleaned\n",
    "    \n",
    "    return \"NO_RELATION\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6571bc-a4cd-435a-b966-6f0cf3b03413",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_resultF = []\n",
    "for x in resultatsF:\n",
    "    few_shot_resultF.append(extract_triplets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce887405-58cc-4664-b65c-75ad2224cdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4587e2ab-ecf1-42ab-b828-80b0424e48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_resultF = [x.replace(\", \", \"; \").replace(\": \", \"; \") for x in few_shot_resultF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89e03d11-93e9-4079-9bcb-84ff9fe49618",
   "metadata": {},
   "outputs": [],
   "source": [
    "trilets_gold = []\n",
    "for k, v in list(d.items()):\n",
    "    trilets_gold.append(v[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541710b-205c-44a1-b0b2-7e8007f43124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c52388e8-e2aa-4995-b639-a8eee993f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour mettre tous les triplets dans la même format\n",
    "\n",
    "def extract_triplets_format(text):\n",
    "    triplets = []\n",
    "    if not text.strip(): \n",
    "        return triplets\n",
    "    \n",
    "    parts = re.split(r\"\\||\\n\", text)\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        if \";\" in part:\n",
    "            elems = [p.strip() for p in part.split(\";\")]\n",
    "            if len(elems) == 3 and elems[1] in relations or elems[2] in relations:\n",
    "                if elems[1] in relations:\n",
    "                    triplets.append(f\"\"\"{elems[0].lower().replace(\" \",\"\")}; {elems[1].lower().replace(\" \",\"\")}; {elems[2].lower().replace(\" \",\"\")}\"\"\")\n",
    "                elif elems[2] in relations:\n",
    "                    triplets.append(f\"\"\"{elems[0].lower().replace(\" \",\"\")}; {elems[2].lower().replace(\" \",\"\")}; {elems[1].lower().replace(\" \",\"\")}\"\"\")\n",
    "            continue\n",
    "        \n",
    "        m = re.match(r\"(.+?),\\s*(.+?):\\s*(\\w+)\", part)\n",
    "        if m:\n",
    "            e1, e2, rel = m.groups()\n",
    "            if rel in relations:\n",
    "                triplets.append(f\"\"\"{e1.strip().lower().replace(\" \",\"\")}; {rel.strip().lower().replace(\" \",\"\")}; {e2.strip().lower().replace(\" \",\"\")}\"\"\")\n",
    "    return triplets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edff96c3-2013-4641-a63d-4edd6bdfca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_results = [extract_triplets_format(el) for el in few_shot_resultF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1b5e691-add7-4d9d-9955-4d12c656e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trilets_gold = [extract_triplets_format(el) for el in trilets_gold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8414de51-26c0-4cb1-8545-f3c365490773",
   "metadata": {},
   "outputs": [],
   "source": [
    "trilets_gold = trilets_gold[:148]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7de30f4-4dda-4f68-950d-50cbb31ae34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], ['tribunepublishing; subsidiary_of; spanfellermediagroup(smg)']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trilets_gold[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627d16d-0ab1-4b25-bbb3-1c46088e9bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3437b0-90b0-4d92-9608-321b2e2167ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "154fcde3-8e46-46b3-9946-f2a7453b5433",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a00eda00-0e31-4251-a935-e53c20ef541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation_triplets_modified import evaluation_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca5c8591-e24e-460f-9e9f-e8f63ae97c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluation_triplets(trilets_gold, few_shot_results, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8af99f5f-007a-4cac-b97f-866d944aebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Exact matching': {'precision': 0.2105543710021322, 'recall': 0.710431654676259, 'f1': 0.3248355263157895}, 'Partial matching (head+tail)': {'precision': 0.2601279317697228, 'recall': 0.8776978417266187, 'f1': 0.40131578947368424}, 'Partial matching (relation + 1 entity)': {'precision': 0.4957356076759062, 'recall': 1.6726618705035972, 'f1': 0.7648026315789473}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d6480-410f-4027-bf97-22fd82da5f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9e092-737a-4748-a39f-0bd7f2200900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d1ffd7-c241-4278-9397-a77beb843f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b6e62-ec93-4bb3-9596-f1008ec3e099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fbe785-bc2d-4838-8c08-61391df60dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
