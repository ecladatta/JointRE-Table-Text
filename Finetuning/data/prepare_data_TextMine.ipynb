{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90f7f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "import random\n",
    "import datasets\n",
    "import torch\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoConfig, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534c2c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projects/melodi/mettaleb/FinGPT/fingpt/FinGPT_Benchmark/data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145839b",
   "metadata": {},
   "source": [
    "# Prepare my data finRED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a52b2d7-2660-4e60-98eb-641e8a18750a",
   "metadata": {},
   "source": [
    "## CORE Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c554796",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "def preprocess_text(text):\n",
    "    processed_words = []\n",
    "    text = text.replace('\\n', '.')\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = text.replace(\"\\\\xa\",\" \")\n",
    "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
    "    text = re.sub(r'\\/.*?\\/', ' ', text)\n",
    "    text = text.replace('\\\\', '')\n",
    "    text = ' '.join(re.findall(r'\\w+', text))\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if not token.is_space and not token.is_punct:\n",
    "            lemma = token.lemma_\n",
    "            if len(lemma)>1:\n",
    "                processed_words.append(lemma.lower())\n",
    "    processed_text = ' '.join(processed_words)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edb4f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relation</th>\n",
       "      <th>invert_relation</th>\n",
       "      <th>e1_start</th>\n",
       "      <th>e1_end</th>\n",
       "      <th>e2_start</th>\n",
       "      <th>e2_end</th>\n",
       "      <th>e1_name</th>\n",
       "      <th>e2_name</th>\n",
       "      <th>context</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E8026501</td>\n",
       "      <td>shareholder_of</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>Badgeville</td>\n",
       "      <td>Norwest Venture Partners</td>\n",
       "      <td>[Badgeville, subsequently, raised, a, $, 12M, ...</td>\n",
       "      <td>Badgeville subsequently raised a $ 12M Series ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E8034594</td>\n",
       "      <td>competitor_of</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Bioverativ Inc.</td>\n",
       "      <td>Baxalta</td>\n",
       "      <td>[Bioverativ, competes, with, Baxalta, (, acqui...</td>\n",
       "      <td>Bioverativ competes with Baxalta ( acquired by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E8029931</td>\n",
       "      <td>client_of</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>Baton Rouge Southern Railroad</td>\n",
       "      <td>Kansas City Southern</td>\n",
       "      <td>[It, also, serves, as, a, switching, and, car,...</td>\n",
       "      <td>It also serves as a switching and car storage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E8202016</td>\n",
       "      <td>shareholder_of</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>PEG Africa Ltd.</td>\n",
       "      <td>Blue Haven Initiative</td>\n",
       "      <td>[In, 2017, ,, the, company, raised, a, further...</td>\n",
       "      <td>In 2017 , the company raised a further $ 13.5 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E8110590</td>\n",
       "      <td>product_or_service_of</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>Lewis Galoob Toys, Inc.</td>\n",
       "      <td>Micro Machines</td>\n",
       "      <td>[Products, Toys, ,, video, games, ,, consumer,...</td>\n",
       "      <td>Products Toys , video games , consumer electro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id               relation  invert_relation  e1_start  e1_end  \\\n",
       "0  E8026501         shareholder_of                1         0       0   \n",
       "1  E8034594          competitor_of                0         0       0   \n",
       "2  E8029931              client_of                1         0       0   \n",
       "3  E8202016         shareholder_of                1         3       4   \n",
       "4  E8110590  product_or_service_of                1        10      14   \n",
       "\n",
       "   e2_start  e2_end                        e1_name                   e2_name  \\\n",
       "0        15      17                     Badgeville  Norwest Venture Partners   \n",
       "1         3       3                Bioverativ Inc.                   Baxalta   \n",
       "2        12      14  Baton Rouge Southern Railroad      Kansas City Southern   \n",
       "3        36      38                PEG Africa Ltd.     Blue Haven Initiative   \n",
       "4        34      35        Lewis Galoob Toys, Inc.            Micro Machines   \n",
       "\n",
       "                                             context  \\\n",
       "0  [Badgeville, subsequently, raised, a, $, 12M, ...   \n",
       "1  [Bioverativ, competes, with, Baxalta, (, acqui...   \n",
       "2  [It, also, serves, as, a, switching, and, car,...   \n",
       "3  [In, 2017, ,, the, company, raised, a, further...   \n",
       "4  [Products, Toys, ,, video, games, ,, consumer,...   \n",
       "\n",
       "                                                text  \n",
       "0  Badgeville subsequently raised a $ 12M Series ...  \n",
       "1  Bioverativ competes with Baxalta ( acquired by...  \n",
       "2  It also serves as a switching and car storage ...  \n",
       "3  In 2017 , the company raised a further $ 13.5 ...  \n",
       "4  Products Toys , video games , consumer electro...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/projects/melodi/mettaleb/CORE/data/train.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "df_train = pd.DataFrame(data)\n",
    "df_train[\"text\"] = [\" \".join(df_train['context'].to_list()[i]) for i in range(len(df_train['context'].to_list()))]\n",
    "with open(\"/projects/melodi/mettaleb/CORE/data/test.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "df_test = pd.DataFrame(data)\n",
    "df_test[\"text\"] = [\" \".join(df_test['context'].to_list()[i]) for i in range(len(df_test['context'].to_list()))]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98fc9ccf-c205-4f62-9e57-8f3320063ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(708, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99262101",
   "metadata": {},
   "source": [
    "## EDA : Easy Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fadfb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>schoolbook</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E8026501</td>\n",
       "      <td>series round norwest stake partners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E8026501</td>\n",
       "      <td>badgeville subsequently raised a m series b ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E8034594</td>\n",
       "      <td>bioverativ competes baxalta shire horse pfizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E8034594</td>\n",
       "      <td>bioverativ competes with baxalta acquired by s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                         schoolbook\n",
       "0        id                                               text\n",
       "1  E8026501                series round norwest stake partners\n",
       "2  E8026501  badgeville subsequently raised a m series b ro...\n",
       "3  E8034594     bioverativ competes baxalta shire horse pfizer\n",
       "4  E8034594  bioverativ competes with baxalta acquired by s..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"/projects/melodi/mettaleb/eda_nlp/data/eda_core.txt\", sep=\"\\t\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63038aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relation</th>\n",
       "      <th>invert_relation</th>\n",
       "      <th>e1_start</th>\n",
       "      <th>e1_end</th>\n",
       "      <th>e2_start</th>\n",
       "      <th>e2_end</th>\n",
       "      <th>e1_name</th>\n",
       "      <th>e2_name</th>\n",
       "      <th>context</th>\n",
       "      <th>text</th>\n",
       "      <th>schoolbook</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E8026501</td>\n",
       "      <td>shareholder_of</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>Badgeville</td>\n",
       "      <td>Norwest Venture Partners</td>\n",
       "      <td>[Badgeville, subsequently, raised, a, $, 12M, ...</td>\n",
       "      <td>Badgeville subsequently raised a $ 12M Series ...</td>\n",
       "      <td>series round norwest stake partners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E8026501</td>\n",
       "      <td>shareholder_of</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>Badgeville</td>\n",
       "      <td>Norwest Venture Partners</td>\n",
       "      <td>[Badgeville, subsequently, raised, a, $, 12M, ...</td>\n",
       "      <td>Badgeville subsequently raised a $ 12M Series ...</td>\n",
       "      <td>badgeville subsequently raised a m series b ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E8034594</td>\n",
       "      <td>competitor_of</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Bioverativ Inc.</td>\n",
       "      <td>Baxalta</td>\n",
       "      <td>[Bioverativ, competes, with, Baxalta, (, acqui...</td>\n",
       "      <td>Bioverativ competes with Baxalta ( acquired by...</td>\n",
       "      <td>bioverativ competes baxalta shire horse pfizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E8034594</td>\n",
       "      <td>competitor_of</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Bioverativ Inc.</td>\n",
       "      <td>Baxalta</td>\n",
       "      <td>[Bioverativ, competes, with, Baxalta, (, acqui...</td>\n",
       "      <td>Bioverativ competes with Baxalta ( acquired by...</td>\n",
       "      <td>bioverativ competes with baxalta acquired by s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E8029931</td>\n",
       "      <td>client_of</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>Baton Rouge Southern Railroad</td>\n",
       "      <td>Kansas City Southern</td>\n",
       "      <td>[It, also, serves, as, a, switching, and, car,...</td>\n",
       "      <td>It also serves as a switching and car storage ...</td>\n",
       "      <td>switching car storage adroitness southern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id        relation  invert_relation  e1_start  e1_end  e2_start  \\\n",
       "0  E8026501  shareholder_of                1         0       0        15   \n",
       "1  E8026501  shareholder_of                1         0       0        15   \n",
       "2  E8034594   competitor_of                0         0       0         3   \n",
       "3  E8034594   competitor_of                0         0       0         3   \n",
       "4  E8029931       client_of                1         0       0        12   \n",
       "\n",
       "   e2_end                        e1_name                   e2_name  \\\n",
       "0      17                     Badgeville  Norwest Venture Partners   \n",
       "1      17                     Badgeville  Norwest Venture Partners   \n",
       "2       3                Bioverativ Inc.                   Baxalta   \n",
       "3       3                Bioverativ Inc.                   Baxalta   \n",
       "4      14  Baton Rouge Southern Railroad      Kansas City Southern   \n",
       "\n",
       "                                             context  \\\n",
       "0  [Badgeville, subsequently, raised, a, $, 12M, ...   \n",
       "1  [Badgeville, subsequently, raised, a, $, 12M, ...   \n",
       "2  [Bioverativ, competes, with, Baxalta, (, acqui...   \n",
       "3  [Bioverativ, competes, with, Baxalta, (, acqui...   \n",
       "4  [It, also, serves, as, a, switching, and, car,...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Badgeville subsequently raised a $ 12M Series ...   \n",
       "1  Badgeville subsequently raised a $ 12M Series ...   \n",
       "2  Bioverativ competes with Baxalta ( acquired by...   \n",
       "3  Bioverativ competes with Baxalta ( acquired by...   \n",
       "4  It also serves as a switching and car storage ...   \n",
       "\n",
       "                                          schoolbook  \n",
       "0                series round norwest stake partners  \n",
       "1  badgeville subsequently raised a m series b ro...  \n",
       "2     bioverativ competes baxalta shire horse pfizer  \n",
       "3  bioverativ competes with baxalta acquired by s...  \n",
       "4          switching car storage adroitness southern  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(df_train, df2, on='id')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c49462",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_train, df2, on='id')\n",
    "\n",
    "# Renommer les colonnes pour plus de clarté\n",
    "merged_df = merged_df.rename(columns={'text': 'original_text', 'schoolbook': 'text'})\n",
    "del merged_df['original_text']\n",
    "# Afficher le résultat final\n",
    "df_train = pd.concat([df_train, merged_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "311d51da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relation</th>\n",
       "      <th>invert_relation</th>\n",
       "      <th>e1_start</th>\n",
       "      <th>e1_end</th>\n",
       "      <th>e2_start</th>\n",
       "      <th>e2_end</th>\n",
       "      <th>e1_name</th>\n",
       "      <th>e2_name</th>\n",
       "      <th>context</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E8026501</td>\n",
       "      <td>shareholder_of</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>Badgeville</td>\n",
       "      <td>Norwest Venture Partners</td>\n",
       "      <td>[Badgeville, subsequently, raised, a, $, 12M, ...</td>\n",
       "      <td>Badgeville subsequently raised a $ 12M Series ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E8034594</td>\n",
       "      <td>competitor_of</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Bioverativ Inc.</td>\n",
       "      <td>Baxalta</td>\n",
       "      <td>[Bioverativ, competes, with, Baxalta, (, acqui...</td>\n",
       "      <td>Bioverativ competes with Baxalta ( acquired by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E8029931</td>\n",
       "      <td>client_of</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>Baton Rouge Southern Railroad</td>\n",
       "      <td>Kansas City Southern</td>\n",
       "      <td>[It, also, serves, as, a, switching, and, car,...</td>\n",
       "      <td>It also serves as a switching and car storage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E8202016</td>\n",
       "      <td>shareholder_of</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>PEG Africa Ltd.</td>\n",
       "      <td>Blue Haven Initiative</td>\n",
       "      <td>[In, 2017, ,, the, company, raised, a, further...</td>\n",
       "      <td>In 2017 , the company raised a further $ 13.5 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E8110590</td>\n",
       "      <td>product_or_service_of</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>Lewis Galoob Toys, Inc.</td>\n",
       "      <td>Micro Machines</td>\n",
       "      <td>[Products, Toys, ,, video, games, ,, consumer,...</td>\n",
       "      <td>Products Toys , video games , consumer electro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id               relation  invert_relation  e1_start  e1_end  \\\n",
       "0  E8026501         shareholder_of                1         0       0   \n",
       "1  E8034594          competitor_of                0         0       0   \n",
       "2  E8029931              client_of                1         0       0   \n",
       "3  E8202016         shareholder_of                1         3       4   \n",
       "4  E8110590  product_or_service_of                1        10      14   \n",
       "\n",
       "   e2_start  e2_end                        e1_name                   e2_name  \\\n",
       "0        15      17                     Badgeville  Norwest Venture Partners   \n",
       "1         3       3                Bioverativ Inc.                   Baxalta   \n",
       "2        12      14  Baton Rouge Southern Railroad      Kansas City Southern   \n",
       "3        36      38                PEG Africa Ltd.     Blue Haven Initiative   \n",
       "4        34      35        Lewis Galoob Toys, Inc.            Micro Machines   \n",
       "\n",
       "                                             context  \\\n",
       "0  [Badgeville, subsequently, raised, a, $, 12M, ...   \n",
       "1  [Bioverativ, competes, with, Baxalta, (, acqui...   \n",
       "2  [It, also, serves, as, a, switching, and, car,...   \n",
       "3  [In, 2017, ,, the, company, raised, a, further...   \n",
       "4  [Products, Toys, ,, video, games, ,, consumer,...   \n",
       "\n",
       "                                                text  \n",
       "0  Badgeville subsequently raised a $ 12M Series ...  \n",
       "1  Bioverativ competes with Baxalta ( acquired by...  \n",
       "2  It also serves as a switching and car storage ...  \n",
       "3  In 2017 , the company raised a further $ 13.5 ...  \n",
       "4  Products Toys , video games , consumer electro...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f78bfb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1399f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2c9c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_actor1 = df_test['e1_name'].to_list()\n",
    "l_actor2 = df_test['e2_name'].to_list()\n",
    "l_relation = df_test['relation'].to_list()\n",
    "with open(\"./mydata/test.tup\", \"w\") as file:\n",
    "    for i in range(len(l_actor1)):\n",
    "        file.write(f\"{l_actor1[i]} ; {l_actor2[i]} ; {l_relation[i]}\\n\")\n",
    "        #file.write(f\"{l_relation[i].strip()}\\n\")\n",
    "        \n",
    "l_actor1 = df_train['e1_name'].to_list()\n",
    "l_actor2 = df_train['e2_name'].to_list()\n",
    "l_relation = df_train['relation'].to_list()\n",
    "with open(\"./mydata/train.tup\", \"w\") as file:\n",
    "    for i in range(len(l_actor1)):\n",
    "        file.write(f\"{l_actor1[i]} ; {l_actor2[i]} ; {l_relation[i]}\\n\")\n",
    "        #file.write(f\"{l_relation[i].strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b7d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./mydata/train.sent\", \"w\") as file:\n",
    "    for sentence in df_train[\"text\"].to_list():\n",
    "        file.write(f\"{sentence}\\n\")\n",
    "with open(\"./mydata/test.sent\", \"w\") as file:\n",
    "    for sentence in df_test[\"text\"].to_list():\n",
    "        file.write(f\"{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd18169b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acquired_by',\n",
       " 'brand_of',\n",
       " 'client_of',\n",
       " 'collaboration',\n",
       " 'competitor_of',\n",
       " 'merged_with',\n",
       " 'product_or_service_of',\n",
       " 'regulated_by',\n",
       " 'shareholder_of',\n",
       " 'subsidiary_of',\n",
       " 'traded_on',\n",
       " 'undefined'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_train['relation'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9c408aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projects/melodi/mettaleb/FinGPT/fingpt/FinGPT_Benchmark/data'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3dae1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations= ['acquired_by',\n",
    " 'brand_of',\n",
    " 'client_of',\n",
    " 'collaboration',\n",
    " 'competitor_of',\n",
    " 'merged_with',\n",
    " 'product_or_service_of',\n",
    " 'regulated_by',\n",
    " 'shareholder_of',\n",
    " 'subsidiary_of',\n",
    " 'traded_on',\n",
    " 'undefined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0271ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('mydata/relations.txt') as f:\n",
    "#relations = [r.strip() for r in set(df_train['relation'].to_list())]\n",
    "\n",
    "    \n",
    "def get_instruction(sent, tuples, with_orig=True, with_cls=False):\n",
    "    \n",
    "    instructions, inputs, outputs = [], [], []\n",
    "    if with_orig:\n",
    "        instructions.append(f\"Given a paragraph that describes the relationship between two words/phrases as options, extract the word/phrase pair and the corresponding lexical relationship between them from the input text. The output format should be \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Options: {', '.join(relations)}. \")\n",
    "        instructions.append(f\"Given the input paragraph, please extract the subject and object containing a certain relation in the sentence according to the following relation types, in the format of \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Relations include: {'; '.join(relations)}.\")\n",
    "        instructions.append(f\"Provide a paragraph containing relationships between entities. Extract and identify the specific relations between entity pairs mentioned in the paragraph. Output the results in the format \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Relations should be determined based on the context provided in the paragraph. Relations include: {'; '.join(relations)}.\")\n",
    "\n",
    "        inputs.extend([sent] * 3)\n",
    "        outputs.extend([\"; \".join([f\"{tup[-1]}: {tup[0]}, {tup[1]}\" for tup in tuples])] * 3)\n",
    "    \n",
    "    if with_cls:\n",
    "        for tup in tuples:\n",
    "            #instructions.append(f\"Utilize the input text as a context reference, choose the right relationship between {tup[0]} and {tup[1]} from the options. Options: {', '.join(relations)}.\")\n",
    "            instructions.append(f\"What is the relationship between {tup[0]} and {tup[1]} in the context of the input sentence. Choose an answer from: {'; '.join(relations)}.\")\n",
    "            #instructions.append(f\"\"\"Analyze the following sentence and identify the relationship between the two mentioned entities. The relationship must be selected from the predefined list below. If none of the relationships apply, respond with \"undefined\".\\nEntité 1 : {tup[0]}.\\n \\nEntité 2 : {tup[1]}.\\nRelations : {'; '.join(relations)}.\"\"\")\n",
    "            inputs.extend([sent] * 1)\n",
    "            outputs.extend([tup[-1]] * 1)\n",
    "    \n",
    "    return instructions, inputs, outputs\n",
    "\n",
    "\n",
    "def get_finred_dataset(sent_file, tup_file, with_orig, with_cls):\n",
    "\n",
    "    instructions, inputs, outputs = [], [], []\n",
    "\n",
    "    with open(sent_file) as f:\n",
    "        sentences = [s.strip() for s in f.readlines()]\n",
    "    with open(tup_file) as f:\n",
    "        tuples_list = [s.split(' | ') for s in f.readlines()]\n",
    "        \n",
    "    for sent, tuples in zip(sentences, tuples_list):\n",
    "        tuples = [[e.strip() for e in tup.split(' ; ')] for tup in tuples]\n",
    "        \n",
    "        ins, i, o = get_instruction(sent, tuples, with_orig, with_cls)\n",
    "        \n",
    "        instructions.extend(ins)\n",
    "        inputs.extend(i)\n",
    "        outputs.extend(o)\n",
    "    print(\"longuer = \",len(outputs))\n",
    "    print(\"exemple : \",outputs[:10])\n",
    "        \n",
    "    return Dataset.from_dict({\n",
    "        'input': inputs,\n",
    "        'output': outputs,\n",
    "        'instruction': instructions\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116831fa",
   "metadata": {},
   "source": [
    "## with multiple instruction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538898ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6383c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('mydata/relations.txt') as f:\n",
    "#relations = [r.strip() for r in set(df_train['relation'].to_list())]\n",
    "\n",
    "    \n",
    "def get_instruction(sent, tuples, with_orig=True, with_cls=False):\n",
    "    \n",
    "    instructions, inputs, outputs = [], [], []\n",
    "    if with_orig:\n",
    "        instructions.append(f\"Given phrases that describe the relationship between two words/phrases as options, extract the word/phrase pair and the corresponding lexical relationship between them from the input text. The output format should be \\\"relation1: word1, word2; relation2: word3, word4\\\". Options: {', '.join(relations)}. \")\n",
    "        instructions.append(f\"Given the input sentence, please extract the subject and object containing a certain relation in the sentence according to the following relation types, in the format of \\\"relation1: word1, word2; relation2: word3, word4\\\". Relations include: {'; '.join(relations)}.\")\n",
    "        instructions.append(f\"Provide a paragraph containing relationships between entities. Extract and identify the specific relations between entity pairs mentioned in the paragraph. Output the results in the format \\'relation1: entity1, entity2; relation2: entity3, entity4\\'. Relations should be determined based on the context provided in the paragraph. Relations include: {'; '.join(relations)}.\")\n",
    "\n",
    "        inputs.extend([sent] * 3)\n",
    "        outputs.extend([\"; \".join([f\"{tup[-1]}: {tup[0]}, {tup[1]}\" for tup in tuples])] * 3)\n",
    "    \n",
    "    if with_cls:\n",
    "        for tup in tuples:\n",
    "            #instructions.append(f\"Utilize the input text as a context reference, choose the right relationship between {tup[0]} and {tup[1]} from the options. Options: {', '.join(relations)}.\")\n",
    "            #instructions.append(f\"Refer to the input text as context and select the correct relationship between '{tup[0]}' and '{tup[1]}' from the available options.\\nOptions: {', '.join(relations)}\")\n",
    "            #instructions.append(f\"Refer to the input text as context and select the correct relationship between '{tup[0]}' and '{tup[1]}' from the available options.\\nOptions: {', '.join(relations)}\")\n",
    "            #instructions.append(f\"Take context from the input text and decide on the accurate relationship between '{tup[0]}' and '{tup[1]}' from the options provided.\\nOptions: {', '.join(relations)}\")\n",
    "            #instructions.append(f\"What is the relationship between '{tup[0]}' and '{tup[1]}' in the context of the input sentence.\\nOptions: {', '.join(relations)}\")\n",
    "            instructions.append(f\"In the context of the input sentence, determine the relationship between '{tup[0]}' and '{tup[1]}'.\\nOptions: {', '.join(relations)}\")\n",
    "            #instructions.append(f\"Analyze the relationship between '{tup[0]}' and '{tup[1]}' within the context of the input sentence.\\nOptions: {', '.join(relations)}\")\n",
    "            inputs.extend([sent] * 1)\n",
    "            outputs.extend([tup[-1]] * 1)\n",
    "    \n",
    "    return instructions, inputs, outputs\n",
    "\n",
    "\n",
    "def get_finred_dataset(sent_file, tup_file, with_orig, with_cls):\n",
    "\n",
    "    instructions, inputs, outputs = [], [], []\n",
    "\n",
    "    with open(sent_file) as f:\n",
    "        sentences = [s.strip() for s in f.readlines()]\n",
    "    with open(tup_file) as f:\n",
    "        tuples_list = [s.split(' | ') for s in f.readlines()]\n",
    "    cpt=1   \n",
    "    for sent, tuples in zip(sentences, tuples_list):\n",
    "        #print(cpt)\n",
    "        tuples = [[e.strip() for e in tup.split(' ; ')] for tup in tuples]\n",
    "        \n",
    "        ins, i, o = get_instruction(sent, tuples, with_orig, with_cls)\n",
    "        \n",
    "        instructions.extend(ins)\n",
    "        inputs.extend(i)\n",
    "        outputs.extend(o)\n",
    "        cpt+=1\n",
    "    print(\"longuer = \",len(outputs))\n",
    "    print(\"exemple : \",outputs[:10])\n",
    "        \n",
    "    return Dataset.from_dict({\n",
    "        'input': inputs,\n",
    "        'output': outputs,\n",
    "        'instruction': instructions\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ca9ae7-3982-48e8-b092-9f060684fe29",
   "metadata": {},
   "source": [
    "### Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943435f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longuer =  4000\n",
      "exemple :  ['shareholder_of', 'competitor_of', 'client_of', 'shareholder_of', 'product_or_service_of', 'product_or_service_of', 'collaboration', 'acquired_by', 'client_of', 'subsidiary_of']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_finred_dataset('mydata/train.sent', 'mydata/train.tup', with_orig=False, with_cls=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0669cd42-42f8-472f-8cb5-41b75aff0a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longuer =  708\n",
      "exemple :  ['undefined', 'product_or_service_of', 'shareholder_of', 'collaboration', 'subsidiary_of', 'subsidiary_of', 'client_of', 'product_or_service_of', 'undefined', 'competitor_of']\n"
     ]
    }
   ],
   "source": [
    "test_dataset = get_finred_dataset('mydata/test.sent', 'mydata/test.tup', with_orig=False, with_cls=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "476b3bbe-d49c-4913-af59-6c3eb82038f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4000/4000 [00:00<00:00, 184722.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 708/708 [00:00<00:00, 62461.98 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 708\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "finred_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "finred_dataset.save_to_disk('fingpt-finred')\n",
    "finred_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "675bc7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'instruction'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a90387-e4dd-403d-b030-408995deb027",
   "metadata": {},
   "source": [
    "## CORE Paragraphes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cba27bb-cefa-414d-91a4-ae4bed06ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/projects/melodi/mettaleb/Annotation/120_paragraphs.txt\") as f:\n",
    "    paragraphes = f.readlines()\n",
    "with open(\"/projects/melodi/mettaleb/Annotation/Labelsmodifier.txt\") as f:\n",
    "    Labels = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c905c14-59e1-4505-84b1-3b84b0653048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c88e53a-db0f-446a-8172-ba79a2c2dbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a03a590a-c465-4b7d-8ec0-1f02d2c268f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphesF = []\n",
    "LabelF = []\n",
    "for i in range(len(Labels)):\n",
    "    if \"pas_de_label\" not in Labels[i]:\n",
    "        list_labels =  Labels[i].strip().replace('\\n','').replace('\"','').split(\";\")\n",
    "        list_labels = [label for label in list_labels if label]\n",
    "        for j in range(len(list_labels)):\n",
    "            paragraphesF.append(paragraphes[i].strip())\n",
    "            LabelF.append(list_labels[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56219101-2998-48c2-853b-f69d9412dca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(553, 553)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphesF), len(LabelF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "578adbcb-0345-458c-b272-bf10de8eda61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the Kansas City Southern,The Baton Rouge Southern Railroad:client_of',\n",
       " 'The Baton Rouge Southern Railroad,Watco:shareholder_of',\n",
       " 'Energy Access Ventures,the company:shareholder_of',\n",
       " 'Blue Haven Initiative,the company:shareholder_of',\n",
       " 'Investisseurs & Partenaires,the company:shareholder_of',\n",
       " 'ENGIE Rassembleurs d’Energies,the company:shareholder_of',\n",
       " 'Impact Assets,the company:shareholder_of',\n",
       " 'Acumen,the company:shareholder_of',\n",
       " 'PCG Investments,the company:shareholder_of',\n",
       " 'Blue-Tongue Films,Evermore:client_of',\n",
       " 'Blue-Tongue Films,The Veronicas:client_of',\n",
       " 'Blue-Tongue Films,Empire of the Sun:client_of',\n",
       " 'Blue-Tongue Films,Rahzel:client_of',\n",
       " 'Animal Kingdom,Blue-Tongue Films:product_or_service_of',\n",
       " 'Hesher,Blue-Tongue Films:product_or_service_of']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LabelF[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b1f25978-68d4-40e5-9b7b-d771cc7e8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./mydata/116train667.sent\", \"w\") as file:\n",
    "    for i in range(len(paragraphesF[:400])):\n",
    "        file.write(f\"{paragraphesF[i]}\\n\")\n",
    "with open(\"./mydata/116test667.sent\", \"w\") as file:\n",
    "    for i in range(400,len(paragraphesF)):\n",
    "        file.write(f\"{paragraphesF[i]}\\n\")\n",
    "with open(\"./mydata/116train667.tup\", \"w\") as file:\n",
    "    for i in range(len(LabelF[:400])):\n",
    "        label = LabelF[i].replace(\",\", \" ; \").replace(\":\", \" ; \")\n",
    "        file.write(f\"{label}\\n\")\n",
    "with open(\"./mydata/116test667.tup\", \"w\") as file:\n",
    "    for i in range(400,len(LabelF)):\n",
    "        label = LabelF[i].replace(\",\", \" ; \").replace(\":\", \" ; \")\n",
    "        file.write(f\"{label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488a5c7-5950-4610-846b-de8ec49e9455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "54798118-426a-41ec-bbe1-cf7929f08629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('mydata/relations.txt') as f:\n",
    "#relations = [r.strip() for r in set(df_train['relation'].to_list())]\n",
    "\n",
    "    \n",
    "def get_instruction(sent, tuples, with_orig=True, with_cls=False):\n",
    "    \n",
    "    instructions, inputs, outputs = [], [], []\n",
    "    if with_orig:\n",
    "        instructions.append(f\"Given a paragraph that describes the relationship between two words/phrases as options, extract the word/phrase pair and the corresponding lexical relationship between them from the input text. The output format should be \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Options: {', '.join(relations)}. \")\n",
    "        instructions.append(f\"Given the input paragraph, please extract the subject and object containing a certain relation in the sentence according to the following relation types, in the format of \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Relations include: {'; '.join(relations)}.\")\n",
    "        instructions.append(f\"Provide a paragraph containing relationships between entities. Extract and identify the specific relations between entity pairs mentioned in the paragraph. Output the results in the format \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Relations should be determined based on the context provided in the paragraph. Relations include: {'; '.join(relations)}.\")\n",
    "\n",
    "        inputs.extend([sent] * 3)\n",
    "        outputs.extend([\"; \".join([f\"{tup[-1]}: {tup[0]}, {tup[1]}\" for tup in tuples])] * 3)\n",
    "    \n",
    "    if with_cls:\n",
    "        for tup in tuples:\n",
    "            #instructions.append(f\"Utilize the input text as a context reference, choose the right relationship between {tup[0]} and {tup[1]} from the options. Options: {', '.join(relations)}.\")\n",
    "            instructions.append(f\"\"\"What is the relationship between '{{{tup[0]}}}' and '{{{tup[1]}}}' in the context of the input sentence. Choose an answer from: {{{'; '.join(relations)}}}.\\nOutput the results in the format: {{relation}}\"\"\")\n",
    "            #instructions.append(f\"\"\"Analyze the following sentence and identify the relationship between the two mentioned entities. The relationship must be selected from the predefined list below. If none of the relationships apply, respond with \"undefined\".\\nEntité 1 : {tup[0]}.\\n \\nEntité 2 : {tup[1]}.\\nRelations : {'; '.join(relations)}.\"\"\")\n",
    "            inputs.extend([sent] * 1)\n",
    "            outputs.extend([tup[-1]] * 1)\n",
    "    \n",
    "    return instructions, inputs, outputs\n",
    "\n",
    "\n",
    "def get_finred_dataset(sent_file, tup_file, with_orig, with_cls):\n",
    "\n",
    "    instructions, inputs, outputs = [], [], []\n",
    "\n",
    "    with open(sent_file) as f:\n",
    "        sentences = [s.strip() for s in f.readlines()]\n",
    "    with open(tup_file) as f:\n",
    "        tuples_list = [s.split(' | ') for s in f.readlines()]\n",
    "        \n",
    "    for sent, tuples in zip(sentences, tuples_list):\n",
    "        tuples = [[e.strip() for e in tup.split(' ; ')] for tup in tuples]\n",
    "        \n",
    "        ins, i, o = get_instruction(sent, tuples, with_orig, with_cls)\n",
    "        \n",
    "        instructions.extend(ins)\n",
    "        inputs.extend(i)\n",
    "        outputs.extend(o)\n",
    "    print(\"longuer = \",len(outputs))\n",
    "    print(\"exemple : \",outputs[:10])\n",
    "    print(\"exemple : \",instructions[:2])\n",
    "        \n",
    "    return Dataset.from_dict({\n",
    "        'input': inputs,\n",
    "        'output': outputs,\n",
    "        'instruction': instructions\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a4f8cb6a-f87b-4f52-8652-70c0d0df56b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longuer =  400\n",
      "exemple :  ['client_of', 'shareholder_of', 'shareholder_of', 'shareholder_of', 'shareholder_of', 'shareholder_of', 'shareholder_of', 'shareholder_of', 'shareholder_of', 'client_of']\n",
      "exemple :  [\"What is the relationship between '{the Kansas City Southern}' and '{The Baton Rouge Southern Railroad}' in the context of the input sentence. Choose an answer from: {acquired_by; brand_of; client_of; collaboration; competitor_of; merged_with; product_or_service_of; regulated_by; shareholder_of; subsidiary_of; traded_on; undefined}.\\nOutput the results in the format: {relation}\", \"What is the relationship between '{The Baton Rouge Southern Railroad}' and '{Watco}' in the context of the input sentence. Choose an answer from: {acquired_by; brand_of; client_of; collaboration; competitor_of; merged_with; product_or_service_of; regulated_by; shareholder_of; subsidiary_of; traded_on; undefined}.\\nOutput the results in the format: {relation}\"]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_finred_dataset('mydata/116train667.sent', 'mydata/116train667.tup', with_orig=False, with_cls=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "79d95330-0808-44b5-ae65-3fe9da5ebe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longuer =  153\n",
      "exemple :  ['product_or_service_of', 'product_or_service_of', 'traded_on', 'traded_on', 'traded_on', 'collaboration', 'shareholder_of', 'shareholder_of', 'shareholder_of', 'product_or_service_of']\n",
      "exemple :  [\"What is the relationship between '{Coins}' and '{Coins ’N Things}' in the context of the input sentence. Choose an answer from: {acquired_by; brand_of; client_of; collaboration; competitor_of; merged_with; product_or_service_of; regulated_by; shareholder_of; subsidiary_of; traded_on; undefined}.\\nOutput the results in the format: {relation}\", \"What is the relationship between '{silver bullion coins}' and '{Coins ’N Things}' in the context of the input sentence. Choose an answer from: {acquired_by; brand_of; client_of; collaboration; competitor_of; merged_with; product_or_service_of; regulated_by; shareholder_of; subsidiary_of; traded_on; undefined}.\\nOutput the results in the format: {relation}\"]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = get_finred_dataset('mydata/116test667.sent', 'mydata/116test667.tup', with_orig=False, with_cls=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b84b837e-4e22-4d99-be38-e1cd7fbe3d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 400/400 [00:00<00:00, 29718.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 153/153 [00:00<00:00, 14160.88 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 153\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "finred_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "finred_dataset.save_to_disk('fingpt-finred')\n",
    "finred_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82643c6-d9d5-4be7-8c3a-a004d56b216a",
   "metadata": {},
   "source": [
    "## TextMine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fcfc4e-8c12-47fc-9cc2-1885c6f5f8ba",
   "metadata": {},
   "source": [
    "### Strategie 1 : \n",
    " * Donner le paragraphe , toutes les entities (avec ou **SANS** les types des entities), il doit tourver la relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e787443-0f0b-4df9-af81-2e74de7cef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def get_instruction(sent, tuples, entities, with_orig=True, with_cls=False):\n",
    "    \n",
    "    instructions, inputs, outputs = [], [], []\n",
    "    if with_orig:\n",
    "        instructions.append(f\"Given a paragraph that describes the relationship between two entities, extract All entities pair and the corresponding lexical relationship between them from the input paragraph. The output format should be \\\"{{relation1: entity1, entity2}}; {{relation2: entity3, entity4}}\\\". Entities must be in this list: {', '.join(entities)}. Relations must be in this list: {', '.join(relations)}. \")\n",
    "        instructions.append(f\"Given the input paragraph, please extract All entities pair containing a certain relation in the paragraph according to the following relation types, in the format of \\\"{{relation1: entity1, entity2}}; {{relation2: entity3, entity}}\\\". Entities must be in this list: {', '.join(entities)}. Relations should be include: {'; '.join(relations)}.\")\n",
    "        instructions.append(f\"I Provide you with paragraph containing entities. Generate the relations between entity pairs from the following list: {', '.join(entities)}. Output the results in the format \\\"{{relation1: entity1, entity}}; {{relation2: entity3, entity4}}\\\". Relations should be determined based on the context provided in the paragraph. Relations must be in this list: {'; '.join(relations)}.\")\n",
    "\n",
    "        inputs.extend([sent] * 3)\n",
    "        outputs.extend([\"; \".join([f\"{tup[-1]}: {tup[0]}, {tup[1]}\" for tup in tuples])] * 3)\n",
    "    \n",
    "    if with_cls:\n",
    "        for tup in tuples:\n",
    "            #instructions.append(f\"Utilize the input text as a context reference, and choose the correct relationship between {tup[0]} and {tup[1]} from the options. Options: {', '.join(relations)}.\")\n",
    "            instructions.append(f\"What is the relationship between {tup[0]} and {tup[1]} in the context of the input sentence. Choose an answer from: {'; '.join(relations)}.\")\n",
    "            inputs.extend([sent] * 1)\n",
    "            outputs.extend([tup[-1]] * 1)\n",
    "    \n",
    "    return instructions, inputs, outputs\n",
    "\n",
    "def get_finred_dataset(sent_file, tup_file, with_orig, with_cls):\n",
    "\n",
    "    instructions, inputs, outputs = [], [], []\n",
    "\n",
    "    with open(sent_file) as f:\n",
    "        sentences = [s.strip() for s in f.readlines()]\n",
    "\n",
    "    with open(tup_file) as f:\n",
    "        tuples_list = [s.split(' | ') for s in f.readlines()]\n",
    "    \n",
    "    entities_list=[]\n",
    "    with open(tup_file) as f:\n",
    "        for ligne in f.readlines():\n",
    "            triplets = ligne.split(' | ')\n",
    "            entities_set = set()\n",
    "            for triplet in triplets:\n",
    "                entity1, entity2, _ = triplet.split(' ; ')\n",
    "                entities_set.add(entity1)\n",
    "                entities_set.add(entity2)\n",
    "            entities_list.append(entities_set)    \n",
    "    cpt=1   \n",
    "    for sent, tuples, entities in zip(sentences, tuples_list, entities_list):\n",
    "        tuples = [[e.strip() for e in tup.split(' ; ')] for tup in tuples]\n",
    "        ins, i, o = get_instruction(sent, tuples, entities, with_orig, with_cls)\n",
    "        instructions.extend(ins)\n",
    "        inputs.extend(i)\n",
    "        outputs.extend(o)\n",
    "        cpt+=1\n",
    "    #print(entities)\n",
    "    print(\"longuer = \",len(outputs))\n",
    "    print(\"exemple : \",outputs[:1])\n",
    "    for i in range(len(outputs)):\n",
    "        if not outputs[i]:\n",
    "            print(f\"l'output {i} est vide\")\n",
    "    return Dataset.from_dict({\n",
    "        'input': inputs,\n",
    "        'output': outputs,\n",
    "        'instruction': instructions\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a755b298-cbd1-4e6a-a3d2-31b1b06fa422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RESIDES_IN', 'HAS_CONTROL_OVER', 'IS_COOPERATING_WITH', 'IS_BORN_ON', 'HAS_LONGITUDE', 'HAS_LATITUDE', 'IS_OF_SIZE', 'IS_BORN_IN', 'IS_PART_OF', 'IS_AT_ODDS_WITH', 'IS_LOCATED_IN', 'STARTED_IN', 'IS_OF_NATIONALITY', 'HAS_COLOR', 'WEIGHS', 'IS_REGISTERED_AS', 'START_DATE', 'INJURED_NUMBER', 'DIED_IN', 'DEATHS_NUMBER', 'CREATED', 'GENDER_FEMALE', 'INITIATED', 'HAS_FOR_HEIGHT', 'HAS_FOR_LENGTH', 'HAS_CATEGORY', 'HAS_CONSEQUENCE', 'WAS_CREATED_IN', 'HAS_FAMILY_RELATIONSHIP', 'OPERATES_IN', 'WAS_DISSOLVED_IN', 'IS_DEAD_ON', 'HAS_QUANTITY', 'HAS_FOR_WIDTH', 'END_DATE', 'IS_IN_CONTACT_WITH', 'GENDER_MALE']\n"
     ]
    }
   ],
   "source": [
    "print(relat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "36674655-801f-4d35-8c2f-078826ea85e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f484d3-5468-47fd-b7a5-4d5bb5505eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = ['RESIDES_IN', 'HAS_CONTROL_OVER', 'IS_COOPERATING_WITH', 'IS_BORN_ON', 'HAS_LONGITUDE', 'HAS_LATITUDE', 'IS_OF_SIZE', 'IS_BORN_IN', 'IS_PART_OF', 'IS_AT_ODDS_WITH', 'IS_LOCATED_IN', 'STARTED_IN', 'IS_OF_NATIONALITY', 'HAS_COLOR', 'WEIGHS', 'IS_REGISTERED_AS', 'START_DATE', 'INJURED_NUMBER', 'DIED_IN', 'DEATHS_NUMBER', 'CREATED', 'GENDER_FEMALE', 'INITIATED', 'HAS_FOR_HEIGHT', 'HAS_FOR_LENGTH', 'HAS_CATEGORY', 'HAS_CONSEQUENCE', 'WAS_CREATED_IN', 'HAS_FAMILY_RELATIONSHIP', 'OPERATES_IN', 'WAS_DISSOLVED_IN', 'IS_DEAD_ON', 'HAS_QUANTITY', 'HAS_FOR_WIDTH', 'END_DATE', 'IS_IN_CONTACT_WITH', 'GENDER_MALE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99798663-cab1-4db7-8d8e-aa4c60260429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "545de1de-5f63-42fd-91b5-abe1517ed608",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_trad = {'RESIDES_IN': 'RÉSIDE_DANS','IS_OF_SIZE': 'EST_DE_TAILLE',    'IS_BORN_ON': 'EST_NÉ_LE',    'CREATED': 'CRÉÉ',    'HAS_CONSEQUENCE': 'A_DES_CONSÉQUENCES',    'HAS_FOR_LENGTH': 'A_POUR_LONGUEUR',    'DIED_IN': 'MORT_EN',    'START_DATE': 'DATE_DE_DÉBUT',    'INITIATED': 'INITIÉ',    'HAS_CATEGORY': 'A_UNE_CATEGORIE',    'HAS_LATITUDE': 'A_LA_LATITUDE',    'GENDER_FEMALE': 'GENRE_FEMININ',    'DEATHS_NUMBER': 'NOMBRE_DE_MORTS',    'GENDER_MALE': 'GENRE_MASCULIN',    'IS_PART_OF': 'FAIT_PARTIE_DE',    'WEIGHS': 'PESENT',    'IS_REGISTERED_AS': 'EST_ENREGISTRÉ_COMME',    'HAS_QUANTITY': 'A_UNE_QUANTITÉ',    'IS_OF_NATIONALITY': 'EST_DE_NATIONALITÉ',    'INJURED_NUMBER': 'NOMBRE_DE_BLESSÉS',    'END_DATE': 'DATE_DE_FIN',    'HAS_CONTROL_OVER': 'A_UNE_CE_CONTROLE_SUR',    'IS_COOPERATING_WITH': 'COOPÈRE_AVEC',    'IS_BORN_IN': 'EST_NÉ_A',    'HAS_FOR_WIDTH': 'A_POUR_LARGEUR',    'IS_AT_ODDS_WITH': 'EST_EN_CONFLIT_AVEC',    'HAS_COLOR': 'A_COULEUR',    'HAS_FAMILY_RELATIONSHIP': 'A_UNE_RELATION_FAMILIALE',    'WAS_DISSOLVED_IN': 'A_ÉTÉ_DISSOUTE_EN',    'HAS_FOR_HEIGHT': 'A_POUR_HAUTEUR',    'IS_DEAD_ON': 'EST_MORT_LE',    'STARTED_IN': 'A_COMMENCÉ_EN',    'OPERATES_IN': 'OPÈRE_EN',    'IS_LOCATED_IN': 'EST_LOCALISÉ_EN',    'WAS_CREATED_IN': 'A_ÉTÉ_CRÉÉ_EN',    'HAS_LONGITUDE': 'A_LA_LONGITUDE','IS_IN_CONTACT_WITH': 'EST_EN_CONTACT_AVEC', 'PAS_DE_RELATION': 'PAS_DE_RELATION'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0c044-0af2-4771-b2e2-98f8f4206e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_finred_dataset('/projects/melodi/mettaleb/Textmine/train.sent', '/projects/melodi/mettaleb/Textmine/train_small.tup', with_orig=True, with_cls=False)\n",
    "test_dataset = get_finred_dataset('/projects/melodi/mettaleb/Textmine/test.sent', '/projects/melodi/mettaleb/Textmine/test_small.tup', with_orig=True, with_cls=False)\n",
    "\n",
    "finred_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "finred_dataset.save_to_disk('fingpt-finred-re')\n",
    "finred_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2073c60-ee34-400d-b7a1-c910f2f848a1",
   "metadata": {},
   "source": [
    "### Strategie 2 : \n",
    " * Donner le paragraphe , toutes les entities (**AVEC** les types des entities), il doit tourver la relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba536d37-05bd-4f65-a904-7be1c2eca78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dif_entities = {\n",
    "    \"Actor\": [\"Organization\",  \"Organisation\", \"Person\"],\n",
    "    \"Organization\": [\"Gouvernement Organization\"  , \"Group of Individuals\", \"Intergovernmental Organization\", \"Intergovernmental Organisation\", \"Non Governmental Organization\", \"Non Governmental Organisation\"],\n",
    "    \"Organisation\": [\"Gouvernement Organisation\", \"Group of Individuals\", \"Intergovernmental Organization\", \"Intergovernmental Organisation\", \"Non Governmental Organization\", \"Non Governmental Organisation\"],\n",
    "    \"Gouvernement Organization\": [\"Military Organization\", \"Non Military Organization\",\"NON_MILITARY_GOVERNMENT_ORGANIZATION\"],\n",
    "    \"Gouvernement Organisation\": [\"Military Organisation\", \"Non Military Organisation\",\"NON_MILITARY_GOVERNMENT_ORGANISATION\"],\n",
    "    \"Person\": [\"Civilian\", \"Criminal\", \"Military\"],\n",
    "    \"Event\": [\"Accident\", \"CBRN Event\", \"Civil Unrest\", \"Criminal Event\", \"Large Scale Event\"],\n",
    "    \"Civil Unrest\": [\"Agitating Trouble Making\", \"Civil War Outbreak\", \"Coup d'État\", \"Demonstration\", \"Election\",\n",
    "                     \"Gathering\", \"Illegal Civil Demonstration\", \"Natural Causes Death\", \"Riot\", \"Strike\", \"Suicide\"],\n",
    "    \"Criminal Event\": [\"Bombing\", \"Criminal Arrest\", \"Drug Operation\", \"Hooliganism Trouble-making\", \"Political Violence\", \"Theft\", \"Trafficking\"],\n",
    "    \"Large Scale Event\": [\"Economical Crisis\", \"Epidemic\", \"Fire\", \"Natural Event\", \"Pollution\"],\n",
    "    \"Material\": [\"MATERIEL\",\"Material\"],\n",
    "    \"Place\": [\"Place\"],\n",
    "    \"TERRORIST_OR_CRIMINAL\": ['TERRORIST_OR_CRIMINAL']\n",
    "}\n",
    "dif_Attrinuts = {\"Category\": 0,\"Color\": 0,\"First Name\": 0,\"Height\": 0,\"Last Name\": 0,\"Length\": 0,\"Material Reference\": 0,\"Nationality\": 0,\"Quantity\": [\"Quantity Exact\",\"Quantity Fuzzy\",\"Quantity Max\",\"Quantity Min\"],\"Time\": [\"Time Exact\",\"Time Fuzzy\",\"Time Max\",\"Time Min\"]}\n",
    "Relations_Definition = ['Actor,Is_Located_In,Place','Event,Is_Located_In,Place','Place,Is_Located_In,Place','Actor,Is_of_Nationality,Nationality','Place,Is_of_Nationality,Nationality', 'Actor,Created,Organization','Actor,Has_Control_Over,Actor','Actor,Has_Control_Over,Material','Actor,Has_Control_Over,Place','Actor,Initiated,Event', 'Actor,Is_At_Odds_With,Actor','Actor,Is_Cooperating_With,Actor','Actor,Is_In_Contact_With,Actor','Actor,Is_Part_Of,Organization','Event,Deaths_Number,Quantity', 'Event,End_Date,Time','Event,Has_Consequence,Event','Event,Injured_Number,Quantity','Event,Start_Date,Time','Event,Started_In,Place', 'Material,Has_Color,Color','Material,Has_for_Height,Height','Material,Has_for_Length,Length','Material,Has_for_Width,Width','Material,Has_Quantity,Quantity', 'Material,Is_Registered_As,Material reference', 'Material,Weighs,Weight', 'Organization,Was_Created_In,Time', 'Organization,Was_Dissolved_In,Time', 'Organization,Is_Of_Size,Quantity', 'Organization,Operates_In,Place', 'Person,Died_In,Event', 'Person,Has_Category,Category', 'Person,Has_Family_Relationship,Person',  'Person,Gender_Female,N/A', 'Person,Gender_Male,N/A', 'Person,Is_Born_In,Place','Person,Is_Born_On,Time', 'Person,Is_Dead_On,Time', 'Person,Resides_In,Place', 'Place,Is_Located_In,Place','Actor,Is_of_Nationality,Nationality','Place,Is_of_Nationality,Nationality', 'Place,Has_Latitude,Latitude','Place,Has_Longitude,Longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d083841f-9024-4465-a242-488fcfc307cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def find_original_parent(term, dif_entities):\\n    for parent, subclasses in dif_entities.items():\\n        if term in subclasses:\\n            return find_original_parent(parent, dif_entities)\\n    return term\\ndef find_attribute(term, dif_Attrinuts):\\n    for key, values in dif_Attrinuts.items():\\n        if term == key:\\n            return key\\n        elif isinstance(values, list) and term in values:\\n            return key\\n    return None\\ndef find_type_entity(term):\\n    if find_attribute(term, dif_Attrinuts):\\n        type_entity = find_attribute(term, dif_Attrinuts)\\n    else:\\n        type_entity = find_original_parent(term, dif_entities)\\n    return type_entity'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def find_original_parent(term, dif_entities):\n",
    "    for parent, subclasses in dif_entities.items():\n",
    "        if term in subclasses:\n",
    "            return find_original_parent(parent, dif_entities)\n",
    "    return term\n",
    "def find_attribute(term, dif_Attrinuts):\n",
    "    for key, values in dif_Attrinuts.items():\n",
    "        if term == key:\n",
    "            return key\n",
    "        elif isinstance(values, list) and term in values:\n",
    "            return key\n",
    "    return None\n",
    "def find_type_entity(term):\n",
    "    if find_attribute(term, dif_Attrinuts):\n",
    "        type_entity = find_attribute(term, dif_Attrinuts)\n",
    "    else:\n",
    "        type_entity = find_original_parent(term, dif_entities)\n",
    "    return type_entity\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb6b723f-9a05-4c08-8a2a-4476520bf30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les classes mères de 'Non Military Organization' sont : ['Gouvernement Organization', 'Organization', 'Actor']\n",
      "Les classes mères de 'person' sont : ['Actor']\n"
     ]
    }
   ],
   "source": [
    "def normalize(term):\n",
    "    return re.sub(r'[\\W_]+', '', term).lower()\n",
    "def find_superclasses(entity, dif_entities):\n",
    "    superclasses = []\n",
    "    normalized_entity = normalize(entity)\n",
    "    \n",
    "    def search_parents(subclass):\n",
    "        normalized_subclass = normalize(subclass)\n",
    "        for superclass, subclasses in dif_entities.items():\n",
    "            normalized_superclass = normalize(superclass)\n",
    "            if any(normalize(sub) == normalized_subclass for sub in subclasses):\n",
    "                if normalized_superclass not in superclasses:\n",
    "                    superclasses.append(superclass)\n",
    "                    search_parents(superclass)\n",
    "    \n",
    "    # Commencer la recherche pour l'entité donnée\n",
    "    search_parents(entity)\n",
    "    return list(superclasses)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "entity = \"Non Military Organization\"\n",
    "print(f\"Les classes mères de '{entity}' sont :\", find_superclasses(entity, dif_entities))\n",
    "\n",
    "entity = \"person\"\n",
    "print(f\"Les classes mères de '{entity}' sont :\", find_superclasses(entity, dif_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d861c3-1481-4d55-8994-194e4ca3da72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97fd5d74-0b60-4643-914a-0b2f1c6483a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_graph(data):\n",
    "    G = nx.DiGraph()\n",
    "    for parent, children in data.items():\n",
    "        for child in children:\n",
    "            G.add_edge(parent.upper(), child.upper())\n",
    "    return G\n",
    "graph = create_graph(dif_entities)\n",
    "\n",
    "def find_super_and_sub_classes(graph, entity):\n",
    "    super_classes = list(nx.ancestors(graph, entity))  \n",
    "\n",
    "    sub_classes = list(graph.successors(entity))  \n",
    "    L_term = []\n",
    "    L_term.append(entity)    \n",
    "    return super_classes if super_classes else L_term\n",
    "\n",
    "\n",
    "def normalize_term(term):\n",
    "    return re.sub(r'[\\W_]+', '', term).lower()\n",
    "\n",
    "\n",
    "def normalize(term):\n",
    "    return re.sub(r'[\\W_]+', '', term).lower()\n",
    "\n",
    "# Fonction pour récupérer les classes mères d'une entité\n",
    "def find_superclasses(entity, dif_entities):\n",
    "    superclasses = set()\n",
    "    visited = set()  # Ensemble pour garder une trace des classes déjà visitées\n",
    "    normalized_entity = normalize(entity)\n",
    "    \n",
    "    def search_parents(subclass):\n",
    "        normalized_subclass = normalize(subclass)\n",
    "        \n",
    "        # Si déjà visité, arrêter pour éviter les boucles infinies\n",
    "        if normalized_subclass in visited:\n",
    "            return\n",
    "        \n",
    "        visited.add(normalized_subclass)  # Marquer comme visité\n",
    "        for superclass, subclasses in dif_entities.items():\n",
    "            normalized_superclass = normalize(superclass)\n",
    "            if any(normalize(sub) == normalized_subclass for sub in subclasses):\n",
    "                if normalized_superclass not in superclasses:\n",
    "                    superclasses.add(superclass)\n",
    "                    search_parents(superclass)\n",
    "    \n",
    "    # Commencer la recherche pour l'entité donnée\n",
    "    search_parents(entity)\n",
    "    return list(superclasses)\n",
    "\n",
    "def find_original_parent(term, dif_entities):\n",
    "    normalized_term = normalize_term(term)\n",
    "    for parent, subclasses in dif_entities.items():\n",
    "        normalized_subclasses = [normalize_term(subclass) for subclass in subclasses]\n",
    "        if normalized_term in normalized_subclasses:\n",
    "            return find_original_parent(parent, dif_entities)\n",
    "    L_term = []\n",
    "    L_term.append(term)\n",
    "    return L_term\n",
    "\n",
    "def find_attribute(term, dif_Attrinuts):\n",
    "    normalized_term = normalize_term(term)\n",
    "    for key, values in dif_Attrinuts.items():\n",
    "        normalized_key = normalize_term(key)\n",
    "        \n",
    "        if normalized_term == normalized_key:\n",
    "            return key\n",
    "        elif isinstance(values, list):\n",
    "            normalized_values = [normalize_term(value) for value in values]\n",
    "            if normalized_term in normalized_values:\n",
    "                return key\n",
    "    return None\n",
    "\n",
    "def find_type_entity(term):\n",
    "    if find_attribute(term, dif_Attrinuts):\n",
    "        type_entity = []\n",
    "        type_entity_a = find_attribute(term, dif_Attrinuts)\n",
    "        type_entity.append(type_entity_a)\n",
    "    else:\n",
    "        #type_entity = find_original_parent(term, dif_entities)\n",
    "        type_entity = find_superclasses(term, dif_entities)\n",
    "    return type_entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa0e38-9727-4f7c-9aa2-757d43d599a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3648c6d9-1e0d-40f9-90f1-75304b76b260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f23c7adf-28d5-4a67-ab99-d349190eb323",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def get_instruction(sent, tuples, entities, with_orig=True, with_cls=False):\n",
    "    \n",
    "    instructions, inputs, outputs = [], [], []\n",
    "    if with_orig:\n",
    "        instructions.append(f\"Given a paragraph that describes the relationship between two entities, extract All entities pair and the corresponding lexical relationship between them from the input paragraph. The output format should be \\\"{{relation1: entity1, entity2}}; {{relation2: entity3, entity4}}\\\". Entities must be in this list(in the form entity:Type_entity): {', '.join(entities)}. Relations must be in this list: {', '.join(relations)}. \")\n",
    "        instructions.append(f\"Given the input paragraph, please extract All entities pair containing a certain relation in the paragraph according to the following relation types, in the format of \\\"{{relation1: entity1, entity2}}; {{relation2: entity3, entity}}\\\". Entities must be in this list(in the form entity:Type_entity): {', '.join(entities)}. Relations should be include: {'; '.join(relations)}.\")\n",
    "        instructions.append(f\"I Provide you with paragraph containing entities. Generate the relations between entity pairs from the following list(in the form entity:Type_entity): {', '.join(entities)}. Output the results in the format \\\"{{relation1: entity1, entity}}; {{relation2: entity3, entity4}}\\\". Relations should be determined based on the context provided in the paragraph. Relations must be in this list: {'; '.join(relations)}.\")\n",
    "\n",
    "        inputs.extend([sent] * 3)\n",
    "        outputs.extend([\"; \".join([f\"{tup[-1]}: {tup[0]}, {tup[1]}\" for tup in tuples])] * 3)\n",
    "    \n",
    "    if with_cls:\n",
    "        for tup in tuples:\n",
    "            #instructions.append(f\"Utilize the input text as a context reference, and choose the correct relationship between {tup[0]} and {tup[1]} from the options. Options: {', '.join(relations)}.\")\n",
    "            instructions.append(f\"What is the relationship between {tup[0]} and {tup[1]} in the context of the input sentence. Choose an answer from: {'; '.join(relations)}.\")\n",
    "            inputs.extend([sent] * 1)\n",
    "            outputs.extend([tup[-1]] * 1)\n",
    "    \n",
    "    return instructions, inputs, outputs\n",
    "\n",
    "def get_finred_dataset(sent_file, tup_file,entity_types_file, with_orig, with_cls):\n",
    "\n",
    "    instructions, inputs, outputs = [], [], []\n",
    "\n",
    "    with open(sent_file) as f:\n",
    "        sentences = [s.strip() for s in f.readlines()]\n",
    "\n",
    "    with open(tup_file) as f:\n",
    "        tuples_list = [s.split(' | ') for s in f.readlines()]\n",
    "    \n",
    "    entities_list=[]\n",
    "    with open(entity_types_file) as f:\n",
    "        for ligne in f.readlines():\n",
    "            triplets = ligne.strip().split(';')\n",
    "            entities_set = []\n",
    "            for triplet in triplets:\n",
    "                #entity,type_entity= triplet.split('_')\n",
    "                entity, type_entity = triplet.split('_', 1)\n",
    "                type_entity = find_type_entity(type_entity)\n",
    "                final_entity = f\"{entity}:{type_entity}\"\n",
    "                entities_set.append(final_entity)\n",
    "            entities_list.append(entities_set)    \n",
    "    cpt=1\n",
    "    print(entities_list[0])\n",
    "    for sent, tuples, entities in zip(sentences, tuples_list, entities_list):\n",
    "        tuples = [[e.strip() for e in tup.split(' ; ')] for tup in tuples]\n",
    "        ins, i, o = get_instruction(sent, tuples, entities, with_orig, with_cls)\n",
    "        instructions.extend(ins)\n",
    "        inputs.extend(i)\n",
    "        outputs.extend(o)\n",
    "        cpt+=1\n",
    "    #print(entities)\n",
    "    print(\"longuer = \",len(outputs))\n",
    "    print(\"exemple : \",outputs[:1])\n",
    "    for i in range(len(outputs)):\n",
    "        if not outputs[i]:\n",
    "            print(f\"l'output {i} est vide\")\n",
    "    return Dataset.from_dict({\n",
    "        'input': inputs,\n",
    "        'output': outputs,\n",
    "        'instruction': instructions\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1b5d073-6abf-4c14-804e-d9740f77cb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ONG:Actor', 'moto:MATERIEL', 'accident de circulation:Event', 'passagers:Actor', 'Destresse:Last Name', 'deux:Quantity', 'garde du corps:Actor', 'Ma passion:Actor', 'Il:TERRORIST_OR_CRIMINAL', 'conducteur:Category', 'président:Category', 'blessés:Actor', 'Anam:First Name', 'Anam Destresse:Actor', 'accident:Event', '30 juin 2022:Time', 'conducteur:TERRORIST_OR_CRIMINAL', 'Italie:PLACE', 'panneaux de signalisation:MATERIEL', 'hélicoptère:MATERIEL', 'autoroute de Saint-Marin:PLACE', '20:Quantity', 'hôpital:PLACE', 'garde du corps:Category', 'bus:MATERIEL']\n",
      "longuer =  1800\n",
      "exemple :  ['STARTED_IN: accident de circulation, autoroute de Saint-Marin; GENDER_FEMALE: Anam Destresse, Anam Destresse; HAS_CATEGORY: Anam Destresse, président; IS_DEAD_ON: conducteur, 30 juin 2022; HAS_CATEGORY: garde du corps, garde du corps; GENDER_MALE: garde du corps, garde du corps; HAS_CATEGORY: conducteur, conducteur; STARTED_IN: accident, Italie; START_DATE: accident, 30 juin 2022; IS_LOCATED_IN: garde du corps, Italie; END_DATE: accident, 30 juin 2022; GENDER_MALE: conducteur, conducteur; IS_OF_SIZE: passagers, 20; IS_LOCATED_IN: Anam Destresse, hôpital; IS_OF_SIZE: passagers, deux']\n",
      "['Europe:PLACE', 'denrées alimentaires:MATERIEL', 'augmenté:Event', 'Certains:Quantity', 'Bruno:First Name', 'certains:Quantity', 'vendus:Event', 'crise économique:Event', 'Bruno Alves:Actor', 'pièces détachées:MATERIEL', 'volés:Event', 'Alves:Last Name', 'quartiers:PLACE', 'Lisbonne:PLACE', 'crise:Event', 'portugais:Nationality', 'sociologue:Category', 'revue:MATERIEL', 'voitures:MATERIEL', 'articles:MATERIEL', 'médicaments:MATERIEL', 'plusieurs mois:Time', 'gouvernement:NON_MILITARY_GOVERNMENT_ORGANISATION', 'billets de train:MATERIEL', 'crise financière:Event', 'gouvernements:NON_MILITARY_GOVERNMENT_ORGANISATION']\n",
      "longuer =  600\n",
      "exemple :  ['STARTED_IN: vendus, quartiers; HAS_QUANTITY: médicaments, certains; HAS_CONSEQUENCE: crise financière, vendus; OPERATES_IN: gouvernement, quartiers; IS_OF_NATIONALITY: gouvernement, portugais; HAS_CONSEQUENCE: volés, vendus; HAS_CONSEQUENCE: augmenté, vendus; STARTED_IN: volés, quartiers; HAS_CONSEQUENCE: crise financière, volés; GENDER_MALE: Bruno Alves, Bruno Alves; IS_LOCATED_IN: volés, Lisbonne; STARTED_IN: crise financière, Europe; START_DATE: crise financière, plusieurs mois; HAS_CONSEQUENCE: augmenté, volés; HAS_CATEGORY: Bruno Alves, sociologue; IS_LOCATED_IN: crise financière, quartiers; HAS_CONSEQUENCE: crise financière, crise économique; IS_LOCATED_IN: quartiers, Lisbonne; IS_LOCATED_IN: vendus, Europe; OPERATES_IN: gouvernement, Lisbonne; IS_LOCATED_IN: augmenté, quartiers; OPERATES_IN: gouvernements, Europe; HAS_CONTROL_OVER: Bruno Alves, revue; IS_LOCATED_IN: volés, quartiers']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1800/1800 [00:00<00:00, 45512.21 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 600/600 [00:00<00:00, 31340.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = get_finred_dataset('/projects/melodi/mettaleb/Textmine/train.sent', '/projects/melodi/mettaleb/Textmine/train_small.tup', '/projects/melodi/mettaleb/Textmine/train_entities_types.txt', with_orig=True, with_cls=False)\n",
    "test_dataset = get_finred_dataset('/projects/melodi/mettaleb/Textmine/test.sent', '/projects/melodi/mettaleb/Textmine/test_small.tup','/projects/melodi/mettaleb/Textmine/test_entities_types.txt', with_orig=True, with_cls=False)\n",
    "\n",
    "finred_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "finred_dataset.save_to_disk('fingpt-finred-re')\n",
    "finred_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df4c70-56f7-4687-b4e2-4ee615d70e4e",
   "metadata": {},
   "source": [
    "### Strategie 3 :\n",
    " * **Etape 1. Combinaison:** Extraire tous les paires d'entités possibles (**SANS** Type) </br>\n",
    " * **Etape 2. Filtrage:** (A) On cherche la classe mère de chaque entité. (B) Vérifier si la combinaison possible ou pas\n",
    " * **Etape 3. Prompt:** prompt pour chaque (ei,ej) avec que les relations possibles avec une relation **Autre**. On donne le paragraphe complet ou bien que le span entre les deux entitiés   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ecc8972-643f-46c0-b89f-dce549e2e569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1204</td>\n",
       "      <td>“FEAR” est une organisation spécialisée dans l...</td>\n",
       "      <td>[{'id': 0, 'mentions': [{'value': 'crash', 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4909</td>\n",
       "      <td>48 heures après le braquage du \"DC Supermarket...</td>\n",
       "      <td>[{'id': 0, 'mentions': [{'value': 'DC Supermar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2353</td>\n",
       "      <td>À Genève, la boîte de nuit \"Pretty Woman\" a en...</td>\n",
       "      <td>[{'id': 0, 'mentions': [{'value': 'tombé', 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1210</td>\n",
       "      <td>À Caracas, l'an dernier, une mine située à l'o...</td>\n",
       "      <td>[{'id': 0, 'mentions': [{'value': 'effondremen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41948</td>\n",
       "      <td>À Genève, une industrie de fabrication d'arbre...</td>\n",
       "      <td>[{'id': 0, 'mentions': [{'value': 'incendiée',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  \\\n",
       "0   1204  “FEAR” est une organisation spécialisée dans l...   \n",
       "1   4909  48 heures après le braquage du \"DC Supermarket...   \n",
       "2   2353  À Genève, la boîte de nuit \"Pretty Woman\" a en...   \n",
       "3   1210  À Caracas, l'an dernier, une mine située à l'o...   \n",
       "4  41948  À Genève, une industrie de fabrication d'arbre...   \n",
       "\n",
       "                                            entities  \n",
       "0  [{'id': 0, 'mentions': [{'value': 'crash', 'st...  \n",
       "1  [{'id': 0, 'mentions': [{'value': 'DC Supermar...  \n",
       "2  [{'id': 0, 'mentions': [{'value': 'tombé', 'st...  \n",
       "3  [{'id': 0, 'mentions': [{'value': 'effondremen...  \n",
       "4  [{'id': 0, 'mentions': [{'value': 'incendiée',...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('/projects/melodi/mettaleb/Textmine/data/test.csv')\n",
    "#df_train = df_train.set_index(\"id\")\n",
    "df_test.entities = df_test.entities.apply(json.loads)\n",
    "#df_test.relations = df_test.relations.apply(json.loads)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2665c781-a908-4843-8fa2-a0aab08442f3",
   "metadata": {},
   "source": [
    "#### ETAPE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537059a-4eb8-49de-9703-eefd337cd863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dbd6f0-4ddb-4061-95af-f2fa5001543c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae50ceb3-9a43-4cbf-8d4d-b920c6c7bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinaison_train=[]\n",
    "with open(\"/projects/melodi/mettaleb/Textmine/train_entities_pairs.txt\", \"r\") as f1:\n",
    "    for ligne in f1.readlines():\n",
    "        combinaison_train.append(ligne.replace(\"\\n\",\"\"))\n",
    "combinaison_test=[]\n",
    "with open(\"/projects/melodi/mettaleb/Textmine/test_entities_pairs.txt\", \"r\") as f1:\n",
    "    for ligne in f1.readlines():\n",
    "        combinaison_test.append(ligne.replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae40e96d-7b44-47f7-85c9-3be63af93950",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinaison_soumission=[]\n",
    "with open(\"/projects/melodi/mettaleb/Textmine/soumission_entities_pairs.txt\", \"r\") as f1:\n",
    "    for ligne in f1.readlines():\n",
    "        combinaison_soumission.append(ligne.replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e755e68f-e14f-474a-a461-af50fb696c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combinaison_soumission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac7510-f58f-4b2c-869e-eacbd78afb51",
   "metadata": {},
   "source": [
    "#### ETAPE 2 : Préprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "89189b36-1477-46fb-b50d-ac198163b9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Actor,Is_Located_In,Place', 'Event,Is_Located_In,Place', 'Place,Is_Located_In,Place', 'Actor,Is_of_Nationality,Nationality', 'Place,Is_of_Nationality,Nationality', 'Actor,Created,Organization', 'Actor,Has_Control_Over,Actor', 'Actor,Has_Control_Over,Material', 'Actor,Has_Control_Over,Place', 'Actor,Initiated,Event', 'Actor,Is_At_Odds_With,Actor', 'Actor,Is_Cooperating_With,Actor', 'Actor,Is_In_Contact_With,Actor', 'Actor,Is_Part_Of,Organization', 'Event,Deaths_Number,Quantity', 'Event,End_Date,Time', 'Event,Has_Consequence,Event', 'Event,Injured_Number,Quantity', 'Event,Start_Date,Time', 'Event,Started_In,Place', 'Material,Has_Color,Color', 'Material,Has_for_Height,Height', 'Material,Has_for_Length,Length', 'Material,Has_for_Width,Width', 'Material,Has_Quantity,Quantity', 'Material,Is_Registered_As,Material reference', 'Material,Weighs,Weight', 'Organization,Was_Created_In,Time', 'Organization,Was_Dissolved_In,Time', 'Organization,Is_Of_Size,Quantity', 'Organization,Operates_In,Place', 'Person,Died_In,Event', 'Person,Has_Category,Category', 'Person,Has_Family_Relationship,Person', 'Person,Gender_Female,N/A', 'Person,Gender_Male,N/A', 'Person,Is_Born_In,Place', 'Person,Is_Born_On,Time', 'Person,Is_Dead_On,Time', 'Person,Resides_In,Place', 'Place,Is_Located_In,Place', 'Actor,Is_of_Nationality,Nationality', 'Place,Is_of_Nationality,Nationality', 'Place,Has_Latitude,Latitude', 'Place,Has_Longitude,Longitude']\n"
     ]
    }
   ],
   "source": [
    "print(Relations_Definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30226d1-6934-4d78-aedb-bb669908f086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b30f8f69-7a48-4068-8c1f-58e7d05e4991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_entity_pair(entity1, entity2, relations_definitions):\n",
    "    for relation in relations_definitions:\n",
    "        entities = relation.split(',')\n",
    "        if len(entities) == 3 and entities[0].lower() == entity1.lower() and entities[2].lower() == entity2.lower():\n",
    "            #print(relation)\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83571d-fb13-4ac3-8022-a6ba46b8c28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "123d5a5a-4c08-40f7-a716-3486a62de21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinaison_train_F=[]\n",
    "#combinaison_test_F=[]\n",
    "#combinaison_soumission_F=[]\n",
    "X = [\"(Bruno Alves_CIVILIAN,Lisbonne_PLACE)\",\"(Bruno Alves_CIVILIAN,Lisbonne_PLACE)\"]\n",
    "def verif_combinaison(all_com):\n",
    "    final_comb = []\n",
    "    for i in range(len(all_com)):\n",
    "        pairs = all_com[i].split(\";\")\n",
    "        vrai_pairs=[]\n",
    "        set_civilian = set()\n",
    "        for j in range(len(pairs)):\n",
    "            #print(pairs[j],j)\n",
    "            if \"_\" in pairs[j]:\n",
    "                if \"_\" in pairs[j].split(\",\")[0] and \"_\" in pairs[j].split(\",\")[1]:\n",
    "                    e1,type_e1 = pairs[j].split(\",\")[0].replace(\"(\",\"\").split(\"_\",1)\n",
    "                    e2,type_e2 = pairs[j].split(\",\")[1].replace(\")\",\"\").split(\"_\",1)\n",
    "                    #class_e1 = find_type_entity(type_e1).replace('MATERIEL',\"MATERIAL\")\n",
    "                    #class_e2 = find_type_entity(type_e2).replace('MATERIEL',\"MATERIAL\")\n",
    "                    if type_e1 == 'CIVILIAN':\n",
    "                        entity_civil = f\"({e1}_PERSON,{e1}_N/A)\"\n",
    "                        set_civilian.add(entity_civil)\n",
    "                    \n",
    "                    class_e1 = find_type_entity(type_e1)\n",
    "                    class_e2 = find_type_entity(type_e2)\n",
    "                    class_e1.reverse()\n",
    "                    class_e2.reverse()\n",
    "                    #print(class_e1,class_e2)\n",
    "                    trouve = False\n",
    "                    for ent_e1 in class_e1:\n",
    "                        for ent_e2 in class_e2:\n",
    "                            if check_entity_pair(ent_e1, ent_e2, Relations_Definition) and not trouve:\n",
    "                                #print(ent_e1, ent_e2)\n",
    "                                pair_exact = f\"({e1}_{ent_e1},{e2}_{ent_e2})\"\n",
    "                                #vrai_pairs.append(pairs[j])\n",
    "                                vrai_pairs.append(pair_exact)\n",
    "                                trouve =True\n",
    "        set_civilian_l = list(set_civilian)\n",
    "        vrai_pairs.extend(set_civilian_l)\n",
    "        final_pairs = \";\".join(vrai_pairs)\n",
    "        final_comb.append(final_pairs)\n",
    "    return final_comb\n",
    "    return True\n",
    "\n",
    "combinaison_train_F = verif_combinaison(combinaison_train)\n",
    "combinaison_test_F = verif_combinaison(combinaison_test)\n",
    "combinaison_soumission_F = verif_combinaison(combinaison_soumission)\n",
    "#Y = verif_combinaison(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b8ad14a4-20a5-4fe1-bea7-8aca80f03cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(Bruno Alves_Person,Lisbonne_Place);(Bruno Alves_PERSON,Bruno Alves_N/A)',\n",
       " '(Bruno Alves_Person,Lisbonne_Place);(Bruno Alves_PERSON,Bruno Alves_N/A)']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f52eb2-5abb-4b4c-a58d-7ef5932950df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "559b6599-fdeb-4a5b-93e0-30b0ee3e4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinaison_train[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6802cd-e9da-4453-a30e-0d176e4e8e4a",
   "metadata": {},
   "source": [
    "#### ETAPE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4103f3a4-b8e1-4237-ad80-6a006901ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#phrases_train=[]\n",
    "with open (\"/projects/melodi/mettaleb/Textmine/train.sent\", \"r\") as f:\n",
    "    phrases_train=f.readlines()\n",
    "phrases_test=[]\n",
    "with open (\"/projects/melodi/mettaleb/Textmine/test.sent\", \"r\") as f:\n",
    "    phrases_test=f.readlines()\n",
    "phrases_soumission=[]\n",
    "with open (\"/projects/melodi/mettaleb/Textmine/soumission.sent\", \"r\") as f:\n",
    "    phrases_soumission=f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd515148-6414-4a73-ad4c-4470480e7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "tup_train=[]\n",
    "with open (\"/projects/melodi/mettaleb/Textmine/train.tup\", \"r\") as f:\n",
    "    for ligne in f.readlines():\n",
    "        tup_train.append(ligne.strip())\n",
    "tup_test=[]\n",
    "with open (\"/projects/melodi/mettaleb/Textmine/test.tup\", \"r\") as f:\n",
    "    for ligne in f.readlines():\n",
    "        tup_test.append(ligne.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec5208-6394-4903-98d6-7b9352a850ed",
   "metadata": {},
   "source": [
    "### Filtrage 1 : \n",
    " **Vérifie si une paire d'entités est présente dans triplets et retourne le triplet sous la forme \"e1 ; e2 ; relation\" ou \"e1 ; e2 ; PAS_DE_RELATION\" si aucune relation n'est trouvée.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85a85eb0-c111-45e9-8567-d1c605cce503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs(combinaison):\n",
    "    pairs = []\n",
    "    combi = combinaison.strip(\"()\").split(\");(\")\n",
    "    for pair in combi:\n",
    "        e1, e2 = pair.split(\",\")\n",
    "        pairs.append((e1.strip(), e2.strip()))\n",
    "    return pairs\n",
    "\n",
    "def clean_entity(entity):\n",
    "    return entity.split(\"_\")[0]\n",
    "\n",
    "def find_triplet_for_pair(pair, triplets):\n",
    "    e1, e2 = pair\n",
    "    e1_clean, e2_clean = clean_entity(e1), clean_entity(e2)\n",
    "    \n",
    "    triplet_list = triplets.split(\" | \")\n",
    "    \n",
    "    for triplet in triplet_list:\n",
    "        t_e1, t_e2, relation = map(str.strip, triplet.split(\" ; \"))\n",
    "        if clean_entity(t_e1) == e1_clean and clean_entity(t_e2) == e2_clean:\n",
    "            return f\"{e1} ; {e2} ; {relation}\"\n",
    "    \n",
    "    return f\"{e1} ; {e2} ; PAS_DE_RELATION\"\n",
    "\n",
    "def process_combinaison_triplets(combinaison, triplets):\n",
    "    pairs = extract_pairs(combinaison)\n",
    "    results = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        triplet = find_triplet_for_pair(pair, triplets)\n",
    "        results.append(triplet)\n",
    "    \n",
    "    return results\n",
    "\n",
    "result_triplets_train=[]\n",
    "result_triplets_test=[]\n",
    "for i in range(len(combinaison_train_F)):\n",
    "    triplets = process_combinaison_triplets(combinaison_train_F[i], tup_train[i])\n",
    "    result_triplets_train.append(triplets)\n",
    "\n",
    "for i in range(len(combinaison_test_F)):\n",
    "    triplets = process_combinaison_triplets(combinaison_test_F[i], tup_test[i])\n",
    "    result_triplets_test.append(triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a78d567c-1b0a-4294-957e-00d92d4ed7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinaison_test_F[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f912ee97-80b7-4ba5-8066-f1ce859fb3d8",
   "metadata": {},
   "source": [
    "### Filtrage 2 : Attention Scores\n",
    "On calcul le score d'attention de chaque pair d'entitié on garde que les pair qui ont des score >= seuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55ebba37-f5bd-44f0-8074-e40b5e80146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name, output_attentions=True)\n",
    "#'(volés_THEFT,Europe_PLACE);(Bruno Alves_CIVILIAN,Lisbonne_PLACE)'\n",
    "def transform_pair_to_entities(pairs):\n",
    "    list_pair = pairs.split(\";\")\n",
    "    entities=[]\n",
    "    for pair in list_pair:\n",
    "        e1,e2 = pair.replace(\"(\",\"\").replace(\")\",\"\").split(\",\")\n",
    "        e1 = e1.split(\"_\")[0]\n",
    "        e2 = e2.split(\"_\")[0]\n",
    "        entities.append(e1)\n",
    "        entities.append(e2)\n",
    "    return entities\n",
    "def find_entity_indices(paragraph, entities, tokenizer):\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    entity_indices = {}\n",
    "    entities = transform_pair_to_entities(entities)\n",
    "    for entity in entities:\n",
    "        entity_tokens = tokenizer.tokenize(entity)\n",
    "        for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "            if tokens[i:i + len(entity_tokens)] == entity_tokens:\n",
    "                entity_indices[entity] = (i, i + len(entity_tokens) - 1)\n",
    "                break\n",
    "    return entity_indices\n",
    "\n",
    "def get_attention_score(attentions, idx1, idx2):\n",
    "    total_attention = torch.stack(attentions).sum(dim=0)\n",
    "    score = total_attention[:, :, idx1, idx2].mean().item()\n",
    "    #score = total_attention[:,:, idx1, idx2].mean(dim=0).item()\n",
    "    return score\n",
    "\n",
    "\n",
    "entites_SA=[]\n",
    "for paragraph, entities in zip(phrases_test, combinaison_test_F):\n",
    "    inputs = tokenizer(paragraph, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "\n",
    "    entity_indices = find_entity_indices(paragraph, entities, tokenizer)\n",
    "    \n",
    "    ents=[]\n",
    "    list_pairs = entities.split(\";\")\n",
    "    for i, pair in enumerate(list_pairs):\n",
    "        entity1,entity2 = pair.replace(\"(\",\"\").replace(\")\",\"\").split(\",\")\n",
    "        entity1 = entity1.split(\"_\")[0]\n",
    "        entity2 = entity2.split(\"_\")[0]\n",
    "        if entity1 in entity_indices and entity2 in entity_indices:\n",
    "            idx1 = entity_indices[entity1][0]\n",
    "            idx2 = entity_indices[entity2][0]\n",
    "            score = get_attention_score(attentions, idx1, idx2)\n",
    "            ents.append(f\"({entity1} ; {entity2}): {score:.4f}\")\n",
    "        else:\n",
    "            print(f\"Entity '{entity1}' or '{entity2}' not found in paragraph.\")\n",
    "    entites_SA.append(ents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f3ab8cf8-59d5-4db7-8b80-af130fe0155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entites_SA[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b3e430-03af-4f18-83d7-833279aa878c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1718ebee-69ec-4c28-8610-5bcd60e9560c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89764386-c2f4-419a-be99-ba527542f361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508a055-6a17-4885-86eb-13972bd5249f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c960f-65d3-443a-b1f0-33d57ebff612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fdf4ae-b0d1-4bdd-bf46-d3b850c58777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e09f8c2f-cbd6-48a2-bda4-351a28636fa3",
   "metadata": {},
   "source": [
    "**Filtrage : on diminue le nombre de triplet represente la relation \"PAS_DE_RELATION\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "501a7266-2df9-4c78-953f-50f4425af110",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_triplets_train_F = []\n",
    "def filtrage_etiquette(l_triplet):\n",
    "    result_triplets_F =[]\n",
    "    for i in range(len(l_triplet)):\n",
    "        trouve = False\n",
    "        L = []\n",
    "        cpt=0\n",
    "        for j in range(len(l_triplet[i])):\n",
    "            _,_,rel = l_triplet[i][j].split(' ; ')       \n",
    "            if rel == \"PAS_DE_RELATION\" and not trouve:\n",
    "                L.append(l_triplet[i][j])\n",
    "                cpt+=1\n",
    "                if cpt ==20:\n",
    "                    trouve =True\n",
    "            elif rel != \"PAS_DE_RELATION\":\n",
    "                L.append(l_triplet[i][j])\n",
    "        result_triplets_F.append(L)\n",
    "    return result_triplets_F\n",
    "result_triplets_train_F = filtrage_etiquette(result_triplets_train)\n",
    "result_triplets_test_F = filtrage_etiquette(result_triplets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41e0be20-da96-446c-b3f8-99a27a42d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "for l_triplet in result_triplets_train_F:\n",
    "    for triplet in l_triplet:\n",
    "        _,_,rel = triplet.split(\" ; \")\n",
    "        if rel in freq :\n",
    "            freq[rel] += 1\n",
    "        else:\n",
    "            freq[rel] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2335d220-d452-4a1d-b448-d636be6f4c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PAS_DE_RELATION': 11997, 'IS_LOCATED_IN': 5651, 'IS_OF_SIZE': 320, 'HAS_CATEGORY': 745, 'STARTED_IN': 882, 'START_DATE': 612, 'GENDER_FEMALE': 300, 'OPERATES_IN': 1730, 'END_DATE': 162, 'HAS_COLOR': 68, 'HAS_QUANTITY': 128, 'GENDER_MALE': 625, 'IS_PART_OF': 952, 'IS_IN_CONTACT_WITH': 1997, 'CREATED': 90, 'INITIATED': 415, 'HAS_CONTROL_OVER': 2455, 'HAS_FOR_LENGTH': 11, 'HAS_CONSEQUENCE': 405, 'IS_OF_NATIONALITY': 110, 'IS_AT_ODDS_WITH': 1026, 'HAS_FOR_HEIGHT': 9, 'RESIDES_IN': 67, 'IS_BORN_IN': 21, 'IS_BORN_ON': 18, 'IS_COOPERATING_WITH': 197, 'INJURED_NUMBER': 57, 'HAS_FAMILY_RELATIONSHIP': 146, 'IS_DEAD_ON': 53, 'WAS_CREATED_IN': 4, 'IS_REGISTERED_AS': 26, 'DEATHS_NUMBER': 55, 'WAS_DISSOLVED_IN': 8, 'DIED_IN': 20}\n"
     ]
    }
   ],
   "source": [
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a09807ae-9571-46b5-8a07-9de8fd61c365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PAS_DE_RELATION': 11997, 'IS_LOCATED_IN': 5651, 'IS_OF_SIZE': 320, 'HAS_CATEGORY': 745, 'STARTED_IN': 882, 'START_DATE': 612, 'GENDER_FEMALE': 300, 'OPERATES_IN': 1730, 'END_DATE': 162, 'HAS_COLOR': 68, 'HAS_QUANTITY': 128, 'GENDER_MALE': 625, 'IS_PART_OF': 952, 'IS_IN_CONTACT_WITH': 1997, 'CREATED': 90, 'INITIATED': 415, 'HAS_CONTROL_OVER': 2455, 'HAS_FOR_LENGTH': 11, 'HAS_CONSEQUENCE': 405, 'IS_OF_NATIONALITY': 110, 'IS_AT_ODDS_WITH': 1026, 'HAS_FOR_HEIGHT': 9, 'RESIDES_IN': 67, 'IS_BORN_IN': 21, 'IS_BORN_ON': 18, 'IS_COOPERATING_WITH': 197, 'INJURED_NUMBER': 57, 'HAS_FAMILY_RELATIONSHIP': 146, 'IS_DEAD_ON': 53, 'WAS_CREATED_IN': 4, 'IS_REGISTERED_AS': 26, 'DEATHS_NUMBER': 55, 'WAS_DISSOLVED_IN': 8, 'DIED_IN': 20}\n"
     ]
    }
   ],
   "source": [
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5afe45c9-2c21-48e6-bb2d-78c5f3208cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_triplets_train_F[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ff550-a2dc-4793-b6fa-5347a2fc0639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc135ede-438d-4710-b6c3-887c0dfdb960",
   "metadata": {},
   "source": [
    "**Résultat to files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50ac3aa7-829c-406c-9995-95c7d5ca66d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Actor,Is_Located_In,Place', 'Event,Is_Located_In,Place', 'Place,Is_Located_In,Place', 'Actor,Is_of_Nationality,Nationality', 'Place,Is_of_Nationality,Nationality', 'Actor,Created,Organization', 'Actor,Has_Control_Over,Actor', 'Actor,Has_Control_Over,Material', 'Actor,Has_Control_Over,Place', 'Actor,Initiated,Event', 'Actor,Is_At_Odds_With,Actor', 'Actor,Is_Cooperating_With,Actor', 'Actor,Is_In_Contact_With,Actor', 'Actor,Is_Part_Of,Organization', 'Event,Death_Number,Quantity', 'Event,End_Date,Time', 'Event,Has_Consequence,Event', 'Event,Injured_Number,Quantity', 'Event,Start_Date,Time', 'Event,Started_In,Place', 'Material,Has_Color,Color', 'Material,Has_for_Height,Height', 'Material,Has_for_Length,Length', 'Material,Has_for_Width,Width', 'Material,Has_Quantity,Quantity', 'Material,Is_Registered_As,Material reference', 'Material,Weighs,Weight', 'Organization,Created_In,Time', 'Organization,Dissolved_In,Time', 'Organization,Is_Of_Size,Quantity', 'Organization,Operates_In,Place', 'Person,Died_In,Event', 'Person,Has_Category,Category', 'Person,Has_Family_Relationship,Person', 'Person,Has_First_Name,First_Name', 'Person,Has_Gender_Female,N/A', 'Person,Has_Gender_Male,N/A', 'Person,Has_Last_Name,Last_Name', 'Person,Is_Born_In,Place', 'Person,Is_Born_On,Time', 'Person,Is_Dead_On,Time', 'Person,Resides_In,Place', 'Place,Is_Located_In,Place', 'Actor,Is_of_Nationality,Nationality', 'Place,Is_of_Nationality,Nationality', 'Place,Has_Latitude,Latitude', 'Place,Has_Longitude,Longitude']\n"
     ]
    }
   ],
   "source": [
    "print(Relations_Definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a68106d-c660-4131-8fd9-7057dc15bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chercher_tous_relations_possibles(e1,e2,relations_definitions):\n",
    "    relations_possibles=\"\"\n",
    "    class_e1 = e1.split(\"_\",1)[1]\n",
    "    class_e2 = e2.split(\"_\",1)[1]\n",
    "    #type_e1 = find_type_entity(class_e1).replace('MATERIEL',\"MATERIAL\")\n",
    "    #type_e2 = find_type_entity(class_e2).replace('MATERIEL',\"MATERIAL\")\n",
    "    type_e1 = class_e1\n",
    "    type_e2 = class_e2\n",
    "    #print(type_e1,type_e2)\n",
    "    for relation in relations_definitions:\n",
    "        entities = relation.split(',')\n",
    "        if len(entities) == 3 and entities[0].lower() == type_e1.lower() and entities[2].lower() == type_e2.lower():\n",
    "            relations_possibles += f\"{entities[1]};\"\n",
    "    relations_possibles += \"PAS_DE_RELATION\\n\"\n",
    "    return relations_possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9787274d-a336-4eb5-ad4f-76dbf5fe94bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor MATERIAL\n"
     ]
    }
   ],
   "source": [
    "x = 'gouvernements_NON_MILITARY_GOVERNMENT_ORGANISATION ; pièces détachées_MATERIEL ; PAS_DE_RELATION'\n",
    "e1,e2,rel = x.split(\" ; \")\n",
    "rel_possible = chercher_tous_relations_possibles(e1,e2,Relations_Definition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6dcff495-f803-41ee-908b-0cfd612009b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Has_Control_Over;PAS_DE_RELATION\\n'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "739c8ca1-71ba-4788-ae1e-e269122fc98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gouvernements_NON_MILITARY_GOVERNMENT_ORGANISATION ; pièces détachées_MATERIEL ; PAS_DE_RELATION'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_triplets_test[0][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0bf6a788-7154-4f7e-a11f-d3da7510c251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ma passion_NON_GOVERNMENTAL_ORGANISATION ; accident de circulation_ACCIDENT ; PAS_DE_RELATION',\n",
       " 'Anam Destresse_CIVILIAN ; Italie_PLACE ; IS_LOCATED_IN']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_triplets_train[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31443a49-1b0f-4cee-af9f-b710c4d353ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "af6e0c3a-4d9e-4149-9e5e-59ef223c6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/projects/melodi/mettaleb/Textmine/train_strategie3.sent\", \"w\") as fich1, open(\"/projects/melodi/mettaleb/Textmine/train_strategie3.tup\", \"w\") as fich2, open(\"/projects/melodi/mettaleb/Textmine/relation_possibles_strategie3_train.sent\", \"w\") as fich3:\n",
    "    for i in range(len(result_triplets_train_F)):\n",
    "        for j in range(len(result_triplets_train_F[i])):\n",
    "            e1,e2,rel = result_triplets_train_F[i][j].split(\" ; \")\n",
    "            relations_possibles = chercher_tous_relations_possibles(e1,e2,Relations_Definition)\n",
    "            e1 = e1.split(\"_\")[0]\n",
    "            e2 = e2.split(\"_\")[0]\n",
    "            triplet = f\"{e1} ; {e2} ; {rel}\\n\"\n",
    "            fich2.write(triplet)\n",
    "            fich1.write(f\"{phrases_train[i]}\")\n",
    "            fich3.write(relations_possibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "205d0f3c-1564-470d-80e7-f888e96f111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=[]\n",
    "tupp=[]\n",
    "with open(\"/projects/melodi/mettaleb/Textmine/test_strategie3.sent\", \"w\") as fich1, open(\"/projects/melodi/mettaleb/Textmine/test_strategie3.tup\", \"w\") as fich2 , open(\"/projects/melodi/mettaleb/Textmine/relation_possibles_strategie3_test.sent\", \"w\") as fich3:\n",
    "    for i in range(len(result_triplets_test_F)):\n",
    "        for j in range(len(result_triplets_test_F[i])):\n",
    "            e1,e2,rel = result_triplets_test_F[i][j].split(\" ; \")\n",
    "            relations_possibles = chercher_tous_relations_possibles(e1,e2,Relations_Definition)\n",
    "            e1 = e1.split(\"_\")[0]\n",
    "            e2 = e2.split(\"_\")[0]\n",
    "            triplet = f\"{e1} ; {e2} ; {rel}\\n\"\n",
    "            fich2.write(triplet)\n",
    "            fich1.write(f\"{phrases_test[i]}\")\n",
    "            fich3.write(relations_possibles)\n",
    "            tupp.append(triplet)\n",
    "            sent.append(phrases_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0a7e039f-121a-4924-b6c4-78f2d4811cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10299, 10299)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent), len(tupp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe5eb7-a554-49f2-a7a6-d47ac8bad059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b9622f5b-4c86-40b3-a264-6fca9cf11f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df_test['id'].to_list()\n",
    "with open(\"/projects/melodi/mettaleb/Textmine/soumission_strategie3.sent\", \"w\") as fich1, open(\"/projects/melodi/mettaleb/Textmine/soumission_pairs.sent\", \"w\") as fich2, open(\"/projects/melodi/mettaleb/Textmine/relation_possibles_strategie3_soumission.sent\", \"w\") as fich3:\n",
    "    for i in range(len(combinaison_soumission_F)):\n",
    "        result_pair_test = combinaison_soumission_F[i].split(\";\")\n",
    "        for j in range(len(result_pair_test)):\n",
    "            e1,e2 = result_pair_test[j].split(\",\")\n",
    "            e1 = e1.replace(\"(\",\"\")\n",
    "            e2 = e2.replace(\")\",\"\")\n",
    "            relations_possibles = chercher_tous_relations_possibles(e1,e2,Relations_Definition)\n",
    "            e1 = e1.split(\"_\")[0]\n",
    "            e2 = e2.split(\"_\")[0]\n",
    "            pair = f\"{ids[i]} ; {e1} ; {e2}\\n\"\n",
    "            fich2.write(pair)\n",
    "            fich1.write(f\"{phrases_soumission[i]}\")\n",
    "            fich3.write(relations_possibles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe0146-c33c-4d7b-80c0-fa1d36864afe",
   "metadata": {},
   "source": [
    "# Pour Strategie 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "704896d3-2d88-491b-8793-729528597bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>relations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181</td>\n",
       "      <td>Anam Destresse, président de l'ONG \"Ma passion...</td>\n",
       "      <td>[{'id': 0, 'mentions': [{'value': 'accident', ...</td>\n",
       "      <td>[[0, STARTED_IN, 9], [7, IS_LOCATED_IN, 9], [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31669</td>\n",
       "      <td>À Paris, le 8 avril 2022, l'usine de déodorant...</td>\n",
       "      <td>[{'id': 0, 'mentions': [{'value': 'explosé', '...</td>\n",
       "      <td>[[9, IS_LOCATED_IN, 8], [11, OPERATES_IN, 8], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51470</td>\n",
       "      <td>En Espagne, dans une région agricole, une cont...</td>\n",
       "      <td>[{'id': 0, 'mentions': [{'value': 'contaminati...</td>\n",
       "      <td>[[7, IS_PART_OF, 8], [9, OPERATES_IN, 1], [0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51332</td>\n",
       "      <td>Un important incendie a fait des ravages dans ...</td>\n",
       "      <td>[{'id': 0, 'mentions': [{'value': 'incendie', ...</td>\n",
       "      <td>[[12, IS_IN_CONTACT_WITH, 5], [0, IS_LOCATED_I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1131</td>\n",
       "      <td>« Je coule » : onze heures après avoir envoyé ...</td>\n",
       "      <td>[{'id': 0, 'mentions': [{'value': 'renversé', ...</td>\n",
       "      <td>[[9, IS_LOCATED_IN, 2], [0, START_DATE, 17], [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  \\\n",
       "0    181  Anam Destresse, président de l'ONG \"Ma passion...   \n",
       "1  31669  À Paris, le 8 avril 2022, l'usine de déodorant...   \n",
       "2  51470  En Espagne, dans une région agricole, une cont...   \n",
       "3  51332  Un important incendie a fait des ravages dans ...   \n",
       "4   1131  « Je coule » : onze heures après avoir envoyé ...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  [{'id': 0, 'mentions': [{'value': 'accident', ...   \n",
       "1  [{'id': 0, 'mentions': [{'value': 'explosé', '...   \n",
       "2  [{'id': 0, 'mentions': [{'value': 'contaminati...   \n",
       "3  [{'id': 0, 'mentions': [{'value': 'incendie', ...   \n",
       "4  [{'id': 0, 'mentions': [{'value': 'renversé', ...   \n",
       "\n",
       "                                           relations  \n",
       "0  [[0, STARTED_IN, 9], [7, IS_LOCATED_IN, 9], [5...  \n",
       "1  [[9, IS_LOCATED_IN, 8], [11, OPERATES_IN, 8], ...  \n",
       "2  [[7, IS_PART_OF, 8], [9, OPERATES_IN, 1], [0, ...  \n",
       "3  [[12, IS_IN_CONTACT_WITH, 5], [0, IS_LOCATED_I...  \n",
       "4  [[9, IS_LOCATED_IN, 2], [0, START_DATE, 17], [...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('/projects/melodi/mettaleb/Textmine//data/train.csv')\n",
    "#df_train = df_train.set_index(\"id\")\n",
    "df_train.entities = df_train.entities.apply(json.loads)\n",
    "df_train.relations = df_train.relations.apply(json.loads)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4728a568-be4f-4af5-9752-4a94edf75b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train =  df_train['id'].to_list()[:600]\n",
    "ids_test =  df_train['id'].to_list()[600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa5dea65-efc3-4af4-80a7-f3b7b3984303",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/projects/melodi/mettaleb/Textmine/strategie5/train_strategie3.sent\", \"w\") as fich1, open(\"/projects/melodi/mettaleb/Textmine/strategie5/train_strategie3.tup\", \"w\") as fich2, open(\"/projects/melodi/mettaleb/Textmine/strategie5/relation_possibles_strategie3_train.sent\", \"w\") as fich3:\n",
    "    for i in range(len(result_triplets_train_F)):\n",
    "        for j in range(len(result_triplets_train_F[i])):\n",
    "            e1,e2,rel = result_triplets_train_F[i][j].split(\" ; \")\n",
    "            relations_possibles = chercher_tous_relations_possibles(e1,e2,Relations_Definition)\n",
    "            e1 = e1.split(\"_\")[0]\n",
    "            e2 = e2.split(\"_\")[0]\n",
    "            triplet = f\"{ids_train[i]} ; {e1} ; {e2} ; {rel}\\n\"\n",
    "            fich2.write(triplet)\n",
    "            fich1.write(f\"{phrases_train[i]}\")\n",
    "            fich3.write(relations_possibles)\n",
    "with open(\"/projects/melodi/mettaleb/Textmine/strategie5/test_strategie3.sent\", \"w\") as fich1, open(\"/projects/melodi/mettaleb/Textmine/strategie5/test_strategie3.tup\", \"w\") as fich2 , open(\"/projects/melodi/mettaleb/Textmine/strategie5/relation_possibles_strategie3_test.sent\", \"w\") as fich3:\n",
    "    for i in range(len(result_triplets_test_F)):\n",
    "        for j in range(len(result_triplets_test_F[i])):\n",
    "            e1,e2,rel = result_triplets_test_F[i][j].split(\" ; \")\n",
    "            relations_possibles = chercher_tous_relations_possibles(e1,e2,Relations_Definition)\n",
    "            e1 = e1.split(\"_\")[0]\n",
    "            e2 = e2.split(\"_\")[0]\n",
    "            triplet = f\"{ids_test[i]} ; {e1} ; {e2} ; {rel}\\n\"\n",
    "            fich2.write(triplet)\n",
    "            fich1.write(f\"{phrases_test[i]}\")\n",
    "            fich3.write(relations_possibles)\n",
    "\n",
    "ids = df_test['id'].to_list()\n",
    "with open(\"/projects/melodi/mettaleb/Textmine/strategie5/soumission_strategie3.sent\", \"w\") as fich1, open(\"/projects/melodi/mettaleb/Textmine/strategie5/soumission_pairs.sent\", \"w\") as fich2, open(\"/projects/melodi/mettaleb/Textmine/strategie5/relation_possibles_strategie3_soumission.sent\", \"w\") as fich3:\n",
    "    for i in range(len(combinaison_soumission_F)):\n",
    "        result_pair_test = combinaison_soumission_F[i].split(\";\")\n",
    "        for j in range(len(result_pair_test)):\n",
    "            e1,e2 = result_pair_test[j].split(\",\")\n",
    "            e1 = e1.replace(\"(\",\"\")\n",
    "            e2 = e2.replace(\")\",\"\")\n",
    "            relations_possibles = chercher_tous_relations_possibles(e1,e2,Relations_Definition)\n",
    "            e1 = e1.split(\"_\")[0]\n",
    "            e2 = e2.split(\"_\")[0]\n",
    "            pair = f\"{ids[i]} ; {e1} ; {e2}\\n\"\n",
    "            fich2.write(pair)\n",
    "            fich1.write(f\"{phrases_soumission[i]}\")\n",
    "            fich3.write(relations_possibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35256c-5248-45fe-9665-3a210d97df2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66c4de-4e0c-46a9-9687-9e8985049062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf8b051-4572-4224-950c-04c0aa9b752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "          A_COMMENCÉ_EN       0.16      0.32      0.21        19\n",
    "               A_COULEUR       0.75      1.00      0.86         3\n",
    "      A_DES_CONSÉQUENCES       0.63      0.73      0.68        30\n",
    "         A_UNE_CATEGORIE       0.75      0.67      0.71        18\n",
    "   A_UNE_CE_CONTROLE_SUR       0.67      0.36      0.47       119\n",
    "          A_UNE_QUANTITÉ       0.57      0.57      0.57         7\n",
    "A_UNE_RELATION_FAMILIALE       0.00      0.00      0.00         2\n",
    "            COOPÈRE_AVEC       0.00      0.00      0.00         2\n",
    "                    CRÉÉ       0.00      0.00      0.00         1\n",
    "           DATE_DE_DÉBUT       0.55      0.84      0.67        19\n",
    "             DATE_DE_FIN       0.00      0.00      0.00         7\n",
    "      EST_DE_NATIONALITÉ       0.40      0.50      0.44         4\n",
    "           EST_DE_TAILLE       0.91      0.77      0.83        13\n",
    "    EST_ENREGISTRÉ_COMME       1.00      0.50      0.67         2\n",
    "     EST_EN_CONFLIT_AVEC       0.23      0.58      0.33        12\n",
    "     EST_EN_CONTACT_AVEC       0.32      0.47      0.38        43\n",
    "         EST_LOCALISÉ_EN       0.64      0.77      0.70       191\n",
    "             EST_MORT_LE       0.00      0.00      0.00         1\n",
    "                EST_NÉ_A       0.00      0.00      0.00         3\n",
    "          FAIT_PARTIE_DE       0.50      0.45      0.48        22\n",
    "           GENRE_FEMININ       0.80      0.89      0.84         9\n",
    "          GENRE_MASCULIN       0.77      0.91      0.83        11\n",
    "                  INITIÉ       0.00      0.00      0.00         2\n",
    "                 MORT_EN       0.00      0.00      0.00         2\n",
    "       NOMBRE_DE_BLESSÉS       0.33      1.00      0.50         2\n",
    "         NOMBRE_DE_MORTS       1.00      0.33      0.50         3\n",
    "                OPÈRE_EN       0.78      0.74      0.76        58\n",
    "         PAS_DE_RELATION       0.66      0.60      0.63       394\n",
    "             RÉSIDE_DANS       0.00      0.00      0.00         1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4800744-7560-4a97-9868-efb187317c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aa5f835-11d7-4946-a027-45a0f13965ae",
   "metadata": {},
   "source": [
    "**Préparer un prompt pour chaque triplet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1040042f-584a-49e3-8e8a-ba13d712a343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IS_LOCATED_IN': 1887, 'PAS_DE_RELATION': 4000, 'HAS_CONSEQUENCE': 346, 'STARTED_IN': 230, 'HAS_CONTROL_OVER': 1129, 'HAS_QUANTITY': 63, 'IS_OF_NATIONALITY': 63, 'OPERATES_IN': 587, 'START_DATE': 252, 'HAS_CATEGORY': 238, 'GENDER_MALE': 170, 'IS_OF_SIZE': 115, 'IS_AT_ODDS_WITH': 134, 'IS_IN_CONTACT_WITH': 511, 'IS_PART_OF': 208, 'DEATHS_NUMBER': 18, 'GENDER_FEMALE': 97, 'END_DATE': 74, 'HAS_COLOR': 23, 'HAS_FAMILY_RELATIONSHIP': 33, 'INITIATED': 18, 'IS_COOPERATING_WITH': 22, 'INJURED_NUMBER': 12, 'IS_BORN_IN': 11, 'RESIDES_IN': 4, 'IS_DEAD_ON': 13, 'IS_REGISTERED_AS': 8, 'DIED_IN': 19, 'CREATED': 3, 'HAS_FOR_HEIGHT': 3, 'IS_BORN_ON': 2, 'WAS_DISSOLVED_IN': 3, 'WAS_CREATED_IN': 1, 'HAS_FOR_LENGTH': 2}\n"
     ]
    }
   ],
   "source": [
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "409f5240-702b-4ec5-9b7f-37066c77418e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "82df2df2-8ecd-4f49-8315-7fe4e3d5f19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "42e50717-d8af-4c16-9dfb-60e8424da01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6843d1fd-5266-412a-b0c8-7823e1d89397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9007"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LL = 0\n",
    "for k in freq_reference_train:\n",
    "    LL += freq_reference_train[k]\n",
    "LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c843b3e1-5f7a-435d-9948-1cce74310344",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "42988d66-9f44-4c51-bbaa-9bc1ca983c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_reference_train = {'PAS_DE_RELATION': 5000, 'IS_LOCATED_IN': 2000, 'IS_OF_SIZE': 400, 'HAS_CATEGORY': 500, 'STARTED_IN': 400, 'START_DATE': 250, 'GENDER_FEMALE': 300, 'OPERATES_IN': 700, 'END_DATE': 100, 'HAS_COLOR': 68, 'HAS_QUANTITY': 100, 'GENDER_MALE': 300, 'IS_PART_OF': 700, 'IS_IN_CONTACT_WITH': 1000, 'CREATED': 90, 'INITIATED': 400, 'HAS_CONTROL_OVER': 1000, 'HAS_FOR_LENGTH': 11, 'HAS_CONSEQUENCE': 300, 'IS_OF_NATIONALITY': 100, 'IS_AT_ODDS_WITH': 400, 'HAS_FOR_HEIGHT': 9, 'RESIDES_IN': 67, 'IS_BORN_IN': 21, 'IS_BORN_ON': 18, 'IS_COOPERATING_WITH': 450, 'INJURED_NUMBER': 57, 'HAS_FAMILY_RELATIONSHIP': 200, 'IS_DEAD_ON': 53, 'WAS_CREATED_IN': 100,  'IS_REGISTERED_AS': 26, 'DEATHS_NUMBER': 55, 'WAS_DISSOLVED_IN': 50 , 'DIED_IN': 20}\n",
    "freq_reference_test  = {'PAS_DE_RELATION': 500, 'IS_LOCATED_IN': 300, 'IS_OF_SIZE': 100, 'HAS_CATEGORY': 100, 'STARTED_IN': 100, 'START_DATE': 100, 'GENDER_FEMALE': 100, 'OPERATES_IN': 100, 'END_DATE': 100, 'HAS_COLOR': 68, 'HAS_QUANTITY': 100, 'GENDER_MALE': 100, 'IS_PART_OF': 100, 'IS_IN_CONTACT_WITH': 100, 'CREATED': 90, 'INITIATED': 100, 'HAS_CONTROL_OVER': 100, 'HAS_FOR_LENGTH': 11, 'HAS_CONSEQUENCE': 100, 'IS_OF_NATIONALITY': 100, 'IS_AT_ODDS_WITH': 100, 'HAS_FOR_HEIGHT': 9, 'RESIDES_IN': 67, 'IS_BORN_IN': 21, 'IS_BORN_ON': 18, 'IS_COOPERATING_WITH': 100, 'INJURED_NUMBER': 57, 'HAS_FAMILY_RELATIONSHIP': 100, 'IS_DEAD_ON': 53, 'WAS_CREATED_IN': 50, 'IS_REGISTERED_AS': 26, 'DEATHS_NUMBER': 55, 'WAS_DISSOLVED_IN': 50 , 'DIED_IN': 20}\n",
    "freq_train_initial = {k:0 for k in freq_reference_train}\n",
    "freq_test_initial = {k:0 for k in freq_reference_test}\n",
    "\n",
    "def choisi_instance(liste_tuple, freq_train_initial, freq_reference_train):\n",
    "    final_indices = []\n",
    "    indices = random.sample(range(len(liste_tuple)),len(liste_tuple))\n",
    "    for indice in indices:\n",
    "        tuples_fr =[]\n",
    "        for tupl in liste_tuple[indice]:\n",
    "            e1, e2, rel = tupl.split(\" ; \")\n",
    "            rel = rel.strip()\n",
    "            if freq_train_initial[rel] <= freq_reference_train[rel]:\n",
    "                freq_train_initial[rel]+=1\n",
    "                final_indices.append(indice)\n",
    "    return final_indices\n",
    "\n",
    "with open('/projects/melodi/mettaleb/Textmine/train_strategie3.tup') as f:\n",
    "    tuples_list_train = [s.split(' | ') for s in f.readlines()]\n",
    "with open('/projects/melodi/mettaleb/Textmine/test_strategie3.tup') as f:\n",
    "    tuples_list_test = [s.split(' | ') for s in f.readlines()]\n",
    "final_indices_train = choisi_instance(tuples_list_train, freq_train_initial, freq_reference_train)\n",
    "final_indices_test = choisi_instance(tuples_list_test, freq_test_initial, freq_reference_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd47c916-d7c6-4c97-8156-cafe7e5656e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31354"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(final_indices_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b08919ec-742a-427b-8d0f-94fef8036281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83595319-1dc5-4fb9-8a3f-7083a19451c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2405"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "857b6df5-66db-4ea1-b8fe-5db954e6864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/projects/melodi/mettaleb/Textmine/train_strategie3.tup') as f :\n",
    "    relationst = f.readlines()\n",
    "    relationst = [x.strip().split(\" ; \")[2] for x in relationst]\n",
    "    relationst = list(set(relationst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26623463-cf85-4b0c-b84b-f722cdfb1e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relationst = [x.split(\" ; \")[2].strip() for x in relationst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2d824-8c4b-497b-bd23-89f6443d2a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationst = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62760125-08f4-4337-9520-62329770241f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relat = list(set(relat))\n",
    "len(relat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0674da16-bca0-4005-8fa0-34350fb3c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAS_LONGITUDE\n",
      "HAS_LATITUDE\n",
      "WEIGHS\n",
      "HAS_FOR_WIDTH\n"
     ]
    }
   ],
   "source": [
    "for r in relat : \n",
    "    if r not in relationst:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "688e4b57-651b-439b-b49e-f79a473d674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationst = list(set(relationst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "551549ae-e428-4698-ab41-354c0874d1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relationst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2e6dedb-6e8d-43a4-8395-329bfbc0ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHRASE  = \"Lisbonne ; Europe ; IS_LOCATED_IN | vendus ; quartiers ; STARTED_IN | crise financière ; Europe ; IS_LOCATED_IN | médicaments ; certains ; HAS_QUANTITY | crise financière ; vendus ; HAS_CONSEQUENCE | gouvernement ; quartiers ; OPERATES_IN | gouvernement ; portugais ; IS_OF_NATIONALITY | crise économique ; Europe ; IS_LOCATED_IN | vendus ; Lisbonne ; IS_LOCATED_IN | volés ; Europe ; IS_LOCATED_IN | volés ; vendus ; HAS_CONSEQUENCE | Bruno Alves ; Europe ; IS_LOCATED_IN | augmenté ; vendus ; HAS_CONSEQUENCE | volés ; quartiers ; STARTED_IN | crise financière ; Lisbonne ; IS_LOCATED_IN | crise financière ; volés ; HAS_CONSEQUENCE | Bruno Alves ; Bruno Alves ; GENDER_MALE | augmenté ; Lisbonne ; IS_LOCATED_IN | vendus ; quartiers ; IS_LOCATED_IN | volés ; Lisbonne ; IS_LOCATED_IN | crise financière ; Europe ; STARTED_IN | crise financière ; plusieurs mois ; START_DATE | augmenté ; volés ; HAS_CONSEQUENCE | Bruno Alves ; sociologue ; HAS_CATEGORY | crise financière ; quartiers ; IS_LOCATED_IN | crise financière ; crise économique ; HAS_CONSEQUENCE | quartiers ; Lisbonne ; IS_LOCATED_IN | vendus ; Europe ; IS_LOCATED_IN | gouvernement ; Lisbonne ; OPERATES_IN | augmenté ; quartiers ; IS_LOCATED_IN | gouvernements ; Europe ; OPERATES_IN | Bruno Alves ; revue ; HAS_CONTROL_OVER | volés ; quartiers ; IS_LOCATED_IN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "356983fa-f399-4ef5-981a-bbeb1da0a8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(PHRASE.split(\" | \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8b1757c9-6843-4702-887a-1d1872488b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('mydata/relations.txt') as f:\n",
    "#relations = [r.strip() for r in set(df_train['relation'].to_list())]\n",
    "\n",
    "    \n",
    "def get_instruction(sent, tuples, rel_possibles, with_orig=True, with_cls=False):\n",
    "    \n",
    "    instructions, inputs, outputs = [], [], []\n",
    "    if with_orig:\n",
    "        instructions.append(f\"Given a paragraph that describes the relationship between two words/phrases as options, extract the word/phrase pair and the corresponding lexical relationship between them from the input text. The output format should be \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Options: {', '.join(relations)}. \")\n",
    "        instructions.append(f\"Given the input paragraph, please extract the subject and object containing a certain relation in the sentence according to the following relation types, in the format of \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Relations include: {'; '.join(relations)}.\")\n",
    "        instructions.append(f\"Provide a paragraph containing relationships between entities. Extract and identify the specific relations between entity pairs mentioned in the paragraph. Output the results in the format \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Relations should be determined based on the context provided in the paragraph. Relations include: {'; '.join(relations)}.\")\n",
    "\n",
    "        inputs.extend([sent] * 3)\n",
    "        outputs.extend([\"; \".join([f\"{tup[-1]}: {tup[0]}, {tup[1]}\" for tup in tuples])] * 3)\n",
    "    \n",
    "    if with_cls:\n",
    "        for tup in tuples:\n",
    "            #instructions.append(f\"Utilize the input text as a context reference, choose the right relationship between {tup[0]} and {tup[1]} from the options. Options: {', '.join(relations)}.\")\n",
    "            instructions.append(f\"\"\"Quelle est la relation entre «{tup[0]}» et «{tup[1]}» dans le contexte du texte d'entrée. Choisissez une réponse parmi : [ {rel_possibles} ].\"\"\")\n",
    "            inputs.extend([sent] * 1)\n",
    "            outputs.extend([tup[-1]] * 1)\n",
    "    \n",
    "    return instructions, inputs, outputs\n",
    "\n",
    "\n",
    "def get_finred_dataset(sent_file, tup_file, rel_file, with_orig, with_cls):\n",
    "\n",
    "    instructions, inputs, outputs = [], [], []\n",
    "\n",
    "    with open(sent_file) as f:\n",
    "        sentences = [s.strip() for s in f.readlines()]\n",
    "        #indices = random.sample(range(len(sentences)), 5000)\n",
    "    with open(tup_file) as f:\n",
    "        tuples_list = [s.split(' | ') for s in f.readlines()]\n",
    "    with open(rel_file) as f:\n",
    "        relations = [s.strip() for s in f.readlines()]\n",
    "    # Traduit les relations en francais \n",
    "    \n",
    "    ###############################################################\n",
    "    tuples_list_fr = []\n",
    "    for tuples in tuples_list:\n",
    "        tuples_fr =[]\n",
    "        for tupl in tuples:\n",
    "            e1, e2, rel = tupl.split(\" ; \")\n",
    "            rel = rel.strip().upper()\n",
    "            rel_fr = relations_trad[rel]\n",
    "            new_tuple = f\"{e1} ; {e2} ; {rel_fr}\"\n",
    "            tuples_fr.append(new_tuple)\n",
    "        tuples_list_fr.append(tuples_fr)\n",
    "    tuples_list = tuples_list_fr\n",
    "    ####################\n",
    "    relations_fr = []\n",
    "    for rel in relations :\n",
    "        lis_rel = rel.split(\";\")\n",
    "        list_rel_fr =[]\n",
    "        for i in range(len(lis_rel)):\n",
    "            rel = lis_rel[i].strip().upper()\n",
    "            rel_fr = relations_trad[rel]\n",
    "            list_rel_fr.append(rel_fr)\n",
    "        rel_poss_fr = \" ; \".join(list_rel_fr)\n",
    "        relations_fr.append(rel_poss_fr)\n",
    "    relations = relations_fr\n",
    "    ###############################################################\n",
    "    \n",
    "    indices = final_indices_test\n",
    "    sentences = [sentences[i] for i in indices]\n",
    "    tuples_list = [tuples_list[i] for i in indices]\n",
    "    relations = [relations[i] for i in indices]\n",
    "    for sent, tuples, rel_possibles in zip(sentences, tuples_list, relations):\n",
    "        tuples = [[e.strip() for e in tup.split(' ; ')] for tup in tuples]\n",
    "        rel_possibles = \" ; \".join(list(set(rel_possibles.split(\" ; \"))))\n",
    "        ins, i, o = get_instruction(sent, tuples, rel_possibles, with_orig, with_cls)\n",
    "        \n",
    "        instructions.extend(ins)\n",
    "        inputs.extend(i)\n",
    "        outputs.extend(o)\n",
    "    print(\"longuer = \",len(outputs))\n",
    "    print(\"exemple : \",outputs[:10])\n",
    "    print(\"exemple Instruciton: \",instructions[10])    \n",
    "    return Dataset.from_dict({\n",
    "        'input': inputs,\n",
    "        'output': outputs,\n",
    "        'instruction': instructions\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c73f74f-10f6-4c06-9f9e-c21f616e2e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29987dc5-bf8e-4730-b810-2dc9efb53b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8a10b618-f8d5-478b-b591-b339656fec5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longuer =  14736\n",
      "exemple :  ['PAS_DE_RELATION', 'PAS_DE_RELATION', 'PAS_DE_RELATION', 'PAS_DE_RELATION', 'PAS_DE_RELATION', 'COOPÈRE_AVEC', 'EST_LOCALISÉ_EN', 'OPÈRE_EN', 'DATE_DE_DÉBUT', 'OPÈRE_EN']\n",
      "exemple Instruciton:  Quelle est la relation entre «responsable» et «représentants» dans le contexte du texte d'entrée. Choisissez une réponse parmi : [ PAS_DE_RELATION ; A_UNE_CE_CONTROLE_SUR ; EST_EN_CONFLIT_AVEC ; EST_EN_CONTACT_AVEC ; COOPÈRE_AVEC ].\n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_finred_dataset('/projects/melodi/mettaleb/Textmine/train_strategie3.sent', '/projects/melodi/mettaleb/Textmine/train_strategie3.tup', '/projects/melodi/mettaleb/Textmine/relation_possibles_strategie3_train.sent', with_orig=False, with_cls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "408eff43-256c-4820-ad99-126dd618e137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longuer =  2405\n",
      "exemple :  ['GENRE_MASCULIN', 'GENRE_MASCULIN', 'EST_DE_TAILLE', 'PAS_DE_RELATION', 'EST_LOCALISÉ_EN', 'A_UNE_CE_CONTROLE_SUR', 'PAS_DE_RELATION', 'A_DES_CONSÉQUENCES', 'EST_LOCALISÉ_EN', 'A_UNE_CATEGORIE']\n",
      "exemple Instruciton:  Quelle est la relation entre «Pétrole pour tous» et «locaux» dans le contexte du texte d'entrée. Choisissez une réponse parmi : [ EST_LOCALISÉ_EN ; A_UNE_CE_CONTROLE_SUR ; PAS_DE_RELATION ].\n"
     ]
    }
   ],
   "source": [
    "test_dataset = get_finred_dataset('/projects/melodi/mettaleb/Textmine/test_strategie3.sent', '/projects/melodi/mettaleb/Textmine/test_strategie3.tup','/projects/melodi/mettaleb/Textmine/relation_possibles_strategie3_test.sent', with_orig=False, with_cls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "80542370-7d87-45e3-9ff6-cee95ddbb56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 14736/14736 [00:00<00:00, 112237.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2405/2405 [00:00<00:00, 79298.32 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 14736\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 2405\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "finred_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "finred_dataset.save_to_disk('fingpt-finred')\n",
    "finred_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941135f6-6a48-41ad-8fc8-2181fc68c6ad",
   "metadata": {},
   "source": [
    "#### Soumission Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c62971c7-1c45-4333-b702-1f0dacc74468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longuer =  73693\n",
      "exemple :  ['PAS_DE_RELATION', 'PAS_DE_RELATION', 'PAS_DE_RELATION', 'PAS_DE_RELATION', 'PAS_DE_RELATION', 'PAS_DE_RELATION', 'PAS_DE_RELATION', 'PAS_DE_RELATION', 'PAS_DE_RELATION', 'PAS_DE_RELATION']\n",
      "exemple Instruciton:  Quelle est la relation entre «FEAR» et «avion commercial» dans le contexte du texte d'entrée. Choisissez une réponse parmi : [ PAS_DE_RELATION ; A_UNE_CE_CONTROLE_SUR ].\n"
     ]
    }
   ],
   "source": [
    "test_dataset = get_finred_dataset('/projects/melodi/mettaleb/Textmine/soumission_strategie3.sent', '/projects/melodi/mettaleb/Textmine/soumission_pairs_fake.tup','/projects/melodi/mettaleb/Textmine/relation_possibles_strategie3_soumission.sent', with_orig=False, with_cls=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5cf4cc-2fbe-4928-bced-b3883c64089a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2405/2405 [00:00<00:00, 90332.96 examples/s]\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/73693 [00:00<?, ? examples/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "finred_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "finred_dataset.save_to_disk('fingpt-finred')\n",
    "finred_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2a7db-3526-4a2b-bcb0-b1f03a509657",
   "metadata": {},
   "source": [
    "#### Version with entity:type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5be70a68-19d2-4131-b541-b24f7276de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('mydata/relations.txt') as f:\n",
    "#relations = [r.strip() for r in set(df_train['relation'].to_list())]\n",
    "\n",
    "    \n",
    "def get_instruction(sent, tuples, rel_possibles, with_orig=True, with_cls=False):\n",
    "    \n",
    "    instructions, inputs, outputs = [], [], []\n",
    "    if with_orig:\n",
    "        instructions.append(f\"Given a paragraph that describes the relationship between two words/phrases as options, extract the word/phrase pair and the corresponding lexical relationship between them from the input text. The output format should be \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Options: {', '.join(relations)}. \")\n",
    "        instructions.append(f\"Given the input paragraph, please extract the subject and object containing a certain relation in the sentence according to the following relation types, in the format of \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Relations include: {'; '.join(relations)}.\")\n",
    "        instructions.append(f\"Provide a paragraph containing relationships between entities. Extract and identify the specific relations between entity pairs mentioned in the paragraph. Output the results in the format \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Relations should be determined based on the context provided in the paragraph. Relations include: {'; '.join(relations)}.\")\n",
    "\n",
    "        inputs.extend([sent] * 3)\n",
    "        outputs.extend([\"; \".join([f\"{tup[-1]}: {tup[0]}, {tup[1]}\" for tup in tuples])] * 3)\n",
    "    \n",
    "    if with_cls:\n",
    "        for tup in tuples:\n",
    "            e1, type_e1 = tup[0].split(\"_\")\n",
    "            e2, type_e2 = tup[1].split(\"_\")\n",
    "            #instructions.append(f\"Utilize the input text as a context reference, choose the right relationship between {tup[0]} and {tup[1]} from the options. Options: {', '.join(relations)}.\")\n",
    "            instructions.append(f\"\"\"Quelle est la relation entre les deux entités «{e1}» (de type «{type_e1}» )  et «{e2}» (de type «{type_e2}» ) dans le contexte du texte d'entrée. Choisissez une réponse parmi : [ {rel_possibles} ].\"\"\")\n",
    "            inputs.extend([sent] * 1)\n",
    "            outputs.extend([tup[-1]] * 1)\n",
    "    \n",
    "    return instructions, inputs, outputs\n",
    "\n",
    "\n",
    "def get_finred_dataset(sent_file, tup_file, rel_file, with_orig, with_cls):\n",
    "\n",
    "    instructions, inputs, outputs = [], [], []\n",
    "\n",
    "    with open(sent_file) as f:\n",
    "        sentences = [s.strip() for s in f.readlines()]\n",
    "        #indices = random.sample(range(len(sentences)), 5000)\n",
    "    with open(tup_file) as f:\n",
    "        tuples_list = [s.split(' | ') for s in f.readlines()]\n",
    "    with open(rel_file) as f:\n",
    "        relations = [s.strip() for s in f.readlines()]\n",
    "    # Traduit les relations en francais \n",
    "    \n",
    "    ###############################################################\n",
    "    tuples_list_fr = []\n",
    "    for tuples in tuples_list:\n",
    "        tuples_fr =[]\n",
    "        for tupl in tuples:\n",
    "            e1, e2, rel = tupl.split(\" ; \")\n",
    "            rel = rel.strip().upper().replace('DEATHS_NUMBER', \"DEATH_NUMBER\").replace('HAS_GENDER_FEMALE', \"GENDER_FEMALE\").replace('HAS_GENDER_MALE', \"GENDER_MALE\").replace('WAS_CREATED_IN', \"CREATED_IN\")\n",
    "            rel = rel.replace('WAS_DISSOLVED_IN', \"DISSOLVED_IN\")\n",
    "            rel_fr = relations_trad[rel]\n",
    "            new_tuple = f\"{e1} ; {e2} ; {rel_fr}\"\n",
    "            tuples_fr.append(new_tuple)\n",
    "        tuples_list_fr.append(tuples_fr)\n",
    "    tuples_list = tuples_list_fr\n",
    "    ####################\n",
    "    relations_fr = []\n",
    "    for rel in relations :\n",
    "        lis_rel = rel.split(\";\")\n",
    "        list_rel_fr =[]\n",
    "        for i in range(len(lis_rel)):\n",
    "            rel = lis_rel[i].strip().upper().replace('DEATHS_NUMBER', \"DEATH_NUMBER\").replace('HAS_GENDER_FEMALE', \"GENDER_FEMALE\").replace('HAS_GENDER_MALE', \"GENDER_MALE\").replace('WAS_CREATED_IN', \"CREATED_IN\")\n",
    "            rel = rel.replace('WAS_DISSOLVED_IN', \"DISSOLVED_IN\")\n",
    "            rel_fr = relations_trad[rel]\n",
    "            list_rel_fr.append(rel_fr)\n",
    "        rel_poss_fr = \" ; \".join(list_rel_fr)\n",
    "        relations_fr.append(rel_poss_fr)\n",
    "    relations = relations_fr\n",
    "    ###############################################################\n",
    "    \n",
    "    indices = final_indices_train\n",
    "    sentences = [sentences[i] for i in indices]\n",
    "    tuples_list = [tuples_list[i] for i in indices]\n",
    "    relations = [relations[i] for i in indices]\n",
    "    for sent, tuples, rel_possibles in zip(sentences, tuples_list, relations):\n",
    "        tuples = [[e.strip() for e in tup.split(' ; ')] for tup in tuples]\n",
    "        rel_possibles = \" ; \".join(list(set(rel_possibles.split(\" ; \"))))\n",
    "        ins, i, o = get_instruction(sent, tuples, rel_possibles, with_orig, with_cls)\n",
    "        \n",
    "        instructions.extend(ins)\n",
    "        inputs.extend(i)\n",
    "        outputs.extend(o)\n",
    "    print(\"longuer = \",len(outputs))\n",
    "    print(\"exemple : \",outputs[:10])\n",
    "    print(\"exemple Instruciton: \",instructions[10])    \n",
    "    return Dataset.from_dict({\n",
    "        'input': inputs,\n",
    "        'output': outputs,\n",
    "        'instruction': instructions\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e31a1137-1b01-4a32-be08-3a890bd3473b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longuer =  9026\n",
      "exemple :  ['A_COMMENCÉ_EN', 'EST_EN_CONTACT_AVEC', 'PAS_DE_RELATION', 'EST_EN_CONTACT_AVEC', 'PAS_DE_RELATION', 'PAS_DE_RELATION', 'A_UNE_CATEGORIE', 'EST_LOCALISÉ_EN', 'OPÈRE_EN', 'CRÉÉ']\n",
      "exemple Instruciton:  Quelle est la relation entre les deux entités «individus» (de type «Actor» )  et «disputaient» (de type «Event» ) dans le contexte du texte d'entrée. Choisissez une réponse parmi : [ PAS_DE_RELATION ; INITIÉ ].\n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_finred_dataset('/projects/melodi/mettaleb/Textmine/train_strategie3.sent', '/projects/melodi/mettaleb/Textmine/train_strategie3_entity_type.tup', '/projects/melodi/mettaleb/Textmine/relation_possibles_strategie3_train.sent', with_orig=False, with_cls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6e3e6b3-0d6d-4a12-8a44-5319f5388e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longuer =  2405\n",
      "exemple :  ['A_UNE_CE_CONTROLE_SUR', 'PAS_DE_RELATION', 'EST_LOCALISÉ_EN', 'EST_DE_TAILLE', 'A_UNE_CE_CONTROLE_SUR', 'PAS_DE_RELATION', 'PAS_DE_RELATION', 'A_UNE_CE_CONTROLE_SUR', 'PAS_DE_RELATION', 'OPÈRE_EN']\n",
      "exemple Instruciton:  Quelle est la relation entre les deux entités «explosion» (de type «Event» )  et «brûlé» (de type «Event» ) dans le contexte du texte d'entrée. Choisissez une réponse parmi : [ A_DES_CONSÉQUENCES ; PAS_DE_RELATION ].\n"
     ]
    }
   ],
   "source": [
    "test_dataset = get_finred_dataset('/projects/melodi/mettaleb/Textmine/test_strategie3.sent', '/projects/melodi/mettaleb/Textmine/test_strategie3_entity_type.tup','/projects/melodi/mettaleb/Textmine/relation_possibles_strategie3_test.sent', with_orig=False, with_cls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23448411-a0f8-4359-8d49-b49d5b9de5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 9026/9026 [00:00<00:00, 102385.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2405/2405 [00:00<00:00, 64341.72 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 9026\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 2405\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "finred_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "finred_dataset.save_to_disk('fingpt-finred')\n",
    "finred_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362ba8b-0ced-4c2b-b340-1405e6b87f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6336aa47-6d96-432e-9130-8c7e09ead78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613c552-59bf-4924-9da8-8759562fc2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7dbdb79-21ff-4b22-ab0a-a82142ba6f46",
   "metadata": {},
   "source": [
    "## Strategie 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e172964-bfa5-4d09-8081-75bba5b78d81",
   "metadata": {},
   "source": [
    "### 4.1  Modele de classification binaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c232ba71-1df0-4a2c-bbc7-4892f5073f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('mydata/relations.txt') as f:\n",
    "#relations = [r.strip() for r in set(df_train['relation'].to_list())]\n",
    "\n",
    "    \n",
    "def get_instruction(sent, tuples, with_orig=True, with_cls=False):\n",
    "    \n",
    "    instructions, inputs, outputs = [], [], []\n",
    "    if with_orig:\n",
    "        instructions.append(f\"Given a paragraph that describes the relationship between two words/phrases as options, extract the word/phrase pair and the corresponding lexical relationship between them from the input text. The output format should be \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Options: {', '.join(relations)}. \")\n",
    "        instructions.append(f\"Given the input paragraph, please extract the subject and object containing a certain relation in the sentence according to the following relation types, in the format of \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Relations include: {'; '.join(relations)}.\")\n",
    "        instructions.append(f\"Provide a paragraph containing relationships between entities. Extract and identify the specific relations between entity pairs mentioned in the paragraph. Output the results in the format \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Relations should be determined based on the context provided in the paragraph. Relations include: {'; '.join(relations)}.\")\n",
    "\n",
    "        inputs.extend([sent] * 3)\n",
    "        outputs.extend([\"; \".join([f\"{tup[-1]}: {tup[0]}, {tup[1]}\" for tup in tuples])] * 3)\n",
    "    \n",
    "    if with_cls:\n",
    "        for tup in tuples:\n",
    "            #instructions.append(f\"Utilize the input text as a context reference, choose the right relationship between {tup[0]} and {tup[1]} from the options. Options: {', '.join(relations)}.\")\n",
    "            instructions.append(f\"\"\"Analyser le texte suivant et déterminer s'il existe une relation directe entre les deux entités spécifiées. Répondez uniquement par OUI s’il existe une relation et par NON sinon.\\nEntité 1 : {tup[0]}.\\nEntité 2 : {tup[1]}\"\"\")\n",
    "            inputs.extend([sent] * 1)\n",
    "            outputs.extend([tup[-1]] * 1)\n",
    "    \n",
    "    return instructions, inputs, outputs\n",
    "\n",
    "\n",
    "def get_finred_dataset(sent_file, tup_file, with_orig, with_cls):\n",
    "\n",
    "    instructions, inputs, outputs = [], [], []\n",
    "\n",
    "    with open(sent_file) as f:\n",
    "        sentences = [s.strip() for s in f.readlines()]\n",
    "        #indices = random.sample(range(len(sentences)), 5000)\n",
    "    with open(tup_file) as f:\n",
    "        tuples_list = [s.split(' | ') for s in f.readlines()]\n",
    "    \n",
    "    ###############################################################\n",
    "    tuples_list_fr = []\n",
    "    for tuples in tuples_list:\n",
    "        tuples_fr =[]\n",
    "        for tupl in tuples:\n",
    "            e1, e2, rel = tupl.split(\" ; \")\n",
    "            if \"PAS_DE_RELATION\" in rel:\n",
    "                rel_fr = \"NON\"\n",
    "            else:\n",
    "                rel_fr = \"OUI\"\n",
    "            new_tuple = f\"{e1} ; {e2} ; {rel_fr}\"\n",
    "            tuples_fr.append(new_tuple)\n",
    "        tuples_list_fr.append(tuples_fr)\n",
    "    tuples_list = tuples_list_fr\n",
    "    ###############################################################\n",
    "\n",
    "    for sent, tuples in zip(sentences, tuples_list):\n",
    "        tuples = [[e.strip() for e in tup.split(' ; ')] for tup in tuples]\n",
    "        ins, i, o = get_instruction(sent, tuples, with_orig, with_cls)\n",
    "        \n",
    "        instructions.extend(ins)\n",
    "        inputs.extend(i)\n",
    "        outputs.extend(o)\n",
    "    print(\"longuer = \",len(outputs))\n",
    "    print(\"exemple : \",outputs[:10])\n",
    "    print(\"exemple Instruciton: \",instructions[10])    \n",
    "    return Dataset.from_dict({\n",
    "        'input': inputs,\n",
    "        'output': outputs,\n",
    "        'instruction': instructions\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c813d961-84a5-4725-9b6a-3519f8c47bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longuer =  31362\n",
      "exemple :  ['NON', 'OUI', 'NON', 'NON', 'NON', 'NON', 'NON', 'OUI', 'NON', 'OUI']\n",
      "exemple Instruciton:  Analyser le texte suivant et déterminer s'il existe une relation directe entre les deux entités spécifiées. Répondez uniquement par OUI s’il existe une relation et par NON sinon.\n",
      "Entité 1 : hôpital.\n",
      "Entité 2 : Italie\n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_finred_dataset('/projects/melodi/mettaleb/Textmine/train_strategie3.sent', '/projects/melodi/mettaleb/Textmine/train_strategie3.tup', with_orig=False, with_cls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5de62c-c446-4420-996f-b23248ddb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = get_finred_dataset('/projects/melodi/mettaleb/Textmine/test_strategie3.sent', '/projects/melodi/mettaleb/Textmine/test_strategie3.tup', with_orig=False, with_cls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aef556-5d49-48fd-8a03-2da3208de027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7badba4-3fbe-4149-90b4-cdde8c0bcecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5ff7c-c56e-4890-bede-da47e93972ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb1597-c824-4175-b10d-865d619468f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9d204-e58d-4920-b8af-65caf365622c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd63f44-be7c-4d7b-8a74-ec3350c0a23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ff5248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longuer =  4000\n",
      "exemple :  ['shareholder_of: Badgeville, Norwest Venture Partners', 'competitor_of: Bioverativ Inc., Baxalta', 'client_of: Baton Rouge Southern Railroad, Kansas City Southern', 'shareholder_of: PEG Africa Ltd., Blue Haven Initiative', 'product_or_service_of: Lewis Galoob Toys, Inc., Micro Machines', 'product_or_service_of: Blue-Tongue Films, The Gift', 'collaboration: Brightmail Inc., Excite', 'acquired_by: Elbit Systems Ltd., OIP Sensor Systems', 'client_of: Vision Crew Unlimited, Coca-Cola', 'subsidiary_of: United Business Media, UBM Technology Group']\n",
      "longuer =  708\n",
      "exemple :  ['undefined: Winnie, Inc., Android', 'product_or_service_of: Marker International, Duplex', 'shareholder_of: Stone & Wood Brewing Co., Lion', 'collaboration: Bell Canada, Cellport Systems, Inc.', 'subsidiary_of: Global Electronic Trading Company, GETCO', 'subsidiary_of: Caspian Drilling Company, State Oil Company of Azerbaijan Republic', 'client_of: oeFun, Wii', 'product_or_service_of: Roja, Aalayam Productions', 'undefined: BMD Group, The Australian Financial Review', 'competitor_of: MidOcean Partners, Morgan Stanley']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b788cf32f1d41e6ae333294415218c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b77746f4f654c6b816b150f63b3e0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/708 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 708\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = get_finred_dataset('mydata/train.sent', 'mydata/train.tup', with_orig=True, with_cls=False)\n",
    "test_dataset = get_finred_dataset('mydata/test.sent', 'mydata/test.tup', with_orig=True, with_cls=False)\n",
    "\n",
    "finred_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "finred_dataset.save_to_disk('fingpt-finred-re')\n",
    "finred_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163e40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb5fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880b9512",
   "metadata": {},
   "source": [
    "## CLS Ver. for Zero-shot Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23642282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('mydata/relations.txt') as f:\n",
    " #   all_relations = [r.strip() for r in f.readlines()]\n",
    "all_relations = [r.strip() for r in set(df_train['relation'].to_list())]\n",
    "\n",
    "\n",
    "def get_instruction(sent, tuples):\n",
    "    \n",
    "    instructions, inputs, outputs = [], [], []\n",
    "    for tup in tuples:        \n",
    "        #output = tup[-1].replace('_', ' ').replace(' / ', '/').replace(' or ', '/')\n",
    "        output = tup[-1]\n",
    "        relations = all_relations.copy()\n",
    "        if output in relations:\n",
    "            relations.remove(output)\n",
    "        random.shuffle(relations)\n",
    "        relations = relations[:3] + [output]\n",
    "        random.shuffle(relations)\n",
    "        instructions.append(f\"Utilize the input text as a context reference, choose the right relationship between '{tup[0]}' and '{tup[1]}' from the options.\\nOptions: {', '.join(relations)}\")\n",
    "        random.shuffle(relations)\n",
    "        instructions.append(f\"Refer to the input text as context and select the correct relationship between '{tup[0]}' and '{tup[1]}' from the available options.\\nOptions: {', '.join(relations)}\")\n",
    "        random.shuffle(relations)\n",
    "        instructions.append(f\"Take context from the input text and decide on the accurate relationship between '{tup[0]}' and '{tup[1]}' from the options provided.\\nOptions: {', '.join(relations)}\")\n",
    "        random.shuffle(relations)\n",
    "        instructions.append(f\"What is the relationship between '{tup[0]}' and '{tup[1]}' in the context of the input sentence.\\nOptions: {', '.join(relations)}\")\n",
    "        random.shuffle(relations)\n",
    "        instructions.append(f\"In the context of the input sentence, determine the relationship between '{tup[0]}' and '{tup[1]}'.\\nOptions: {', '.join(relations)}\")\n",
    "        random.shuffle(relations)\n",
    "        instructions.append(f\"Analyze the relationship between '{tup[0]}' and '{tup[1]}' within the context of the input sentence.\\nOptions: {', '.join(relations)}\")\n",
    "        inputs.extend([sent] * 6)\n",
    "        outputs.extend([output] * 6)\n",
    "    \n",
    "    return instructions, inputs, outputs\n",
    "\n",
    "\n",
    "def get_finred_dataset(sent_file, tup_file):\n",
    "    \n",
    "    random.seed(0)\n",
    "    instructions, inputs, outputs = [], [], []\n",
    "\n",
    "    with open(sent_file) as f:\n",
    "        sentences = [s.strip() for s in f.readlines()]\n",
    "    with open(tup_file) as f:\n",
    "        tuples_list = [s.split(' | ') for s in f.readlines()]\n",
    "        \n",
    "    for sent, tuples in zip(sentences, tuples_list):\n",
    "        tuples = [[e.strip() for e in tup.split(' ; ')] for tup in tuples]\n",
    "        \n",
    "        ins, i, o = get_instruction(sent, tuples)\n",
    "        \n",
    "        instructions.extend(ins)\n",
    "        inputs.extend(i)\n",
    "        outputs.extend(o)\n",
    "        \n",
    "    return Dataset.from_dict({\n",
    "        'input': inputs,\n",
    "        'output': outputs,\n",
    "        'instruction': instructions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a05a630",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'FinRED/train.sent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset_instruct \u001b[38;5;241m=\u001b[39m \u001b[43mget_finred_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFinRED/train.sent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFinRED/train.tup\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test_dataset_instruct \u001b[38;5;241m=\u001b[39m get_finred_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinRED/test.sent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinRED/test.tup\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m finred_dataset_instruct \u001b[38;5;241m=\u001b[39m DatasetDict({\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train_dataset_instruct,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: test_dataset_instruct\n\u001b[1;32m      7\u001b[0m })\n",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m, in \u001b[0;36mget_finred_dataset\u001b[0;34m(sent_file, tup_file)\u001b[0m\n\u001b[1;32m     36\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     37\u001b[0m instructions, inputs, outputs \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msent_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     40\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [s\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tup_file) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FinRED/train.sent'"
     ]
    }
   ],
   "source": [
    "train_dataset_instruct = get_finred_dataset('FinRED/train.sent', 'FinRED/train.tup')\n",
    "test_dataset_instruct = get_finred_dataset('FinRED/test.sent', 'FinRED/test.tup')\n",
    "\n",
    "finred_dataset_instruct = DatasetDict({\n",
    "    'train': train_dataset_instruct,\n",
    "    'test': test_dataset_instruct\n",
    "})\n",
    "\n",
    "finred_dataset_instruct.save_to_disk('fingpt-finred-cls-instruct')\n",
    "finred_dataset_instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1add3071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Utilize the input text as a context reference, choose the right relationship between 'Apple Inc' and 'Steve Jobs' from the options.\\nOptions: industry, founded by, owner of, currency\",\n",
       " \"Refer to the input text as context and select the correct relationship between 'Apple Inc' and 'Steve Jobs' from the available options.\\nOptions: industry, currency, owner of, founded by\",\n",
       " \"Take context from the input text and decide on the accurate relationship between 'Apple Inc' and 'Steve Jobs' from the options provided.\\nOptions: industry, currency, owner of, founded by\",\n",
       " \"What is the relationship between 'Apple Inc' and 'Steve Jobs' in the context of the input sentence.\\nOptions: currency, founded by, owner of, industry\",\n",
       " \"In the context of the input sentence, determine the relationship between 'Apple Inc' and 'Steve Jobs'.\\nOptions: industry, founded by, owner of, currency\",\n",
       " \"Analyze the relationship between 'Apple Inc' and 'Steve Jobs' within the context of the input sentence.\\nOptions: currency, founded by, owner of, industry\",\n",
       " \"Utilize the input text as a context reference, choose the right relationship between 'Apple Inc' and 'Steve Jobs' from the options.\\nOptions: chief executive officer, headquarters location, subsidiary, industry\",\n",
       " \"Refer to the input text as context and select the correct relationship between 'Apple Inc' and 'Steve Jobs' from the available options.\\nOptions: subsidiary, chief executive officer, headquarters location, industry\",\n",
       " \"Take context from the input text and decide on the accurate relationship between 'Apple Inc' and 'Steve Jobs' from the options provided.\\nOptions: chief executive officer, industry, subsidiary, headquarters location\",\n",
       " \"What is the relationship between 'Apple Inc' and 'Steve Jobs' in the context of the input sentence.\\nOptions: subsidiary, headquarters location, chief executive officer, industry\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finred_dataset_instruct['train']['instruction'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73828842",
   "metadata": {},
   "source": [
    "# Prepare my data :CLS Ver. for Zero-shot Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fb26876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfcb2abe55d48eeacc1d814b2c8f251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/24000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67063c42ca1d4278be33650f6dc732f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 24000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 4248\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_instruct = get_finred_dataset('mydata/train.sent', 'mydata/train.tup')\n",
    "test_dataset_instruct = get_finred_dataset('mydata/test.sent', 'mydata/test.tup')\n",
    "\n",
    "finred_dataset_instruct = DatasetDict({\n",
    "    'train': train_dataset_instruct,\n",
    "    'test': test_dataset_instruct\n",
    "})\n",
    "\n",
    "finred_dataset_instruct.save_to_disk('fingpt-finred-cls-instruct')\n",
    "finred_dataset_instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01ce7cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shareholder of',\n",
       " 'shareholder of',\n",
       " 'shareholder of',\n",
       " 'shareholder of',\n",
       " 'shareholder of',\n",
       " 'shareholder of',\n",
       " 'competitor of',\n",
       " 'competitor of',\n",
       " 'competitor of',\n",
       " 'competitor of']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finred_dataset_instruct['train']['output'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "08c54fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005600690841674805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Saving the dataset (0/1 shards)",
       "rate": null,
       "total": 17110,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1cce16e3de4d49ae7ac66647e093a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/17110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'instruction'],\n",
       "    num_rows: 17110\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_dict, question_dict = {}, {}\n",
    "for i, row in docs.iterrows():\n",
    "    doc_dict[row['docid']] = row['doc']\n",
    "for i, row in questions.iterrows():\n",
    "    question_dict[row['qid']] = row['question']\n",
    "    \n",
    "instruction_templates = [\n",
    "    \"Utilize your financial knowledge, give your answer or opinion to the input question or subject . Answer format is not limited.\",\n",
    "    \"Offer your insights or judgment on the input financial query or topic using your financial expertise. Reply as normal question answering\",\n",
    "    \"Based on your financial expertise, provide your response or viewpoint on the given financial question or topic. The response format is open.\",\n",
    "    \"Share your insights or perspective on the financial matter presented in the input.\",\n",
    "    \"Offer your thoughts or opinion on the input financial query or topic using your financial background.\"\n",
    "]\n",
    "\n",
    "inputs, outputs, instructions = [], [], []\n",
    "for i, row in qa_pairs.iterrows():\n",
    "    qid, docid = row['qid'], row['docid']\n",
    "    q = str(question_dict[qid])\n",
    "    doc = str(doc_dict[docid])\n",
    "    inputs.append(q)\n",
    "    outputs.append(doc)\n",
    "    instructions.append(instruction_templates[i%5])\n",
    "\n",
    "fiqa_qa_dataset = Dataset.from_dict({\n",
    "    'input': inputs,\n",
    "    'output': outputs,\n",
    "    'instruction': instructions\n",
    "})\n",
    "fiqa_qa_dataset.save_to_disk('fingpt-fiqa_qa')\n",
    "fiqa_qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b93b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (10.0.1.dev0+ga6eabc2b.d20230428)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /users/melodi/mettaleb/.local/lib/python3.10/site-packages (from pyarrow) (1.26.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a20cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d3a002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_parquet = pq.read_table(\"train.parquet\")\n",
    "\n",
    "dataframe_parquet = table_parquet.to_pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd350995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27558, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_parquet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa59f94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce1679d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094c33ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf11230-088e-425b-8cd0-23eb2e3bc317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
