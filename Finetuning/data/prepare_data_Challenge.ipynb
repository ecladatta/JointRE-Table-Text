{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90f7f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mettaleb/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "import random\n",
    "import datasets\n",
    "import torch\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoConfig, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534c2c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projects/melodi/mettaleb/FinGPT/fingpt/FinGPT_Benchmark/data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf299e-62eb-45d0-82b4-4d991a1b286d",
   "metadata": {},
   "source": [
    "## Data Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc77527-69a1-46b9-a613-8b168973de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations= ['acquired_by',\n",
    " 'brand_of',\n",
    " 'client_of',\n",
    " 'collaboration',\n",
    " 'competitor_of',\n",
    " 'merged_with',\n",
    " 'product_or_service_of',\n",
    " 'regulated_by',\n",
    " 'shareholder_of',\n",
    " 'subsidiary_of',\n",
    " 'traded_on',\n",
    " 'undefined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011a5bc1-92db-448a-9c65-72085e36ef66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0:\n",
      "  Text: GV Filmsis an Indianfilm productionanddistributioncompany headed byIshari K. Ganesh. The firm had been a leading production studio in the Tamil film i ...\n",
      "  Nb tables: 1\n",
      "  Triplets: \n",
      "--------------------------------------------------\n",
      "Doc 1:\n",
      "  Text: Spanfeller Media Group(SMG), asubsidiaryof publishing companyTribune Publishing, is adigital mediacompany based inNew York City. It was founded in 201 ...\n",
      "  Nb tables: 1\n",
      "  Triplets: Tribune Publishing ; Spanfeller Media Group(SMG) ; subsidiary_of\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/projects/melodi/mettaleb/Annotation/corpus_challenge/test/F2_nous.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "d = {}\n",
    "\n",
    "for idx, doc in enumerate(data.get(\"documents\", [])):\n",
    "    # ---- TEXTS ----\n",
    "    texts = []\n",
    "    extraction_meta = doc.get(\"raw\", {}).get(\"_source\", {}).get(\"extractionMetadata\", [])\n",
    "    for meta in extraction_meta:\n",
    "        for t in meta.get(\"texts\", []):\n",
    "            texts.append(t.get(\"value\", \"\"))\n",
    "    texts = \" \".join(texts).strip()\n",
    "\n",
    "    # ---- TABLES ----\n",
    "    tables = []\n",
    "    for meta in extraction_meta:\n",
    "        for tbl in meta.get(\"tables\", []):\n",
    "            tables.append({\"tableData\": tbl.get(\"tableData\", [])})\n",
    "\n",
    "    # ---- TRIPLETS ----\n",
    "    triplets_list = []\n",
    "    for ann in doc.get(\"annotations\", []):\n",
    "        subj = ann.get(\"subject\", {}).get(\"annotationValue\", \"\")\n",
    "        obj = ann.get(\"object\", {}).get(\"annotationValue\", \"\")\n",
    "        pred_val = ann.get(\"predicate\", {}).get(\"entityValue\", \"\")\n",
    "        # on ignore si predicate == pertinence\n",
    "        if pred_val.lower() == \"pertinence\":\n",
    "            continue\n",
    "        triplet_str = f\"{subj} ; {obj} ; {pred_val}\"\n",
    "        triplets_list.append(triplet_str)\n",
    "\n",
    "    triplets = \" | \".join(triplets_list)\n",
    "\n",
    "    d[idx] = [texts, tables, triplets]\n",
    "\n",
    "for k, v in list(d.items())[:2]:\n",
    "    print(f\"Doc {k}:\")\n",
    "    print(\"  Text:\", v[0][:150], \"...\")\n",
    "    print(\"  Nb tables:\", len(v[1]))\n",
    "    print(\"  Triplets:\", v[2])\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f7e30-47f4-4f13-8f6f-80f0c1f71552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f13748f3-663a-451c-8174-99189510d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "Texts = []\n",
    "Tables = []\n",
    "Triplets = []\n",
    "for k in d:\n",
    "    Texts.append(d[k][0])\n",
    "    Tables.append(d[k][1])\n",
    "    Triplets.append(d[k][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a4493cc-082f-434f-a706-efce147b0472",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = Texts[:170]\n",
    "text_test = Texts[170:]\n",
    "tables_train = Tables[:170]\n",
    "tables_test = Tables[170:]\n",
    "triplets_train = Triplets[:170]\n",
    "triplets_test = Triplets[170:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f655a04-3de9-49bf-8c8b-b8df5a3f833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./mydata/train.tup\", \"w\") as f:\n",
    "    for triplet in triplets_train:\n",
    "        f.write(f\"{triplet}\\n\")\n",
    "with open(\"./mydata/test.tup\", \"w\") as f:\n",
    "    for triplet in triplets_train:\n",
    "        f.write(f\"{triplet}\\n\")\n",
    "with open(\"./mydata/train.sent\", \"w\") as f:\n",
    "    for text in text_train:\n",
    "        text = text.strip().replace(\"\\n\",\"\")\n",
    "        f.write(f\"{text}\\n\")\n",
    "with open(\"./mydata/test.sent\", \"w\") as f:\n",
    "    for text in text_test:\n",
    "        text = text.strip().replace(\"\\n\",\"\")\n",
    "        f.write(f\"{text}\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f76c2df1-0065-46d6-a225-52bb362e0ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml_tables_train = []\n",
    "xml_tables_test = []\n",
    "for table_group in tables_test:\n",
    "    for tbl in table_group:\n",
    "        root = ET.Element(\"table\")\n",
    "        for row_data in tbl[\"tableData\"]:\n",
    "            row_el = ET.SubElement(root, \"row\")\n",
    "            for cell_data in row_data:\n",
    "                cell_el = ET.SubElement(row_el, \"cell\")\n",
    "                cell_el.text = str(cell_data).strip()\n",
    "        xml_str = ET.tostring(root, encoding=\"unicode\")\n",
    "        xml_tables_test.append(xml_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3d344bb-f3f2-4be9-8dbd-313e02efc5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_train = xml_tables_train\n",
    "tables_test = xml_tables_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f128fffd-9cce-4d78-a5c5-52227235793f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 85)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tables_train) , len(tables_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9065a-1dbb-48c6-bdda-1b10d3996501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7624e699-6c79-4762-b967-ed263ab278a0",
   "metadata": {},
   "source": [
    "## Version data Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d147dab-3fa5-48cd-b686-5a16b120b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Prompt = f\"\"\"  \n",
    "    You are an expert in Natural Language Processing (NLP) specializing in relation extraction. Your task is to extract relations expressed as triplets: (entity1, relation, entity2).\\n\n",
    "    Ensuring that:\n",
    "    - Both entity1 and entity2 are valid named entities.\n",
    "    - Each identified relation must originate jointly from two sources: the text and the table.\n",
    "    - The extracted triplets must reflect connections that are only valid when considering both sources together, not when taken in isolation.\n",
    "    \n",
    "    Available Data:\n",
    "    - Text segment:\n",
    "    - Table content: \n",
    "    - Possible relation types: [{relations}]\n",
    "    \n",
    "    Tasks:\n",
    "    - Identify relations where at least one entity is found in the text and the other entity is found in the table.\n",
    "    - Construct relation triplets combining entities from both sources.\n",
    "    - Return triplets in the following standardized format: entity1, entity2: relation1 | entity3, entity4: relations2\n",
    "    Output Expected: A set of triplets in the precise format above.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "685815e4-3ada-4da9-a6bb-e891ac3ff4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "    You are an expert in Natural Language Processing (NLP) specializing in relation extraction. Your task is to extract relations expressed as triplets: (entity1, relation, entity2).\n",
      "\n",
      "    Ensuring that:\n",
      "    - Both entity1 and entity2 are valid named entities.\n",
      "    - Each identified relation must originate jointly from two sources: the text and the table.\n",
      "    - The extracted triplets must reflect connections that are only valid when considering both sources together, not when taken in isolation.\n",
      "    \n",
      "    Available Data:\n",
      "    - Text segment:\n",
      "    - Table content: \n",
      "    - Possible relation types: [['acquired_by', 'brand_of', 'client_of', 'collaboration', 'competitor_of', 'merged_with', 'product_or_service_of', 'regulated_by', 'shareholder_of', 'subsidiary_of', 'traded_on', 'undefined']]\n",
      "    \n",
      "    Tasks:\n",
      "    - Identify relations where at least one entity is found in the text and the other entity is found in the table.\n",
      "    - Construct relation triplets combining entities from both sources.\n",
      "    - Return triplets in the following standardized format: entity1, entity2: relation1 | entity3, entity4: relations2\n",
      "    Output Expected: A set of triplets in the precise format above.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfedc328-7120-49e7-b09f-a5ee32642f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instruction(sent, table, tuples, with_orig=True, with_cls=False):\n",
    "    \n",
    "    instructions, inputs, outputs = [], [], []\n",
    "    if with_orig:\n",
    "        instructions.append(f\"\"\"  \n",
    "    You are an expert in Natural Language Processing (NLP) specializing in relation extraction. Your task is to extract relations expressed as triplets: (entity1, relation, entity2).\\n\n",
    "    Ensuring that:\n",
    "    - Both entity1 and entity2 are valid named entities.\n",
    "    - Each identified relation must originate jointly from two sources: the text and the table.\n",
    "    - The extracted triplets must reflect connections that are only valid when considering both sources together, not when taken in isolation.\n",
    "    \n",
    "    Available Data:\n",
    "    - Text segment\n",
    "    - Table content\n",
    "    - Possible relation types: [{relations}]\n",
    "    \n",
    "    Tasks:\n",
    "    - Identify relations where at least one entity is found in the text and the other entity is found in the table.\n",
    "    - Construct relation triplets combining entities from both sources.\n",
    "    - Return triplets in the following standardized format: entity1, entity2: relation1 | entity3, entity4: relations2\n",
    "    Output Expected: A set of triplets in the precise format above.\n",
    "    \"\"\")\n",
    "        inp = \"Text segment: \"+sent + \".\\n\" + \"Table content: \"+str(table)\n",
    "        inputs.extend([inp])\n",
    "        #inputs.extend([table])\n",
    "        outputs.extend([\"; \".join([f\"{tup[-1]}: {tup[0]}, {tup[1]}\" if len(tup) >= 3 else \"\" for tup in tuples])])\n",
    "    \n",
    "    if with_cls:\n",
    "        for tup in tuples:\n",
    "            #instructions.append(f\"Utilize the input text as a context reference, choose the right relationship between {tup[0]} and {tup[1]} from the options. Options: {', '.join(relations)}.\")\n",
    "            instructions.append(f\"What is the relationship between {tup[0]} and {tup[1]} in the context of the input sentence. Choose an answer from: {'; '.join(relations)}.\")\n",
    "            #instructions.append(f\"\"\"Analyze the following sentence and identify the relationship between the two mentioned entities. The relationship must be selected from the predefined list below. If none of the relationships apply, respond with \"undefined\".\\nEntité 1 : {tup[0]}.\\n \\nEntité 2 : {tup[1]}.\\nRelations : {'; '.join(relations)}.\"\"\")\n",
    "            inputs.extend([sent] * 1)\n",
    "            outputs.extend([tup[-1]] * 1)\n",
    "    \n",
    "    return instructions, inputs, outputs\n",
    "\n",
    "\n",
    "def get_finred_dataset(tup_file, paragraphes, tables, with_orig, with_cls):\n",
    "\n",
    "    instructions, inputs, outputs = [], [], []\n",
    "    with open(paragraphes) as f:\n",
    "        sentences = [s.strip() for s in f.readlines()]\n",
    "    print(f\"{len(sentences)} phrases\")\n",
    "    with open(tup_file) as f:\n",
    "        tuples_list = [s.split(' | ') for s in f.readlines()]\n",
    "    print(f\"{len(tuples_list)} tuples\")\n",
    "        \n",
    "    for sent, tuples, table in zip(sentences, tuples_list, tables):      \n",
    "        tuples = [[e.strip() for e in tup.split(' ; ')] for tup in tuples]\n",
    "        ins, i, o = get_instruction( sent, table, tuples, with_orig, with_cls)\n",
    "        \n",
    "        instructions.extend(ins)\n",
    "        inputs.extend(i)\n",
    "        outputs.extend(o)\n",
    "    print(\"longuer = \",len(outputs))\n",
    "    print(\"exemple output : \",outputs[0])\n",
    "    print(\"exemple input : \",inputs[0])\n",
    "    print(\"instructions : \",instructions[0])\n",
    "        \n",
    "    return Dataset.from_dict({\n",
    "        'input': inputs,\n",
    "        'output': outputs,\n",
    "        'instruction': instructions\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be14e21-31bd-4484-a0ce-c30a5446a15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cce43214-80fc-4f76-bdb7-604449616087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 phrases\n",
      "170 tuples\n",
      "longuer =  170\n",
      "exemple output :  \n",
      "exemple input :  Text segment: GV Filmsis an Indianfilm productionanddistributioncompany headed byIshari K. Ganesh. The firm had been a leading production studio in the Tamil film industry in the 1990s and had been founded byG. Venkateswaranas Sujatha Films in 1986.[1][2].Sujatha Films was set up in 1986 byG. Venkateswaran, a chartered accountant, as a film production and distribution company. Operating as a family production house, Venkateswaran's brotherMani Ratnamalso often assisted on the production work of films that he directed for the studio.[3]Sujatha Films became GV Films as it became the first publicly listed company from the Indian media industry in 1989.[4].Following Venkateswaran's death, the studio continued to produce media content under the same name. Notably, actressManisha Koiralawas briefly a board member as the studio attempted to make a comeback through Hindi film content and 3D television serials.[13]The studio launched a big budget Hindi film directed byMahesh ManjrekarstarringSanjay Duttin late 2005, though it was later stalled.[14]Kasthuri Shankaralso worked with the studio and assisted on the oversight of the production ofUrchagam(2007).[15]The studio returned to prioritising distribution ventures and instead chose to make small budget films such asKaivantha Kalai(2006) andThirudi(2006).[16]In 2015, GV Films held a ceremony in Mumbai to mark 25 years since its founding.[17].\n",
      "Table content: <table><row><cell>0</cell><cell>1</cell></row><row><cell>Company type</cell><cell>Film production Film distribution</cell></row><row><cell>Industry</cell><cell>Entertainment</cell></row><row><cell>Founded</cell><cell>1989</cell></row><row><cell>Founder</cell><cell>G. Venkateswaran</cell></row><row><cell>Headquarters</cell><cell>Chennai, India</cell></row><row><cell>Products</cell><cell>Motion pictures (Tamil)</cell></row></table>\n",
      "instructions :    \n",
      "    You are an expert in Natural Language Processing (NLP) specializing in relation extraction. Your task is to extract relations expressed as triplets: (entity1, relation, entity2).\n",
      "\n",
      "    Ensuring that:\n",
      "    - Both entity1 and entity2 are valid named entities.\n",
      "    - Each identified relation must originate jointly from two sources: the text and the table.\n",
      "    - The extracted triplets must reflect connections that are only valid when considering both sources together, not when taken in isolation.\n",
      "    \n",
      "    Available Data:\n",
      "    - Text segment\n",
      "    - Table content\n",
      "    - Possible relation types: [['acquired_by', 'brand_of', 'client_of', 'collaboration', 'competitor_of', 'merged_with', 'product_or_service_of', 'regulated_by', 'shareholder_of', 'subsidiary_of', 'traded_on', 'undefined']]\n",
      "    \n",
      "    Tasks:\n",
      "    - Identify relations where at least one entity is found in the text and the other entity is found in the table.\n",
      "    - Construct relation triplets combining entities from both sources.\n",
      "    - Return triplets in the following standardized format: entity1, entity2: relation1 | entity3, entity4: relations2\n",
      "    Output Expected: A set of triplets in the precise format above.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_finred_dataset('mydata/train.tup','mydata/train.sent',tables_train,  with_orig=True, with_cls=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a64fc74-b784-4c7f-b7ee-f14090d23d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 phrases\n",
      "170 tuples\n",
      "longuer =  85\n",
      "exemple output :  \n",
      "exemple input :  Text segment: Phase 4 Filmswas a Canadianfilm distributioncompany headquartered in Toronto. It had two branches in theU.S.:Los Angeles, CaliforniaandFort Mill, South Carolina. Its subsidiary, Kaboom! Entertainment markets children's entertainment with companies such asCorus Entertainment..Phase 4 Films traces its history to Telegenic, a family-oriented film distributor that was founded in 1996. Berry Meyerowitz purchased Telegenic in 2000 and renamed it as \"Kaboom! Entertainment\". In 2006,Peace Arch EntertainmentGroup, which later merged with ContentFilm, purchased Kaboom!. Berry Meyerowitz founded Phase 4 Films in April 2009 when he bought back their North American distribution business.[1].\n",
      "Table content: <table><row><cell>0</cell><cell>1</cell></row><row><cell /><cell /></row><row><cell>Company type</cell><cell>Subsidiary</cell></row><row><cell>Industry</cell><cell>Film Animation</cell></row><row><cell>Predecessor</cell><cell>Peace Arch EntertainmentTelegenic</cell></row><row><cell>Founded</cell><cell>1996; 29 years ago</cell></row><row><cell>Founder</cell><cell>Berry Meyerowitz</cell></row><row><cell>Defunct</cell><cell>2015; 10 years ago</cell></row><row><cell>Fate</cell><cell>Folded into Entertainment One</cell></row><row><cell>Successor</cell><cell>Lionsgate Canada</cell></row><row><cell>Headquarters</cell><cell>Toronto, Ontario, Canada</cell></row><row><cell>Area served</cell><cell>North America</cell></row><row><cell>Key people</cell><cell>Berry Meyerowitz  (President &amp; CEO)</cell></row><row><cell>Services</cell><cell>Film distributionHome videoDigital distribution</cell></row><row><cell>Parent</cell><cell>Entertainment One (2014–2015)</cell></row><row><cell>Subsidiaries</cell><cell>KaBoom! Entertainment Inc.</cell></row></table>\n",
      "instructions :    \n",
      "    You are an expert in Natural Language Processing (NLP) specializing in relation extraction. Your task is to extract relations expressed as triplets: (entity1, relation, entity2).\n",
      "\n",
      "    Ensuring that:\n",
      "    - Both entity1 and entity2 are valid named entities.\n",
      "    - Each identified relation must originate jointly from two sources: the text and the table.\n",
      "    - The extracted triplets must reflect connections that are only valid when considering both sources together, not when taken in isolation.\n",
      "    \n",
      "    Available Data:\n",
      "    - Text segment\n",
      "    - Table content\n",
      "    - Possible relation types: [['acquired_by', 'brand_of', 'client_of', 'collaboration', 'competitor_of', 'merged_with', 'product_or_service_of', 'regulated_by', 'shareholder_of', 'subsidiary_of', 'traded_on', 'undefined']]\n",
      "    \n",
      "    Tasks:\n",
      "    - Identify relations where at least one entity is found in the text and the other entity is found in the table.\n",
      "    - Construct relation triplets combining entities from both sources.\n",
      "    - Return triplets in the following standardized format: entity1, entity2: relation1 | entity3, entity4: relations2\n",
      "    Output Expected: A set of triplets in the precise format above.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "test_dataset = get_finred_dataset('mydata/test.tup','mydata/test.sent',tables_test,  with_orig=True, with_cls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39e23d9c-074c-468d-a780-cbc101ac2104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 170/170 [00:00<00:00, 2419.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 85/85 [00:00<00:00, 5321.93 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 170\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 85\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finred_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "finred_dataset.save_to_disk('fingpt-finred')\n",
    "finred_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a90387-e4dd-403d-b030-408995deb027",
   "metadata": {},
   "source": [
    "## CORE Paragraphes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cba27bb-cefa-414d-91a4-ae4bed06ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/projects/melodi/mettaleb/Annotation/120_paragraphs.txt\") as f:\n",
    "    paragraphes = f.readlines()\n",
    "with open(\"/projects/melodi/mettaleb/Annotation/Labelsmodifier.txt\") as f:\n",
    "    Labels = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c905c14-59e1-4505-84b1-3b84b0653048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c88e53a-db0f-446a-8172-ba79a2c2dbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a03a590a-c465-4b7d-8ec0-1f02d2c268f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphesF = []\n",
    "LabelF = []\n",
    "for i in range(len(Labels)):\n",
    "    if \"pas_de_label\" not in Labels[i]:\n",
    "        list_labels =  Labels[i].strip().replace('\\n','').replace('\"','').split(\";\")\n",
    "        list_labels = [label for label in list_labels if label]\n",
    "        for j in range(len(list_labels)):\n",
    "            paragraphesF.append(paragraphes[i].strip())\n",
    "            LabelF.append(list_labels[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56219101-2998-48c2-853b-f69d9412dca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(553, 553)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphesF), len(LabelF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "578adbcb-0345-458c-b272-bf10de8eda61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the Kansas City Southern,The Baton Rouge Southern Railroad:client_of',\n",
       " 'The Baton Rouge Southern Railroad,Watco:shareholder_of',\n",
       " 'Energy Access Ventures,the company:shareholder_of',\n",
       " 'Blue Haven Initiative,the company:shareholder_of',\n",
       " 'Investisseurs & Partenaires,the company:shareholder_of',\n",
       " 'ENGIE Rassembleurs d’Energies,the company:shareholder_of',\n",
       " 'Impact Assets,the company:shareholder_of',\n",
       " 'Acumen,the company:shareholder_of',\n",
       " 'PCG Investments,the company:shareholder_of',\n",
       " 'Blue-Tongue Films,Evermore:client_of',\n",
       " 'Blue-Tongue Films,The Veronicas:client_of',\n",
       " 'Blue-Tongue Films,Empire of the Sun:client_of',\n",
       " 'Blue-Tongue Films,Rahzel:client_of',\n",
       " 'Animal Kingdom,Blue-Tongue Films:product_or_service_of',\n",
       " 'Hesher,Blue-Tongue Films:product_or_service_of']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LabelF[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b1f25978-68d4-40e5-9b7b-d771cc7e8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./mydata/116train667.sent\", \"w\") as file:\n",
    "    for i in range(len(paragraphesF[:400])):\n",
    "        file.write(f\"{paragraphesF[i]}\\n\")\n",
    "with open(\"./mydata/116test667.sent\", \"w\") as file:\n",
    "    for i in range(400,len(paragraphesF)):\n",
    "        file.write(f\"{paragraphesF[i]}\\n\")\n",
    "with open(\"./mydata/116train667.tup\", \"w\") as file:\n",
    "    for i in range(len(LabelF[:400])):\n",
    "        label = LabelF[i].replace(\",\", \" ; \").replace(\":\", \" ; \")\n",
    "        file.write(f\"{label}\\n\")\n",
    "with open(\"./mydata/116test667.tup\", \"w\") as file:\n",
    "    for i in range(400,len(LabelF)):\n",
    "        label = LabelF[i].replace(\",\", \" ; \").replace(\":\", \" ; \")\n",
    "        file.write(f\"{label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488a5c7-5950-4610-846b-de8ec49e9455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "54798118-426a-41ec-bbe1-cf7929f08629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('mydata/relations.txt') as f:\n",
    "#relations = [r.strip() for r in set(df_train['relation'].to_list())]\n",
    "\n",
    "    \n",
    "def get_instruction(sent, tuples, with_orig=True, with_cls=False):\n",
    "    \n",
    "    instructions, inputs, outputs = [], [], []\n",
    "    if with_orig:\n",
    "        instructions.append(f\"Given a paragraph that describes the relationship between two words/phrases as options, extract the word/phrase pair and the corresponding lexical relationship between them from the input text. The output format should be \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Options: {', '.join(relations)}. \")\n",
    "        instructions.append(f\"Given the input paragraph, please extract the subject and object containing a certain relation in the sentence according to the following relation types, in the format of \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Relations include: {'; '.join(relations)}.\")\n",
    "        instructions.append(f\"Provide a paragraph containing relationships between entities. Extract and identify the specific relations between entity pairs mentioned in the paragraph. Output the results in the format \\\"{{relation1: word1, word2}}; {{relation2: word3, word4}}\\\". Relations should be determined based on the context provided in the paragraph. Relations include: {'; '.join(relations)}.\")\n",
    "\n",
    "        inputs.extend([sent] * 3)\n",
    "        outputs.extend([\"; \".join([f\"{tup[-1]}: {tup[0]}, {tup[1]}\" for tup in tuples])] * 3)\n",
    "    \n",
    "    if with_cls:\n",
    "        for tup in tuples:\n",
    "            #instructions.append(f\"Utilize the input text as a context reference, choose the right relationship between {tup[0]} and {tup[1]} from the options. Options: {', '.join(relations)}.\")\n",
    "            instructions.append(f\"\"\"What is the relationship between '{{{tup[0]}}}' and '{{{tup[1]}}}' in the context of the input sentence. Choose an answer from: {{{'; '.join(relations)}}}.\\nOutput the results in the format: {{relation}}\"\"\")\n",
    "            #instructions.append(f\"\"\"Analyze the following sentence and identify the relationship between the two mentioned entities. The relationship must be selected from the predefined list below. If none of the relationships apply, respond with \"undefined\".\\nEntité 1 : {tup[0]}.\\n \\nEntité 2 : {tup[1]}.\\nRelations : {'; '.join(relations)}.\"\"\")\n",
    "            inputs.extend([sent] * 1)\n",
    "            outputs.extend([tup[-1]] * 1)\n",
    "    \n",
    "    return instructions, inputs, outputs\n",
    "\n",
    "\n",
    "def get_finred_dataset(sent_file, tup_file, with_orig, with_cls):\n",
    "\n",
    "    instructions, inputs, outputs = [], [], []\n",
    "\n",
    "    with open(sent_file) as f:\n",
    "        sentences = [s.strip() for s in f.readlines()]\n",
    "    with open(tup_file) as f:\n",
    "        tuples_list = [s.split(' | ') for s in f.readlines()]\n",
    "        \n",
    "    for sent, tuples in zip(sentences, tuples_list):\n",
    "        tuples = [[e.strip() for e in tup.split(' ; ')] for tup in tuples]\n",
    "        \n",
    "        ins, i, o = get_instruction(sent, tuples, with_orig, with_cls)\n",
    "        \n",
    "        instructions.extend(ins)\n",
    "        inputs.extend(i)\n",
    "        outputs.extend(o)\n",
    "    print(\"longuer = \",len(outputs))\n",
    "    print(\"exemple : \",outputs[:10])\n",
    "    print(\"exemple : \",instructions[:2])\n",
    "        \n",
    "    return Dataset.from_dict({\n",
    "        'input': inputs,\n",
    "        'output': outputs,\n",
    "        'instruction': instructions\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a4f8cb6a-f87b-4f52-8652-70c0d0df56b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longuer =  400\n",
      "exemple :  ['client_of', 'shareholder_of', 'shareholder_of', 'shareholder_of', 'shareholder_of', 'shareholder_of', 'shareholder_of', 'shareholder_of', 'shareholder_of', 'client_of']\n",
      "exemple :  [\"What is the relationship between '{the Kansas City Southern}' and '{The Baton Rouge Southern Railroad}' in the context of the input sentence. Choose an answer from: {acquired_by; brand_of; client_of; collaboration; competitor_of; merged_with; product_or_service_of; regulated_by; shareholder_of; subsidiary_of; traded_on; undefined}.\\nOutput the results in the format: {relation}\", \"What is the relationship between '{The Baton Rouge Southern Railroad}' and '{Watco}' in the context of the input sentence. Choose an answer from: {acquired_by; brand_of; client_of; collaboration; competitor_of; merged_with; product_or_service_of; regulated_by; shareholder_of; subsidiary_of; traded_on; undefined}.\\nOutput the results in the format: {relation}\"]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_finred_dataset('mydata/116train667.sent', 'mydata/116train667.tup', with_orig=False, with_cls=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "79d95330-0808-44b5-ae65-3fe9da5ebe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longuer =  153\n",
      "exemple :  ['product_or_service_of', 'product_or_service_of', 'traded_on', 'traded_on', 'traded_on', 'collaboration', 'shareholder_of', 'shareholder_of', 'shareholder_of', 'product_or_service_of']\n",
      "exemple :  [\"What is the relationship between '{Coins}' and '{Coins ’N Things}' in the context of the input sentence. Choose an answer from: {acquired_by; brand_of; client_of; collaboration; competitor_of; merged_with; product_or_service_of; regulated_by; shareholder_of; subsidiary_of; traded_on; undefined}.\\nOutput the results in the format: {relation}\", \"What is the relationship between '{silver bullion coins}' and '{Coins ’N Things}' in the context of the input sentence. Choose an answer from: {acquired_by; brand_of; client_of; collaboration; competitor_of; merged_with; product_or_service_of; regulated_by; shareholder_of; subsidiary_of; traded_on; undefined}.\\nOutput the results in the format: {relation}\"]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = get_finred_dataset('mydata/116test667.sent', 'mydata/116test667.tup', with_orig=False, with_cls=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b84b837e-4e22-4d99-be38-e1cd7fbe3d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 400/400 [00:00<00:00, 29718.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 153/153 [00:00<00:00, 14160.88 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 153\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "finred_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "finred_dataset.save_to_disk('fingpt-finred')\n",
    "finred_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
